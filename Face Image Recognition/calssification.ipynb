{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "cuda = torch.cuda.is_available()\n",
    "cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(822154, 2300)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset = torchvision.datasets.ImageFolder(root='train_data/medium', \n",
    "                                                       transform=torchvision.transforms.ToTensor())\n",
    "trainset.__len__(), len(trainset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4601, 2300)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evalset = torchvision.datasets.ImageFolder(root='validation_classification/medium', \n",
    "                                                       transform=torchvision.transforms.ToTensor())\n",
    "evalset.__len__(), len(evalset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_args = dict(shuffle=True, batch_size=128,drop_last=False, num_workers=2,pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=True,drop_last=False, batch_size=128)\n",
    "train_loader = DataLoader(trainset, **train_loader_args)\n",
    "\n",
    "eval_loader_args = dict(shuffle=True, batch_size=128,drop_last=False, num_workers=2,pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=True,drop_last=False, batch_size=128)\n",
    "eval_loader = DataLoader(evalset, **eval_loader_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super(IdentityBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut=[]\n",
    "            self.shortcut.append(nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, padding=0, bias=False))\n",
    "            self.shortcut.append(nn.BatchNorm2d(out_channels))\n",
    "            self.shortcut = nn.Sequential(*self.shortcut)\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.bn1(self.conv1(x)), inplace=True)\n",
    "        y = self.bn2(self.conv2(y)) \n",
    "        y += self.shortcut(x)\n",
    "        y = F.relu(y, inplace=True)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenterLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        num_classes (int): number of classes.\n",
    "        feat_dim (int): feature dimension.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, feat_dim, device=torch.device('cpu')):\n",
    "        super(CenterLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.feat_dim = feat_dim\n",
    "        self.device = device\n",
    "        \n",
    "        self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim).to(self.device))\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: feature matrix with shape (batch_size, feat_dim).\n",
    "            labels: ground truth labels with shape (batch_size).\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        distmat = torch.pow(x, 2).sum(dim=1, keepdim=True).expand(batch_size, self.num_classes) + \\\n",
    "                  torch.pow(self.centers, 2).sum(dim=1, keepdim=True).expand(self.num_classes, batch_size).t()\n",
    "        distmat.addmm_(1, -2, x, self.centers.t())\n",
    "\n",
    "        classes = torch.arange(self.num_classes).long().to(self.device)\n",
    "        labels = labels.unsqueeze(1).expand(batch_size, self.num_classes)\n",
    "        mask = labels.eq(classes.expand(batch_size, self.num_classes))\n",
    "\n",
    "        dist = []\n",
    "        for i in range(batch_size):\n",
    "            value = distmat[i][mask[i]]\n",
    "            value = value.clamp(min=1e-12, max=1e+12) # for numerical stability\n",
    "            dist.append(value)\n",
    "        dist = torch.cat(dist)\n",
    "        loss = dist.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self, feat_dim=2):\n",
    "        super(Network, self).__init__()\n",
    "        \n",
    "        self.layers = []\n",
    "        \n",
    "        self.layers.append(nn.Conv2d(in_channels=3, out_channels=128, kernel_size=3, padding=1, stride=1, bias=False))\n",
    "        self.layers.append(nn.BatchNorm2d(128))\n",
    "        self.layers.append(nn.ReLU(inplace=True))\n",
    "        \n",
    "        self.layers.append(IdentityBlock(in_channels=128, out_channels=128, stride=1))\n",
    "        self.layers.append(IdentityBlock(in_channels=128, out_channels=128, stride=1))\n",
    "       \n",
    "        self.layers.append(IdentityBlock(in_channels=128, out_channels=256, stride=2))\n",
    "        self.layers.append(IdentityBlock(in_channels=256, out_channels=256, stride=1))\n",
    "        \n",
    "        self.layers.append(IdentityBlock(in_channels=256, out_channels=512, stride=1))\n",
    "        self.layers.append(IdentityBlock(in_channels=512, out_channels=512, stride=1))\n",
    "        \n",
    "        self.layers.append(IdentityBlock(in_channels=512, out_channels=1024, stride=2))\n",
    "        self.layers.append(IdentityBlock(in_channels=1024, out_channels=1024, stride=1))\n",
    "\n",
    "        self.layers = nn.Sequential(*self.layers)\n",
    "        \n",
    "        self.linear_label = nn.Linear(1024, 2300, bias=True)\n",
    "        self.embedding = nn.Linear(1024, 2048, bias=True)\n",
    "        \n",
    "        # For creating the embedding to be passed into the Center Loss criterion\n",
    "        self.linear_closs = nn.Linear(1024, feat_dim, bias=True)\n",
    "        self.relu_closs = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x, evalMode=False):\n",
    "        output = x\n",
    "        output = self.layers(output)\n",
    "\n",
    "        output = F.avg_pool2d(output, [output.size(2), output.size(3)], stride=1)\n",
    "        output = output.reshape(output.shape[0], output.shape[1])\n",
    "        \n",
    "        embedding = self.embedding(output)\n",
    "        label_output = self.linear_label(output)\n",
    "        label_output = label_output/torch.norm(self.linear_label.weight, dim=1)\n",
    "        \n",
    "        # Create the feature embedding for the Center Loss\n",
    "        closs_output = self.linear_closs(output)\n",
    "        closs_output = self.relu_closs(closs_output)\n",
    "\n",
    "        return closs_output, label_output, embedding\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Conv2d or type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_normal_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, test_loader, task='Classification'):\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(numEpochs):\n",
    "        avg_loss = 0.0\n",
    "        for batch_num, (feats, labels) in enumerate(data_loader):\n",
    "            feats, labels = feats.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer_label.zero_grad()\n",
    "            optimizer_closs.zero_grad()\n",
    "            \n",
    "            feature, outputs, embedding = model(feats)\n",
    "\n",
    "            l_loss = criterion_label(outputs, labels.long())\n",
    "            c_loss = criterion_closs(feature, labels.long())\n",
    "            loss = l_loss + closs_weight * c_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer_label.step()\n",
    "            # by doing so, weight_cent would not impact on the learning of centers\n",
    "            for param in criterion_closs.parameters():\n",
    "                param.grad.data *= (1. / closs_weight)\n",
    "            optimizer_closs.step()\n",
    "\n",
    "            print('batch completed ' + str(batch_num / 822154 * 128 * 100) + '% ', end='\\r')\n",
    "            \n",
    "            avg_loss += loss.item()\n",
    "            if batch_num % 100 == 99:\n",
    "                print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch+1, batch_num+1, avg_loss/100))\n",
    "                avg_loss = 0.0\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "            del feats\n",
    "            del labels\n",
    "            del loss\n",
    "        \n",
    "        if task == 'Classification':\n",
    "            val_loss, val_acc = test_classify_closs(model, test_loader)\n",
    "            #train_loss, train_acc = test_classify_closs(model, data_loader)\n",
    "            print('Val Loss: {:.4f}\\tVal Accuracy: {:.4f}'.format(val_loss, val_acc))\n",
    "        #else:\n",
    "            #test_verify(model, test_loader)\n",
    "\n",
    "def test_classify_closs(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = []\n",
    "    accuracy = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_num, (feats, labels) in enumerate(test_loader):\n",
    "        feats, labels = feats.to(device), labels.to(device)\n",
    "        feature, outputs, embedding = model(feats)\n",
    "        \n",
    "        _, pred_labels = torch.max(F.softmax(outputs, dim=1), 1)\n",
    "        pred_labels = pred_labels.view(-1)\n",
    "        \n",
    "        l_loss = criterion_label(outputs, labels.long())\n",
    "        c_loss = criterion_closs(feature, labels.long())\n",
    "        loss = l_loss + closs_weight * c_loss\n",
    "        \n",
    "        accuracy += torch.sum(torch.eq(pred_labels, labels)).item()\n",
    "        total += len(labels)\n",
    "        test_loss.extend([loss.item()]*feats.size()[0])\n",
    "        del feats\n",
    "        del labels\n",
    "\n",
    "    model.train()\n",
    "    return np.mean(test_loss), accuracy/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "numEpochs = 5\n",
    "closs_weight = 0.003\n",
    "num_feats = 3\n",
    "learningRate = 3e-4\n",
    "lr_cent = 0.5\n",
    "feat_dim = 2\n",
    "\n",
    "cnn=Network(feat_dim)\n",
    "cnn.apply(init_weights)\n",
    "\n",
    "criterion_label = nn.CrossEntropyLoss()\n",
    "criterion_closs = CenterLoss(2300, feat_dim, device)\n",
    "optimizer_label = torch.optim.Adam(cnn.parameters(),lr=learningRate)\n",
    "optimizer_closs = torch.optim.SGD(criterion_closs.parameters(), lr=lr_cent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tBatch: 100\tAvg-Loss: 0.2955 \n",
      "Epoch: 1\tBatch: 200\tAvg-Loss: 0.2515\n",
      "Epoch: 1\tBatch: 300\tAvg-Loss: 0.2654\n",
      "Epoch: 1\tBatch: 400\tAvg-Loss: 0.2614\n",
      "Epoch: 1\tBatch: 500\tAvg-Loss: 0.2689\n",
      "Epoch: 1\tBatch: 600\tAvg-Loss: 0.2759\n",
      "Epoch: 1\tBatch: 700\tAvg-Loss: 0.2948\n",
      "Epoch: 1\tBatch: 800\tAvg-Loss: 0.2907\n",
      "Epoch: 1\tBatch: 900\tAvg-Loss: 0.2902\n",
      "Epoch: 1\tBatch: 1000\tAvg-Loss: 0.3107\n",
      "Epoch: 1\tBatch: 1100\tAvg-Loss: 0.3147\n",
      "Epoch: 1\tBatch: 1200\tAvg-Loss: 0.3267\n",
      "Epoch: 1\tBatch: 1300\tAvg-Loss: 0.3171\n",
      "Epoch: 1\tBatch: 1400\tAvg-Loss: 0.3061\n",
      "Epoch: 1\tBatch: 1500\tAvg-Loss: 0.3215\n",
      "Epoch: 1\tBatch: 1600\tAvg-Loss: 0.3264\n",
      "Epoch: 1\tBatch: 1700\tAvg-Loss: 0.3299\n",
      "Epoch: 1\tBatch: 1800\tAvg-Loss: 0.3349\n",
      "Epoch: 1\tBatch: 1900\tAvg-Loss: 0.3374\n",
      "Epoch: 1\tBatch: 2000\tAvg-Loss: 0.3204\n",
      "Epoch: 1\tBatch: 2100\tAvg-Loss: 0.3568\n",
      "Epoch: 1\tBatch: 2200\tAvg-Loss: 0.3571\n",
      "Epoch: 1\tBatch: 2300\tAvg-Loss: 0.3442\n",
      "Epoch: 1\tBatch: 2400\tAvg-Loss: 0.3544\n",
      "Epoch: 1\tBatch: 2500\tAvg-Loss: 0.3540\n",
      "Epoch: 1\tBatch: 2600\tAvg-Loss: 0.3379\n",
      "Epoch: 1\tBatch: 2700\tAvg-Loss: 0.3609\n",
      "Epoch: 1\tBatch: 2800\tAvg-Loss: 0.3586\n",
      "Epoch: 1\tBatch: 2900\tAvg-Loss: 0.3434\n",
      "Epoch: 1\tBatch: 3000\tAvg-Loss: 0.3758\n",
      "Epoch: 1\tBatch: 3100\tAvg-Loss: 0.3584\n",
      "Epoch: 1\tBatch: 3200\tAvg-Loss: 0.3554\n",
      "Epoch: 1\tBatch: 3300\tAvg-Loss: 0.3652\n",
      "Epoch: 1\tBatch: 3400\tAvg-Loss: 0.3719\n",
      "Epoch: 1\tBatch: 3500\tAvg-Loss: 0.3878\n",
      "Epoch: 1\tBatch: 3600\tAvg-Loss: 0.3630\n",
      "Epoch: 1\tBatch: 3700\tAvg-Loss: 0.3598\n",
      "Epoch: 1\tBatch: 3800\tAvg-Loss: 0.3616\n",
      "Epoch: 1\tBatch: 3900\tAvg-Loss: 0.3721\n",
      "Epoch: 1\tBatch: 4000\tAvg-Loss: 0.3680\n",
      "Epoch: 1\tBatch: 4100\tAvg-Loss: 0.3592\n",
      "Epoch: 1\tBatch: 4200\tAvg-Loss: 0.3681\n",
      "Epoch: 1\tBatch: 4300\tAvg-Loss: 0.3832\n",
      "Epoch: 1\tBatch: 4400\tAvg-Loss: 0.3725\n",
      "Epoch: 1\tBatch: 4500\tAvg-Loss: 0.3702\n",
      "Epoch: 1\tBatch: 4600\tAvg-Loss: 0.3693\n",
      "Epoch: 1\tBatch: 4700\tAvg-Loss: 0.3881\n",
      "Epoch: 1\tBatch: 4800\tAvg-Loss: 0.3796\n",
      "Epoch: 1\tBatch: 4900\tAvg-Loss: 0.3864\n",
      "Epoch: 1\tBatch: 5000\tAvg-Loss: 0.4030\n",
      "Epoch: 1\tBatch: 5100\tAvg-Loss: 0.3974\n",
      "Epoch: 1\tBatch: 5200\tAvg-Loss: 0.4009\n",
      "Epoch: 1\tBatch: 5300\tAvg-Loss: 0.3839\n",
      "Epoch: 1\tBatch: 5400\tAvg-Loss: 0.3693\n",
      "Epoch: 1\tBatch: 5500\tAvg-Loss: 0.3921\n",
      "Epoch: 1\tBatch: 5600\tAvg-Loss: 0.3639\n",
      "Epoch: 1\tBatch: 5700\tAvg-Loss: 0.3619\n",
      "Epoch: 1\tBatch: 5800\tAvg-Loss: 0.3782\n",
      "Epoch: 1\tBatch: 5900\tAvg-Loss: 0.3838\n",
      "Epoch: 1\tBatch: 6000\tAvg-Loss: 0.3840\n",
      "Epoch: 1\tBatch: 6100\tAvg-Loss: 0.3889\n",
      "Epoch: 1\tBatch: 6200\tAvg-Loss: 0.3831\n",
      "Epoch: 1\tBatch: 6300\tAvg-Loss: 0.3885\n",
      "Epoch: 1\tBatch: 6400\tAvg-Loss: 0.3928\n",
      "Val Loss: 0.9850\tVal Accuracy: 0.7890\n"
     ]
    }
   ],
   "source": [
    "numEpochs = 1\n",
    "cnn.train()\n",
    "cnn.to(device)\n",
    "train(cnn, train_loader, eval_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tBatch: 100\tAvg-Loss: 0.2124 \n",
      "Epoch: 1\tBatch: 200\tAvg-Loss: 0.1867\n",
      "Epoch: 1\tBatch: 300\tAvg-Loss: 0.1820\n",
      "Epoch: 1\tBatch: 400\tAvg-Loss: 0.1736\n",
      "Epoch: 1\tBatch: 500\tAvg-Loss: 0.1847\n",
      "Epoch: 1\tBatch: 600\tAvg-Loss: 0.1923\n",
      "Epoch: 1\tBatch: 700\tAvg-Loss: 0.1832\n",
      "Epoch: 1\tBatch: 800\tAvg-Loss: 0.1945\n",
      "Epoch: 1\tBatch: 900\tAvg-Loss: 0.2063\n",
      "Epoch: 1\tBatch: 1000\tAvg-Loss: 0.2024\n",
      "Epoch: 1\tBatch: 1100\tAvg-Loss: 0.2164\n",
      "Epoch: 1\tBatch: 1200\tAvg-Loss: 0.2238\n",
      "Epoch: 1\tBatch: 1300\tAvg-Loss: 0.2338\n",
      "Epoch: 1\tBatch: 1400\tAvg-Loss: 0.2288\n",
      "Epoch: 1\tBatch: 1500\tAvg-Loss: 0.2468\n",
      "Epoch: 1\tBatch: 1600\tAvg-Loss: 0.2446\n",
      "Epoch: 1\tBatch: 1700\tAvg-Loss: 0.2385\n",
      "Epoch: 1\tBatch: 1800\tAvg-Loss: 0.2517\n",
      "Epoch: 1\tBatch: 1900\tAvg-Loss: 0.2449\n",
      "Epoch: 1\tBatch: 2000\tAvg-Loss: 0.2495\n",
      "Epoch: 1\tBatch: 2100\tAvg-Loss: 0.2525\n",
      "Epoch: 1\tBatch: 2200\tAvg-Loss: 0.2684\n",
      "Epoch: 1\tBatch: 2300\tAvg-Loss: 0.2456\n",
      "Epoch: 1\tBatch: 2400\tAvg-Loss: 0.2682\n",
      "Epoch: 1\tBatch: 2500\tAvg-Loss: 0.2504\n",
      "Epoch: 1\tBatch: 2600\tAvg-Loss: 0.2623\n",
      "Epoch: 1\tBatch: 2700\tAvg-Loss: 0.2496\n",
      "Epoch: 1\tBatch: 2800\tAvg-Loss: 0.2687\n",
      "Epoch: 1\tBatch: 2900\tAvg-Loss: 0.2655\n",
      "Epoch: 1\tBatch: 3000\tAvg-Loss: 0.2759\n",
      "Epoch: 1\tBatch: 3100\tAvg-Loss: 0.2679\n",
      "Epoch: 1\tBatch: 3200\tAvg-Loss: 0.2727\n",
      "Epoch: 1\tBatch: 3300\tAvg-Loss: 0.2787\n",
      "Epoch: 1\tBatch: 3400\tAvg-Loss: 0.2747\n",
      "Epoch: 1\tBatch: 3500\tAvg-Loss: 0.2641\n",
      "Epoch: 1\tBatch: 3600\tAvg-Loss: 0.2731\n",
      "Epoch: 1\tBatch: 3700\tAvg-Loss: 0.2884\n",
      "Epoch: 1\tBatch: 3800\tAvg-Loss: 0.2845\n",
      "Epoch: 1\tBatch: 3900\tAvg-Loss: 0.2609\n",
      "Epoch: 1\tBatch: 4000\tAvg-Loss: 0.2894\n",
      "Epoch: 1\tBatch: 4100\tAvg-Loss: 0.2682\n",
      "Epoch: 1\tBatch: 4200\tAvg-Loss: 0.2714\n",
      "Epoch: 1\tBatch: 4300\tAvg-Loss: 0.2884\n",
      "Epoch: 1\tBatch: 4400\tAvg-Loss: 0.2791\n",
      "Epoch: 1\tBatch: 4500\tAvg-Loss: 0.2910\n",
      "Epoch: 1\tBatch: 4600\tAvg-Loss: 0.2866\n",
      "Epoch: 1\tBatch: 4700\tAvg-Loss: 0.2921\n",
      "Epoch: 1\tBatch: 4800\tAvg-Loss: 0.2814\n",
      "Epoch: 1\tBatch: 4900\tAvg-Loss: 0.3068\n",
      "Epoch: 1\tBatch: 5000\tAvg-Loss: 0.2976\n",
      "Epoch: 1\tBatch: 5100\tAvg-Loss: 0.3137\n",
      "Epoch: 1\tBatch: 5200\tAvg-Loss: 0.2943\n",
      "Epoch: 1\tBatch: 5300\tAvg-Loss: 0.2941\n",
      "Epoch: 1\tBatch: 5400\tAvg-Loss: 0.3023\n",
      "Epoch: 1\tBatch: 5500\tAvg-Loss: 0.2873\n",
      "Epoch: 1\tBatch: 5600\tAvg-Loss: 0.3032\n",
      "Epoch: 1\tBatch: 5700\tAvg-Loss: 0.3031\n",
      "Epoch: 1\tBatch: 5800\tAvg-Loss: 0.3067\n",
      "Epoch: 1\tBatch: 5900\tAvg-Loss: 0.2908\n",
      "Epoch: 1\tBatch: 6000\tAvg-Loss: 0.3012\n",
      "Epoch: 1\tBatch: 6100\tAvg-Loss: 0.2938\n",
      "Epoch: 1\tBatch: 6200\tAvg-Loss: 0.2953\n",
      "Epoch: 1\tBatch: 6300\tAvg-Loss: 0.2809\n",
      "Epoch: 1\tBatch: 6400\tAvg-Loss: 0.2975\n",
      "Val Loss: 0.9816\tVal Accuracy: 0.7953\n"
     ]
    }
   ],
   "source": [
    "numEpochs = 1\n",
    "cnn.train()\n",
    "cnn.to(device)\n",
    "train(cnn, train_loader, eval_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tBatch: 100\tAvg-Loss: 0.1412 \n",
      "Epoch: 1\tBatch: 200\tAvg-Loss: 0.1242\n",
      "Epoch: 1\tBatch: 300\tAvg-Loss: 0.1050\n",
      "Epoch: 1\tBatch: 400\tAvg-Loss: 0.1014\n",
      "Epoch: 1\tBatch: 500\tAvg-Loss: 0.0939\n",
      "Epoch: 1\tBatch: 600\tAvg-Loss: 0.0866\n",
      "Epoch: 1\tBatch: 700\tAvg-Loss: 0.0814\n",
      "Epoch: 1\tBatch: 800\tAvg-Loss: 0.0773\n",
      "Epoch: 1\tBatch: 900\tAvg-Loss: 0.0777\n",
      "Epoch: 1\tBatch: 1000\tAvg-Loss: 0.0758\n",
      "Epoch: 1\tBatch: 1100\tAvg-Loss: 0.0735\n",
      "Epoch: 1\tBatch: 1200\tAvg-Loss: 0.0733\n",
      "Epoch: 1\tBatch: 1300\tAvg-Loss: 0.0701\n",
      "Epoch: 1\tBatch: 1400\tAvg-Loss: 0.0725\n",
      "Epoch: 1\tBatch: 1500\tAvg-Loss: 0.0712\n",
      "Epoch: 1\tBatch: 1600\tAvg-Loss: 0.0683\n",
      "Epoch: 1\tBatch: 1700\tAvg-Loss: 0.0681\n",
      "Epoch: 1\tBatch: 1800\tAvg-Loss: 0.0601\n",
      "Epoch: 1\tBatch: 1900\tAvg-Loss: 0.0631\n",
      "Epoch: 1\tBatch: 2000\tAvg-Loss: 0.0608\n",
      "Epoch: 1\tBatch: 2100\tAvg-Loss: 0.0626\n",
      "Epoch: 1\tBatch: 2200\tAvg-Loss: 0.0634\n",
      "Epoch: 1\tBatch: 2300\tAvg-Loss: 0.0598\n",
      "Epoch: 1\tBatch: 2400\tAvg-Loss: 0.0604\n",
      "Epoch: 1\tBatch: 2500\tAvg-Loss: 0.0564\n",
      "Epoch: 1\tBatch: 2600\tAvg-Loss: 0.0611\n",
      "Epoch: 1\tBatch: 2700\tAvg-Loss: 0.0599\n",
      "Epoch: 1\tBatch: 2800\tAvg-Loss: 0.0549\n",
      "Epoch: 1\tBatch: 2900\tAvg-Loss: 0.0593\n",
      "Epoch: 1\tBatch: 3000\tAvg-Loss: 0.0606\n",
      "Epoch: 1\tBatch: 3100\tAvg-Loss: 0.0546\n",
      "Epoch: 1\tBatch: 3200\tAvg-Loss: 0.0583\n",
      "Epoch: 1\tBatch: 3300\tAvg-Loss: 0.0549\n",
      "Epoch: 1\tBatch: 3400\tAvg-Loss: 0.0538\n",
      "Epoch: 1\tBatch: 3500\tAvg-Loss: 0.0555\n",
      "Epoch: 1\tBatch: 3600\tAvg-Loss: 0.0544\n",
      "Epoch: 1\tBatch: 3700\tAvg-Loss: 0.0579\n",
      "Epoch: 1\tBatch: 3800\tAvg-Loss: 0.0484\n",
      "Epoch: 1\tBatch: 3900\tAvg-Loss: 0.0517\n",
      "Epoch: 1\tBatch: 4000\tAvg-Loss: 0.0549\n",
      "Epoch: 1\tBatch: 4100\tAvg-Loss: 0.0568\n",
      "Epoch: 1\tBatch: 4200\tAvg-Loss: 0.0517\n",
      "Epoch: 1\tBatch: 4300\tAvg-Loss: 0.0505\n",
      "Epoch: 1\tBatch: 4400\tAvg-Loss: 0.0496\n",
      "Epoch: 1\tBatch: 4500\tAvg-Loss: 0.0504\n",
      "Epoch: 1\tBatch: 4600\tAvg-Loss: 0.0489\n",
      "Epoch: 1\tBatch: 4700\tAvg-Loss: 0.0528\n",
      "Epoch: 1\tBatch: 4800\tAvg-Loss: 0.0506\n",
      "Epoch: 1\tBatch: 4900\tAvg-Loss: 0.0470\n",
      "Epoch: 1\tBatch: 5000\tAvg-Loss: 0.0507\n",
      "Epoch: 1\tBatch: 5100\tAvg-Loss: 0.0466\n",
      "Epoch: 1\tBatch: 5200\tAvg-Loss: 0.0508\n",
      "Epoch: 1\tBatch: 5300\tAvg-Loss: 0.0481\n",
      "Epoch: 1\tBatch: 5400\tAvg-Loss: 0.0438\n",
      "Epoch: 1\tBatch: 5500\tAvg-Loss: 0.0478\n",
      "Epoch: 1\tBatch: 5600\tAvg-Loss: 0.0505\n",
      "Epoch: 1\tBatch: 5700\tAvg-Loss: 0.0461\n",
      "Epoch: 1\tBatch: 5800\tAvg-Loss: 0.0487\n",
      "Epoch: 1\tBatch: 5900\tAvg-Loss: 0.0492\n",
      "Epoch: 1\tBatch: 6000\tAvg-Loss: 0.0488\n",
      "Epoch: 1\tBatch: 6100\tAvg-Loss: 0.0463\n",
      "Epoch: 1\tBatch: 6200\tAvg-Loss: 0.0488\n",
      "Epoch: 1\tBatch: 6300\tAvg-Loss: 0.0448\n",
      "Epoch: 1\tBatch: 6400\tAvg-Loss: 0.0434\n",
      "Val Loss: 0.7803\tVal Accuracy: 0.8392\n"
     ]
    }
   ],
   "source": [
    "#LR=3e-5\n",
    "numEpochs = 1\n",
    "cnn.train()\n",
    "cnn.to(device)\n",
    "train(cnn, train_loader, eval_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tBatch: 100\tAvg-Loss: 0.0332 \n",
      "Epoch: 1\tBatch: 200\tAvg-Loss: 0.0325\n",
      "Epoch: 1\tBatch: 300\tAvg-Loss: 0.0332\n",
      "Epoch: 1\tBatch: 400\tAvg-Loss: 0.0312\n",
      "Epoch: 1\tBatch: 500\tAvg-Loss: 0.0309\n",
      "Epoch: 1\tBatch: 600\tAvg-Loss: 0.0320\n",
      "Epoch: 1\tBatch: 700\tAvg-Loss: 0.0295\n",
      "Epoch: 1\tBatch: 800\tAvg-Loss: 0.0289\n",
      "Epoch: 1\tBatch: 900\tAvg-Loss: 0.0287\n",
      "Epoch: 1\tBatch: 1000\tAvg-Loss: 0.0313\n",
      "Epoch: 1\tBatch: 1100\tAvg-Loss: 0.0295\n",
      "Epoch: 1\tBatch: 1200\tAvg-Loss: 0.0276\n",
      "Epoch: 1\tBatch: 1300\tAvg-Loss: 0.0273\n",
      "Epoch: 1\tBatch: 1400\tAvg-Loss: 0.0296\n",
      "Epoch: 1\tBatch: 1500\tAvg-Loss: 0.0261\n",
      "Epoch: 1\tBatch: 1600\tAvg-Loss: 0.0263\n",
      "Epoch: 1\tBatch: 1700\tAvg-Loss: 0.0275\n",
      "Epoch: 1\tBatch: 1800\tAvg-Loss: 0.0279\n",
      "Epoch: 1\tBatch: 1900\tAvg-Loss: 0.0271\n",
      "Epoch: 1\tBatch: 2000\tAvg-Loss: 0.0261\n",
      "Epoch: 1\tBatch: 2100\tAvg-Loss: 0.0288\n",
      "Epoch: 1\tBatch: 2200\tAvg-Loss: 0.0244\n",
      "Epoch: 1\tBatch: 2300\tAvg-Loss: 0.0253\n",
      "Epoch: 1\tBatch: 2400\tAvg-Loss: 0.0252\n",
      "Epoch: 1\tBatch: 2500\tAvg-Loss: 0.0257\n",
      "Epoch: 1\tBatch: 2600\tAvg-Loss: 0.0273\n",
      "Epoch: 1\tBatch: 2700\tAvg-Loss: 0.0251\n",
      "Epoch: 1\tBatch: 2800\tAvg-Loss: 0.0264\n",
      "Epoch: 1\tBatch: 2900\tAvg-Loss: 0.0259\n",
      "Epoch: 1\tBatch: 3000\tAvg-Loss: 0.0224\n",
      "Epoch: 1\tBatch: 3100\tAvg-Loss: 0.0281\n",
      "Epoch: 1\tBatch: 3200\tAvg-Loss: 0.0257\n",
      "Epoch: 1\tBatch: 3300\tAvg-Loss: 0.0276\n",
      "Epoch: 1\tBatch: 3400\tAvg-Loss: 0.0239\n",
      "Epoch: 1\tBatch: 3500\tAvg-Loss: 0.0256\n",
      "Epoch: 1\tBatch: 3600\tAvg-Loss: 0.0241\n",
      "Epoch: 1\tBatch: 3700\tAvg-Loss: 0.0256\n",
      "Epoch: 1\tBatch: 3800\tAvg-Loss: 0.0252\n",
      "Epoch: 1\tBatch: 3900\tAvg-Loss: 0.0248\n",
      "Epoch: 1\tBatch: 4000\tAvg-Loss: 0.0262\n",
      "Epoch: 1\tBatch: 4100\tAvg-Loss: 0.0259\n",
      "Epoch: 1\tBatch: 4200\tAvg-Loss: 0.0256\n",
      "Epoch: 1\tBatch: 4300\tAvg-Loss: 0.0251\n",
      "Epoch: 1\tBatch: 4400\tAvg-Loss: 0.0249\n",
      "Epoch: 1\tBatch: 4500\tAvg-Loss: 0.0251\n",
      "Epoch: 1\tBatch: 4600\tAvg-Loss: 0.0262\n",
      "Epoch: 1\tBatch: 4700\tAvg-Loss: 0.0249\n",
      "Epoch: 1\tBatch: 4800\tAvg-Loss: 0.0241\n",
      "Epoch: 1\tBatch: 4900\tAvg-Loss: 0.0273\n",
      "Epoch: 1\tBatch: 5000\tAvg-Loss: 0.0252\n",
      "Epoch: 1\tBatch: 5100\tAvg-Loss: 0.0266\n",
      "Epoch: 1\tBatch: 5200\tAvg-Loss: 0.0276\n",
      "Epoch: 1\tBatch: 5300\tAvg-Loss: 0.0246\n",
      "Epoch: 1\tBatch: 5400\tAvg-Loss: 0.0244\n",
      "Epoch: 1\tBatch: 5500\tAvg-Loss: 0.0262\n",
      "Epoch: 1\tBatch: 5600\tAvg-Loss: 0.0262\n",
      "Epoch: 1\tBatch: 5700\tAvg-Loss: 0.0262\n",
      "Epoch: 1\tBatch: 5800\tAvg-Loss: 0.0254\n",
      "Epoch: 1\tBatch: 5900\tAvg-Loss: 0.0250\n",
      "Epoch: 1\tBatch: 6000\tAvg-Loss: 0.0255\n",
      "Epoch: 1\tBatch: 6100\tAvg-Loss: 0.0245\n",
      "Epoch: 1\tBatch: 6200\tAvg-Loss: 0.0264\n",
      "Epoch: 1\tBatch: 6300\tAvg-Loss: 0.0257\n",
      "Epoch: 1\tBatch: 6400\tAvg-Loss: 0.0244\n",
      "Val Loss: 0.7732\tVal Accuracy: 0.8424\n"
     ]
    }
   ],
   "source": [
    "#LR=3e-6\n",
    "numEpochs = 1\n",
    "cnn.train()\n",
    "cnn.to(device)\n",
    "train(cnn, train_loader, eval_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tBatch: 100\tAvg-Loss: 0.0275 \n",
      "Epoch: 1\tBatch: 200\tAvg-Loss: 0.0280\n",
      "Epoch: 1\tBatch: 300\tAvg-Loss: 0.0272\n",
      "Epoch: 1\tBatch: 400\tAvg-Loss: 0.0270\n",
      "Epoch: 1\tBatch: 500\tAvg-Loss: 0.0291\n",
      "Epoch: 1\tBatch: 600\tAvg-Loss: 0.0299\n",
      "Epoch: 1\tBatch: 700\tAvg-Loss: 0.0267\n",
      "Epoch: 1\tBatch: 800\tAvg-Loss: 0.0263\n",
      "Epoch: 1\tBatch: 900\tAvg-Loss: 0.0271\n",
      "Epoch: 1\tBatch: 1000\tAvg-Loss: 0.0262\n",
      "Epoch: 1\tBatch: 1100\tAvg-Loss: 0.0248\n",
      "Epoch: 1\tBatch: 1200\tAvg-Loss: 0.0248\n",
      "Epoch: 1\tBatch: 1300\tAvg-Loss: 0.0284\n",
      "Epoch: 1\tBatch: 1400\tAvg-Loss: 0.0239\n",
      "Epoch: 1\tBatch: 1500\tAvg-Loss: 0.0238\n",
      "Epoch: 1\tBatch: 1600\tAvg-Loss: 0.0236\n",
      "Epoch: 1\tBatch: 1700\tAvg-Loss: 0.0248\n",
      "Epoch: 1\tBatch: 1800\tAvg-Loss: 0.0248\n",
      "Epoch: 1\tBatch: 1900\tAvg-Loss: 0.0240\n",
      "Epoch: 1\tBatch: 2000\tAvg-Loss: 0.0237\n",
      "Epoch: 1\tBatch: 2100\tAvg-Loss: 0.0232\n",
      "Epoch: 1\tBatch: 2200\tAvg-Loss: 0.0230\n",
      "Epoch: 1\tBatch: 2300\tAvg-Loss: 0.0255\n",
      "Epoch: 1\tBatch: 2400\tAvg-Loss: 0.0226\n",
      "Epoch: 1\tBatch: 2500\tAvg-Loss: 0.0229\n",
      "Epoch: 1\tBatch: 2600\tAvg-Loss: 0.0243\n",
      "Epoch: 1\tBatch: 2700\tAvg-Loss: 0.0251\n",
      "Epoch: 1\tBatch: 2800\tAvg-Loss: 0.0245\n",
      "Epoch: 1\tBatch: 2900\tAvg-Loss: 0.0243\n",
      "Epoch: 1\tBatch: 3000\tAvg-Loss: 0.0221\n",
      "Epoch: 1\tBatch: 3100\tAvg-Loss: 0.0232\n",
      "Epoch: 1\tBatch: 3200\tAvg-Loss: 0.0214\n",
      "Epoch: 1\tBatch: 3300\tAvg-Loss: 0.0231\n",
      "Epoch: 1\tBatch: 3400\tAvg-Loss: 0.0222\n",
      "Epoch: 1\tBatch: 3500\tAvg-Loss: 0.0220\n",
      "Epoch: 1\tBatch: 3600\tAvg-Loss: 0.0230\n",
      "Epoch: 1\tBatch: 3700\tAvg-Loss: 0.0229\n",
      "Epoch: 1\tBatch: 3800\tAvg-Loss: 0.0219\n",
      "Epoch: 1\tBatch: 3900\tAvg-Loss: 0.0228\n",
      "Epoch: 1\tBatch: 4000\tAvg-Loss: 0.0212\n",
      "Epoch: 1\tBatch: 4100\tAvg-Loss: 0.0211\n",
      "Epoch: 1\tBatch: 4200\tAvg-Loss: 0.0227\n",
      "Epoch: 1\tBatch: 4300\tAvg-Loss: 0.0216\n",
      "Epoch: 1\tBatch: 4400\tAvg-Loss: 0.0224\n",
      "Epoch: 1\tBatch: 4500\tAvg-Loss: 0.0213\n",
      "Epoch: 1\tBatch: 4600\tAvg-Loss: 0.0238\n",
      "Epoch: 1\tBatch: 4700\tAvg-Loss: 0.0219\n",
      "Epoch: 1\tBatch: 4800\tAvg-Loss: 0.0224\n",
      "Epoch: 1\tBatch: 4900\tAvg-Loss: 0.0234\n",
      "Epoch: 1\tBatch: 5000\tAvg-Loss: 0.0216\n",
      "Epoch: 1\tBatch: 5100\tAvg-Loss: 0.0237\n",
      "Epoch: 1\tBatch: 5200\tAvg-Loss: 0.0223\n",
      "Epoch: 1\tBatch: 5300\tAvg-Loss: 0.0204\n",
      "Epoch: 1\tBatch: 5400\tAvg-Loss: 0.0226\n",
      "Epoch: 1\tBatch: 5500\tAvg-Loss: 0.0236\n",
      "Epoch: 1\tBatch: 5600\tAvg-Loss: 0.0222\n",
      "Epoch: 1\tBatch: 5700\tAvg-Loss: 0.0238\n",
      "Epoch: 1\tBatch: 5800\tAvg-Loss: 0.0238\n",
      "Epoch: 1\tBatch: 5900\tAvg-Loss: 0.0224\n",
      "Epoch: 1\tBatch: 6000\tAvg-Loss: 0.0202\n",
      "Epoch: 1\tBatch: 6100\tAvg-Loss: 0.0220\n",
      "Epoch: 1\tBatch: 6200\tAvg-Loss: 0.0230\n",
      "Epoch: 1\tBatch: 6300\tAvg-Loss: 0.0233\n",
      "Epoch: 1\tBatch: 6400\tAvg-Loss: 0.0204\n",
      "Val Loss: 0.7732\tVal Accuracy: 0.8398\n"
     ]
    }
   ],
   "source": [
    "#LR=3e-6\n",
    "numEpochs = 1\n",
    "cnn.train()\n",
    "cnn.to(device)\n",
    "train(cnn, train_loader, eval_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tBatch: 100\tAvg-Loss: 0.0212 \n",
      "Epoch: 1\tBatch: 200\tAvg-Loss: 0.0192\n",
      "Epoch: 1\tBatch: 300\tAvg-Loss: 0.0204\n",
      "Epoch: 1\tBatch: 400\tAvg-Loss: 0.0196\n",
      "Epoch: 1\tBatch: 500\tAvg-Loss: 0.0205\n",
      "Epoch: 1\tBatch: 600\tAvg-Loss: 0.0200\n",
      "Epoch: 1\tBatch: 700\tAvg-Loss: 0.0207\n",
      "Epoch: 1\tBatch: 800\tAvg-Loss: 0.0195\n",
      "Epoch: 1\tBatch: 900\tAvg-Loss: 0.0199\n",
      "Epoch: 1\tBatch: 1000\tAvg-Loss: 0.0206\n",
      "Epoch: 1\tBatch: 1100\tAvg-Loss: 0.0205\n",
      "Epoch: 1\tBatch: 1200\tAvg-Loss: 0.0213\n",
      "Epoch: 1\tBatch: 1300\tAvg-Loss: 0.0202\n",
      "Epoch: 1\tBatch: 1400\tAvg-Loss: 0.0211\n",
      "Epoch: 1\tBatch: 1500\tAvg-Loss: 0.0205\n",
      "Epoch: 1\tBatch: 1600\tAvg-Loss: 0.0189\n",
      "Epoch: 1\tBatch: 1700\tAvg-Loss: 0.0215\n",
      "Epoch: 1\tBatch: 1800\tAvg-Loss: 0.0199\n",
      "Epoch: 1\tBatch: 1900\tAvg-Loss: 0.0199\n",
      "Epoch: 1\tBatch: 2000\tAvg-Loss: 0.0207\n",
      "Epoch: 1\tBatch: 2100\tAvg-Loss: 0.0214\n",
      "Epoch: 1\tBatch: 2200\tAvg-Loss: 0.0220\n",
      "Epoch: 1\tBatch: 2300\tAvg-Loss: 0.0208\n",
      "Epoch: 1\tBatch: 2400\tAvg-Loss: 0.0211\n",
      "Epoch: 1\tBatch: 2500\tAvg-Loss: 0.0196\n",
      "Epoch: 1\tBatch: 2600\tAvg-Loss: 0.0213\n",
      "Epoch: 1\tBatch: 2700\tAvg-Loss: 0.0223\n",
      "Epoch: 1\tBatch: 2800\tAvg-Loss: 0.0194\n",
      "Epoch: 1\tBatch: 2900\tAvg-Loss: 0.0204\n",
      "Epoch: 1\tBatch: 3000\tAvg-Loss: 0.0210\n",
      "Epoch: 1\tBatch: 3100\tAvg-Loss: 0.0200\n",
      "Epoch: 1\tBatch: 3200\tAvg-Loss: 0.0212\n",
      "Epoch: 1\tBatch: 3300\tAvg-Loss: 0.0191\n",
      "Epoch: 1\tBatch: 3400\tAvg-Loss: 0.0210\n",
      "Epoch: 1\tBatch: 3500\tAvg-Loss: 0.0219\n",
      "Epoch: 1\tBatch: 3600\tAvg-Loss: 0.0196\n",
      "Epoch: 1\tBatch: 3700\tAvg-Loss: 0.0193\n",
      "Epoch: 1\tBatch: 3800\tAvg-Loss: 0.0221\n",
      "Epoch: 1\tBatch: 3900\tAvg-Loss: 0.0204\n",
      "Epoch: 1\tBatch: 4000\tAvg-Loss: 0.0199\n",
      "Epoch: 1\tBatch: 4100\tAvg-Loss: 0.0216\n",
      "Epoch: 1\tBatch: 4200\tAvg-Loss: 0.0197\n",
      "Epoch: 1\tBatch: 4300\tAvg-Loss: 0.0221\n",
      "Epoch: 1\tBatch: 4400\tAvg-Loss: 0.0204\n",
      "Epoch: 1\tBatch: 4500\tAvg-Loss: 0.0211\n",
      "Epoch: 1\tBatch: 4600\tAvg-Loss: 0.0210\n",
      "Epoch: 1\tBatch: 4700\tAvg-Loss: 0.0185\n",
      "Epoch: 1\tBatch: 4800\tAvg-Loss: 0.0223\n",
      "Epoch: 1\tBatch: 4900\tAvg-Loss: 0.0202\n",
      "Epoch: 1\tBatch: 5000\tAvg-Loss: 0.0203\n",
      "Epoch: 1\tBatch: 5100\tAvg-Loss: 0.0195\n",
      "Epoch: 1\tBatch: 5200\tAvg-Loss: 0.0211\n",
      "Epoch: 1\tBatch: 5300\tAvg-Loss: 0.0206\n",
      "Epoch: 1\tBatch: 5400\tAvg-Loss: 0.0200\n",
      "Epoch: 1\tBatch: 5500\tAvg-Loss: 0.0214\n",
      "Epoch: 1\tBatch: 5600\tAvg-Loss: 0.0227\n",
      "Epoch: 1\tBatch: 5700\tAvg-Loss: 0.0212\n",
      "Epoch: 1\tBatch: 5800\tAvg-Loss: 0.0216\n",
      "Epoch: 1\tBatch: 5900\tAvg-Loss: 0.0191\n",
      "Epoch: 1\tBatch: 6000\tAvg-Loss: 0.0202\n",
      "Epoch: 1\tBatch: 6100\tAvg-Loss: 0.0200\n",
      "Epoch: 1\tBatch: 6200\tAvg-Loss: 0.0207\n",
      "Epoch: 1\tBatch: 6300\tAvg-Loss: 0.0200\n",
      "Epoch: 1\tBatch: 6400\tAvg-Loss: 0.0192\n",
      "Val Loss: 0.7741\tVal Accuracy: 0.8437\n"
     ]
    }
   ],
   "source": [
    "#lr=3e-6\n",
    "numEpochs = 1\n",
    "cnn.train()\n",
    "cnn.to(device)\n",
    "train(cnn, train_loader, eval_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Images4600\n"
     ]
    }
   ],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, file_list):\n",
    "        self.file_list = file_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.file_list[index])\n",
    "        img = torchvision.transforms.ToTensor()(img)\n",
    "        return img\n",
    "\n",
    "img_sequence=[]\n",
    "def parse_data(datadir):\n",
    "    img_list = []\n",
    "    for root, directories, filenames in os.walk(datadir):\n",
    "        for filename in filenames:\n",
    "            img_sequence.append(int(filename[0:4]))\n",
    "            if filename.endswith('.jpg'):\n",
    "                filei = os.path.join(root, filename)\n",
    "                img_list.append(filei)\n",
    "\n",
    "    print('{}{}'.format('#Images', len(img_list)))\n",
    "    return img_list\n",
    "\n",
    "img_list = parse_data('test_classification/medium')\n",
    "testset = TestDataset(img_list)\n",
    "test_loader_args = dict(shuffle=False, batch_size=256, pin_memory=True, drop_last=False) \n",
    "test_loader = DataLoader(testset, **test_loader_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_loader):\n",
    "  \n",
    "  with torch.no_grad():\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    prediction = torch.LongTensor().to(device)\n",
    "\n",
    "    for batch_num, feats in enumerate(test_loader):\n",
    "        feats = feats.to(device)\n",
    "        outputs = model(feats)[1]\n",
    "        \n",
    "        _, pred_labels = torch.max(F.softmax(outputs, dim=1), 1)\n",
    "        pred_labels = pred_labels.view(-1)\n",
    "        for label_index in pred_labels:\n",
    "            label = torch.LongTensor([int(trainset.classes[label_index.item()])]).to(device)\n",
    "            prediction = torch.cat((prediction, label), dim=0)\n",
    "   \n",
    "    return prediction\n",
    "\n",
    "pred = predict(cnn, test_loader)\n",
    "\n",
    "import pandas as pd\n",
    "result = pd.DataFrame()\n",
    "result['id'] = img_sequence\n",
    "result['label'] = pred.cpu().numpy()\n",
    "result_sorted = result.sort_values(by=['id'])\n",
    "result_sorted.to_csv('/home/ubuntu/zichenli_hw2_classification_21.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Network. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type IdentityBlock. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(cnn, 'resnetnet_best.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
