{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "FE3ygN2FGCM5",
    "outputId": "c3d05015-1ac2-4e01-c2f5-85326aa27aa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./.cache/pip/wheels/de/c2/93/660fd5f7559049268ad2dc6d81c4e39e9e36518766eaf7e342/python_Levenshtein-0.12.0-cp36-cp36m-linux_x86_64.whl\n",
      "Requirement already satisfied: setuptools in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from python-Levenshtein) (42.0.2)\n",
      "Installing collected packages: python-Levenshtein\n",
      "Successfully installed python-Levenshtein-0.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qqR0Mm9JLr44",
    "outputId": "060446e4-ef2c-40cd-8a48-1965f122cf1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import *\n",
    "import pickle as pk\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from Levenshtein import distance \n",
    "import time\n",
    "import random\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bQbL0RweIvHr"
   },
   "source": [
    "# **Load data**\n",
    "\n",
    "Loading all the numpy files containing the utterance information and text information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "lK3YBjyBNaBM",
    "outputId": "a79a0f95-0509-4a44-b5d8-6112212c87fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loading Sucessful.....\n"
     ]
    }
   ],
   "source": [
    "speech_train = np.load('train_new.npy', allow_pickle=True, encoding='bytes')\n",
    "speech_valid = np.load('dev_new.npy', allow_pickle=True, encoding='bytes')\n",
    "speech_test = np.load('test_new.npy', allow_pickle=True, encoding='bytes')\n",
    "\n",
    "transcript_train = np.load('./train_transcripts.npy', allow_pickle=True, encoding='bytes')\n",
    "transcript_valid = np.load('./dev_transcripts.npy', allow_pickle=True, encoding='bytes')\n",
    "print(\"Data Loading Sucessful.....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "dhgju3DHkrK6",
    "outputId": "9b62cfda-01ca-44c1-9764-9c4cfd5d794a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(523,)\n"
     ]
    }
   ],
   "source": [
    "print(speech_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BtTaq7LTJh4R"
   },
   "source": [
    "# **Transform Text Data**\n",
    "\n",
    "`transform_letter_to_index` function transforms alphabetical input to numerical input. Each letter is replaced by its corresponding index from `letter_list` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xsweiPx1JCk4"
   },
   "outputs": [],
   "source": [
    "letter_list = ['<eos>', ' ', \"'\", '+', '-', '.', '_','A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q',\\\n",
    "             'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
    "\n",
    "char_to_index = {\n",
    "    '<eos>': 0, \n",
    "    ' ': 1, \n",
    "    \"'\": 2, \n",
    "    '+': 3, \n",
    "    '-': 4, \n",
    "    '.': 5, \n",
    "    '_': 6, \n",
    "    'A': 7, \n",
    "    'B': 8, \n",
    "    'C': 9, \n",
    "    'D': 10, \n",
    "    'E': 11, \n",
    "    'F': 12, \n",
    "    'G': 13, \n",
    "    'H': 14, \n",
    "    'I': 15, \n",
    "    'J': 16, \n",
    "    'K': 17, \n",
    "    'L': 18, \n",
    "    'M': 19, \n",
    "    'N': 20, \n",
    "    'O': 21, \n",
    "    'P': 22, \n",
    "    'Q': 23, \n",
    "    'R': 24, \n",
    "    'S': 25, \n",
    "    'T': 26, \n",
    "    'U': 27, \n",
    "    'V': 28, \n",
    "    'W': 29, \n",
    "    'X': 30, \n",
    "    'Y': 31, \n",
    "    'Z': 32\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P2us4QQFJgBA"
   },
   "outputs": [],
   "source": [
    "def transform_letter_to_index(transcript, char_to_index):\n",
    "    '''\n",
    "    :param transcript :(N, ) Transcripts are the text input\n",
    "    :param letter_list: Letter list defined above\n",
    "    :return letter_to_index_list: Returns a list for all the transcript sentence to index\n",
    "    '''\n",
    "    letter_to_index_list = []\n",
    "    # loop through each transcript\n",
    "    for trans in transcript:\n",
    "        index_list = []\n",
    "        #append <eos> at start of each sentence\n",
    "        index_list.append(0)\n",
    "        #loop through each word\n",
    "        for i, word in enumerate(trans):\n",
    "            word = word.decode()\n",
    "            for char in word:\n",
    "                index_list.append(char_to_index[char])\n",
    "            if i != len(trans)-1:\n",
    "                index_list.append(1)\n",
    "            else:\n",
    "                index_list.append(0)\n",
    "        letter_to_index_list.append(index_list)\n",
    "\n",
    "    assert(len(transcript) == len(letter_to_index_list))\n",
    "\n",
    "    return letter_to_index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hgQhXBkFJgUE",
    "outputId": "c7667022-9ac3-4f56-abc4-ae8f19bf6342"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed data sucessfully.....\n"
     ]
    }
   ],
   "source": [
    "character_text_train = transform_letter_to_index(transcript_train, char_to_index)\n",
    "character_text_valid = transform_letter_to_index(transcript_valid, char_to_index)\n",
    "print(\"Transformed data sucessfully.....\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KRhKPu8kMIJ7"
   },
   "source": [
    "\n",
    "# **Pyramidal BiLSTM**\n",
    " \n",
    "\n",
    "*   The length of utterance (speech input) can be hundereds to thousands of frames long.\n",
    "*   Paper reports that that a direct LSTM implementation as Encoder resulted in slow convergence and inferior results even after extensive training.\n",
    "*   The major reason is inability of `AttendAndSpell` operation to extract relevant information from a large number of input steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cVpvqoT9iCZW"
   },
   "outputs": [],
   "source": [
    "class pBLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(pBLSTM, self).__init__()\n",
    "      \n",
    "        self.blstm = nn.LSTM(input_size=input_dim*2, hidden_size=hidden_dim, num_layers=1, bidirectional=True)\n",
    "  \n",
    "    def forward(self, x, x_lens):\n",
    "        '''\n",
    "        :param x :(N,T) input to the pBLSTM\n",
    "        :return output: (N,T,H) encoded sequence from pyramidal Bi-LSTM \n",
    "        '''\n",
    "\n",
    "        inputs = x.transpose(0, 1) #shape: B x L x _\n",
    "        batch_size = inputs.shape[0]\n",
    "\n",
    "        #concatenate input in consecutive timestamp\n",
    "        inputs_downsample = []\n",
    "        for i in range(batch_size):\n",
    "            length = x_lens[i]\n",
    "            input_downsample = []\n",
    "            if((length % 2) == 1):\n",
    "                inputs_downsample.append(inputs[i][:length-1])\n",
    "            else:\n",
    "                inputs_downsample.append(inputs[i][:length])\n",
    "\n",
    "            inputs_downsample[i] = inputs_downsample[i].reshape(length // 2, -1)\n",
    "    \n",
    "        out_lens = torch.LongTensor([i//2 for i in x_lens])\n",
    "\n",
    "        inputs_downsample = pad_sequence(inputs_downsample).to(device) \n",
    "        inputs = inputs_downsample #.transpose(0, 1)\n",
    "       \n",
    "        packed_input = pack_padded_sequence(inputs, lengths=out_lens, batch_first=False, enforce_sorted=False)\n",
    "        output_packed, _ = self.blstm(packed_input)\n",
    "        output_padded, lens = pad_packed_sequence(output_packed)\n",
    "        \n",
    "        return output_padded, out_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Ca-GREENqrO"
   },
   "source": [
    "# **Encoder**\n",
    "\n",
    "*    Encoder takes the utterances as inputs and returns the key and value.\n",
    "*    Key and value are nothing but simple projections of the output from pBLSTM network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rt-CxFPGuOw-"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, value_size=128,key_size=128):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.key_size = key_size\n",
    "        self.value_size = value_size\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=1, bidirectional=True)\n",
    "        #Here you need to define the blocks of pBLSTMs\n",
    "        self.pblstm1 = pBLSTM(hidden_dim*2, hidden_dim)\n",
    "        self.pblstm2 = pBLSTM(hidden_dim*2, hidden_dim)\n",
    "        self.pblstm3 = pBLSTM(hidden_dim*2, hidden_dim)\n",
    "\n",
    "        self.key_network = nn.Linear(hidden_dim*2, value_size)\n",
    "        self.value_network = nn.Linear(hidden_dim*2, key_size)\n",
    "  \n",
    "    def forward(self, x, lens):\n",
    "        batch_size = x.shape[1]\n",
    "        packed_input = pack_padded_sequence(x, lengths=lens, batch_first=False, enforce_sorted=False)\n",
    "        out, _ = self.lstm(packed_input)\n",
    "        out_padded, _ = pad_packed_sequence(out) #shape: L x B x _\n",
    "\n",
    "        #Use the outputs and pass it through the pBLSTM blocks\n",
    "        outputs, out_lens = self.pblstm1(out_padded, lens)\n",
    "        outputs, out_lens = self.pblstm2(outputs, out_lens)\n",
    "        outputs, out_lens = self.pblstm3(outputs, out_lens)\n",
    "\n",
    "        output_flatten = outputs.reshape(outputs.size(0) * outputs.size(1), outputs.size(2)) # (L * N) x H\n",
    "        linear_input = output_flatten\n",
    "\n",
    "        keys = self.key_network(linear_input)\n",
    "        keys = keys.view(-1, batch_size, self.key_size) # output shape L x N x D\n",
    "\n",
    "        value = self.value_network(linear_input)\n",
    "        value = value.view(-1, batch_size, self.value_size) # output shape L x N x D\n",
    "\n",
    "        return keys, value, out_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z7CdGLzuOvKx"
   },
   "source": [
    "# **Attention**\n",
    "\n",
    "*    Attention is calculated using key, value and query from Encoder and decoder.\n",
    "\n",
    "Below are the set of operations you need to perform for computing attention.\n",
    "\n",
    "```\n",
    "energy = bmm(key, query)\n",
    "attention = softmax(energy)\n",
    "context = bmm(attention, value)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W5vsgAUyzdjh"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "    def forward(self, query, key, value, lens):\n",
    "        '''\n",
    "        :param query :(N,context_size) Query is the output of LSTMCell from Decoder\n",
    "        :param key: (N,key_size) Key Projection from Encoder per time step\n",
    "        :param value: (N,value_size) Value Projection from Encoder per time step\n",
    "        :return output: Attended Context\n",
    "        :return attention_mask: Attention mask that can be plotted  \n",
    "        '''\n",
    "\n",
    "        key = key.transpose(0,1)  #(N, T, H)\n",
    "        # Input/output shape of bmm: (N, T, H), (N, H, 1) -> (N, T, 1)\n",
    "        energy = torch.bmm(key, query.unsqueeze(2)).squeeze(2) # (N, T)\n",
    "        attention = nn.functional.softmax(energy, dim=1) # (N, T)\n",
    "\n",
    "        # Create an (N, T) boolean mask for all padding positions\n",
    "        # Make use of broadcasting: (1, T), (N, 1) -> (N, T)\n",
    "        mask = torch.arange(key.size(1)).unsqueeze(0) >= lens.unsqueeze(1) # (N, T)\n",
    "        mask = mask.to(device)\n",
    "\n",
    "        attention_masked = attention.clone().masked_fill_(mask.to(device), 0)\n",
    "\n",
    "        sum_attention = attention_masked.sum(dim=1) # (N)\n",
    "        normalized_attention = attention_masked / sum_attention.unsqueeze(1)\n",
    "\n",
    "        value = value.transpose(0,1)  #(N, T, H)  \n",
    "        # Compute attention-weighted sum of context vectors\n",
    "        # Input/output shape of bmm: (N, 1, T), (N, T, H) -> (N, 1, H)\n",
    "        context = torch.bmm(attention.unsqueeze(1), value).squeeze(1)\n",
    "\n",
    "        return context, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A24z9vjsPrSP"
   },
   "source": [
    "# **Decoder**\n",
    "\n",
    "*    As mentioned in Recitation-9 each forward call of decoder deals with just one time step. Thus we use LSTMCell instead of LSLTM here.\n",
    "*    Output from the second LSTMCell can be used as query here for attention module.\n",
    "*    In place of `value` that we get from the attention, this can be replace by context we get from the attention.\n",
    "*    Methods like Gumble noise and teacher forcing can also be incorporated for improving the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ze3vySn38YsC"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, value_size=128, key_size=128, isAttended=True):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.key_size = key_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_dim)    \n",
    "        self.lstm1 = nn.LSTMCell(input_size=hidden_dim+value_size, hidden_size=hidden_dim)\n",
    "        self.lstm2 = nn.LSTMCell(input_size=hidden_dim, hidden_size=key_size)\n",
    "        self.isAttended = isAttended\n",
    "        if(isAttended):\n",
    "            self.attention = Attention()\n",
    "            self.character_prob = nn.Linear(key_size+value_size,vocab_size)\n",
    "\n",
    "    def forward(self, key, values, lens, teacher_forcing=None, text=None, train=True):\n",
    "        '''\n",
    "        :param key :(T,N,key_size) Output of the Encoder Key projection layer\n",
    "        :param values: (T,N,value_size) Output of the Encoder Value projection layer\n",
    "        :param text: (N,text_len) Batch input of text with text_length\n",
    "        :param train: Train or eval mode\n",
    "        :return predictions: Returns the character perdiction probability \n",
    "        '''\n",
    "        batch_size = key.shape[1]\n",
    "        if(train):\n",
    "            text = text.transpose(0, 1) # N x max_len\n",
    "            max_len =  text.shape[1]\n",
    "            embeddings = self.embedding(text) # N x T x D\n",
    "        else:\n",
    "            max_len = 250\n",
    "            \n",
    "        predictions = []\n",
    "        \n",
    "        #initialize hidden states\n",
    "        h1 = torch.zeros(batch_size, self.hidden_dim).to(device)\n",
    "        h2 = torch.zeros(batch_size, self.key_size).to(device)\n",
    "        hid1 = (h1, h1)\n",
    "        hid2 = (h2, h2)\n",
    "        hidden_states = [hid1, hid2]\n",
    "        \n",
    "        prediction = torch.zeros(batch_size, 1).to(device)\n",
    "        \n",
    "        for i in range(max_len - 1):\n",
    "            '''\n",
    "            Here you should implement Gumble noise and teacher forcing techniques\n",
    "            '''\n",
    "            #train: use the embedding of current word as input\n",
    "            if(train):\n",
    "                #print('****** teacher forcing: {}******',teacher_forcing)\n",
    "                if(random.random() < teacher_forcing):\n",
    "                    char_embed = self.embedding(prediction.argmax(dim=-1))\n",
    "                else:\n",
    "                    char_embed = embeddings[:,i,:] \n",
    "                \n",
    "            else:\n",
    "                #eval: use the embedding of last prediction as input\n",
    "                char_embed = self.embedding(prediction.argmax(dim=-1))\n",
    "                assert(char_embed.shape[0] == batch_size)\n",
    "          \n",
    "            #attention(query, key, value, lens)\n",
    "            if hidden_states[0] == None:\n",
    "                last_hiddent_state = torch.zeros(batch_size, 128)\n",
    "            else:\n",
    "                last_hiddent_state = hidden_states[0][-1]\n",
    "            context, attention = self.attention(last_hiddent_state.to(device), key, values, lens)\n",
    "\n",
    "            inp = torch.cat([char_embed, context], dim=1)\n",
    "\n",
    "            hidden_states[0] = self.lstm1(inp, hidden_states[0])\n",
    "          \n",
    "            inp_2 = hidden_states[0][0]\n",
    "            hidden_states[1] = self.lstm2(inp_2, hidden_states[1])\n",
    "\n",
    "            output = hidden_states[1][0]\n",
    "            prediction = self.character_prob(torch.cat([output, context], dim=1))  # 64 x 33\n",
    "            predictions.append(prediction.unsqueeze(1)) # 64 x max_len x 33\n",
    "\n",
    "        return torch.cat(predictions, dim=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zDDrRnxtQw5_"
   },
   "source": [
    "# **Sequence to Sequence Model**\n",
    "\n",
    "*    We train an end-to-end sequence to sequence model comprising of Encoder and Decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VkYfmPMdh5Bx"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,input_dim,vocab_size,hidden_dim,value_size=128, key_size=128,isAttended=True):\n",
    "        super(Seq2Seq,self).__init__()\n",
    "        self.encoder = Encoder(input_dim, hidden_dim)\n",
    "        self.decoder = Decoder(vocab_size, hidden_dim)\n",
    "    def forward(self, teacher_forcing, speech_input, speech_len, text_input=None, train=True):\n",
    "        key, value, lens = self.encoder(speech_input, speech_len)\n",
    "        if(train):\n",
    "            predictions = self.decoder(key, value, lens, teacher_forcing, text_input, train=True)\n",
    "        else:\n",
    "            predictions = self.decoder(key, value, lens, teacher_forcing=None, text=None, train=False)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T16pOYYsTnpj"
   },
   "source": [
    "# **DataLoader & Helper functions**\n",
    "\n",
    "Below is the dataloader for the homework.\n",
    "\n",
    "*    You are expected to fill in the collate function if you use this code skeleton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TQCGXuERKHj0"
   },
   "outputs": [],
   "source": [
    "class Speech2Text_Dataset(Dataset):\n",
    "    def __init__(self, speech, text=None, train=True):\n",
    "        self.speech = speech\n",
    "        self.train = train\n",
    "        if(text is not None):\n",
    "            self.text = text\n",
    "    def __len__(self):\n",
    "        return self.speech.shape[0]\n",
    "    def __getitem__(self, index):\n",
    "        if(self.train):\n",
    "            return torch.tensor(self.speech[index].astype(np.float32)), torch.tensor(self.text[index])\n",
    "        else:\n",
    "            return torch.tensor(self.speech[index].astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NcP6ruJWQE0r"
   },
   "outputs": [],
   "source": [
    "def collate_train(batch_data):\n",
    "    inputs, targets = zip(*batch_data)\n",
    "    X_lens = torch.LongTensor([len(seq) for seq in inputs])\n",
    "    Y_lens = torch.LongTensor([len(seq) for seq in targets])\n",
    "\n",
    "    X = pad_sequence(inputs)\n",
    "    Y = pad_sequence(targets)\n",
    "\n",
    "    return X, Y, X_lens, Y_lens\n",
    "\n",
    "def collate_test(batch_data):\n",
    "    inputs = batch_data\n",
    "    X_lens = torch.LongTensor([len(seq) for seq in inputs])\n",
    "    X = pad_sequence(inputs)\n",
    "\n",
    "    return X, X_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ndp6GoOwbexP"
   },
   "outputs": [],
   "source": [
    "Speech2Text_train_Dataset = Speech2Text_Dataset(speech_train, character_text_train)\n",
    "Speech2Text_eval_Dataset = Speech2Text_Dataset(speech_valid, character_text_valid)\n",
    "Speech2Text_test_Dataset = Speech2Text_Dataset(speech_test, None, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8mam6qQ9vAMh"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(Speech2Text_train_Dataset, batch_size=64, shuffle=True, collate_fn=collate_train)\n",
    "eval_loader = DataLoader(Speech2Text_eval_Dataset, batch_size=64, shuffle=True, collate_fn=collate_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(Speech2Text_test_Dataset, batch_size=64, shuffle=False, drop_last=False, collate_fn=collate_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9nUxRUrIjIgW"
   },
   "outputs": [],
   "source": [
    "def greedy(outputs):\n",
    "    for output in outputs:\n",
    "        chars = ''\n",
    "        char_list = torch.argmax(output, dim=1).cpu()\n",
    "        #print(char_list.shape)\n",
    "        for i in range(len(char_list)):\n",
    "            if i == 0:\n",
    "                break\n",
    "            else:\n",
    "                chars += letter_list[char_list[i]]\n",
    "    return chars\n",
    "\n",
    "def index_to_char(char_list):\n",
    "    chars =''\n",
    "    for i in char_list:\n",
    "        if i == 0:\n",
    "            break\n",
    "        else:\n",
    "            chars += letter_list[i]\n",
    "    return chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7nTBl9L9UA_Y"
   },
   "source": [
    "# **Learning**\n",
    "\n",
    "Defining the Sequence to Sequence model, optimizer and criterion for learning.\n",
    "\n",
    "Train routine is also provided here which can be referenced while writing validation and test routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gYCNa25wvI1J"
   },
   "outputs": [],
   "source": [
    "model = Seq2Seq(input_dim=40, vocab_size=len(letter_list), hidden_dim=128)\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "criterion = nn.CrossEntropyLoss(reduction='none').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (lstm): LSTM(40, 128, bidirectional=True)\n",
      "    (pblstm1): pBLSTM(\n",
      "      (blstm): LSTM(512, 128, bidirectional=True)\n",
      "    )\n",
      "    (pblstm2): pBLSTM(\n",
      "      (blstm): LSTM(512, 128, bidirectional=True)\n",
      "    )\n",
      "    (pblstm3): pBLSTM(\n",
      "      (blstm): LSTM(512, 128, bidirectional=True)\n",
      "    )\n",
      "    (key_network): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (value_network): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(33, 128)\n",
      "    (lstm1): LSTMCell(256, 128)\n",
      "    (lstm2): LSTMCell(128, 128)\n",
      "    (attention): Attention()\n",
      "    (character_prob): Linear(in_features=256, out_features=33, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = torch.load('hw4_epoch1.pth')\n",
    "#model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HNksqnudkbg7"
   },
   "outputs": [],
   "source": [
    "def train(model,train_loader, num_epochs, criterion, optimizer, one_epoch):\n",
    "    model.train()\n",
    "    for epochs in range(num_epochs):\n",
    "        if (one_epoch):\n",
    "            teacher_forcing_para = 0.6\n",
    "        else:\n",
    "            if epochs < 4:\n",
    "                teacher_forcing_para = 0.1\n",
    "            elif epochs < 9:\n",
    "                teacher_forcing_para = 0.2\n",
    "            elif epochs < 14:\n",
    "                teacher_forcing_para = 0.3\n",
    "            elif epochs < 19:\n",
    "                teacher_forcing_para = 0.4\n",
    "            elif epochs < 24:\n",
    "                teacher_forcing_para = 0.5\n",
    "            \n",
    "        print('*** Training Epoch {} with teacher forcing {} :***'.format(epochs+1, teacher_forcing_para))\n",
    "        loss_sum = 0\n",
    "        since = time.time()\n",
    "        for (batch_num, collate_output) in enumerate(train_loader):\n",
    "            #with torch.autograd.set_detect_anomaly(True):\n",
    "            optimizer.zero_grad()\n",
    "            speech_input, text_input, speech_len, text_len = collate_output\n",
    "            speech_input = speech_input.to(device)\n",
    "            text_input = text_input.to(device)\n",
    "\n",
    "            predictions = model(teacher_forcing_para, speech_input, speech_len ,text_input, train=True) # 64 x max_len x 33\n",
    "\n",
    "            text_input = text_input.transpose(0 ,1)\n",
    "\n",
    "            # label[1:]\n",
    "            text_array = text_input.cpu().detach().numpy()\n",
    "            labels = []\n",
    "            for i in range(text_array.shape[0]):\n",
    "                labels.append(text_array[i][1:]) \n",
    "            labels = np.array(labels)\n",
    "            labels = torch.from_numpy(labels).to(device)\n",
    "              \n",
    "            mask = torch.zeros(labels.size()).to(device) # 64 x max_len\n",
    "\n",
    "            for length in text_len:\n",
    "                mask[:,:length-1] = 1\n",
    "            \n",
    "            mask = mask.view(-1).to(device)\n",
    "\n",
    "            predictions = predictions.contiguous().view(-1, predictions.size(-1))\n",
    "            labels = labels.contiguous().view(-1)\n",
    "\n",
    "            loss = criterion(predictions, labels)\n",
    "            masked_loss = torch.sum(loss * mask)\n",
    "\n",
    "            masked_loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm(model.parameters(), 2)\n",
    "            optimizer.step()\n",
    "\n",
    "            current_loss = float(masked_loss.item())/int(torch.sum(mask).item())\n",
    "\n",
    "            if batch_num % 50 == 49:\n",
    "                print('Batch: {}\\tTraining loss: {:.4f}'.format(batch_num+1, current_loss))\n",
    "                print('Batch: {}\\tTraining perplexity: {:.4f}'.format(batch_num+1, np.exp(current_loss)))\n",
    "              \n",
    "        #eval        \n",
    "        model.eval()\n",
    "        print('****** Start validation ******')\n",
    "        total_distance = []\n",
    "\n",
    "        for (batch_num, collate_output) in enumerate(eval_loader):\n",
    "            batch_distance = 0\n",
    "\n",
    "            speech_input, text_input, speech_len, text_len = collate_output\n",
    "            speech_input = speech_input.to(device)\n",
    "            text_input = text_input.to(device)\n",
    "            predictions = model(teacher_forcing_para, speech_input, speech_len ,text_input, train = False) #torch.Size([64, 249, 33])\n",
    "          \n",
    "            #ground truth label\n",
    "            text_input = text_input.transpose(0 ,1) #torch.Size([64, 217])\n",
    "            text_array = text_input.cpu().detach().numpy()\n",
    "            labels = []\n",
    "            for i in range(text_array.shape[0]):\n",
    "                labels.append(text_array[i][1:]) \n",
    "            labels_array = np.array(labels) #torch.Size([64, 216])\n",
    "            #labels = torch.from_numpy(labels).to(device)\n",
    "          \n",
    "            for i, length in enumerate(text_len):\n",
    "                label = labels[i][:length-1]\n",
    "                pred = torch.argmax(predictions[i], dim=1).cpu().numpy()\n",
    "                pred = pred[:length-1]\n",
    "                assert(len(pred) == len(label))\n",
    "\n",
    "                ground_truth = index_to_char(label)\n",
    "                #print('****** groudth truth ******')\n",
    "                #print(ground_truth)\n",
    "                pred_word = index_to_char(pred)\n",
    "                #print('****** pred word ******')\n",
    "                #print(pred_word)\n",
    "              \n",
    "                #distance per sequence\n",
    "                dist = distance(pred_word, ground_truth)\n",
    "                batch_distance += dist\n",
    "                total_distance.append(dist)\n",
    "\n",
    "        print(\"\\n Epoch average distance :\", sum(total_distance) / len(total_distance) )\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "sVnvrWJcvwEN",
    "outputId": "01a61b34-0a12-4a5d-fff8-eb0620bacb4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Training Epoch 1 with teacher forcing 0.1 :***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel_launcher.py:53: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1\tTraining loss: 3.4716\n",
      "Batch: 1\tTraining perplexity: 32.1885\n",
      "Batch: 26\tTraining loss: 1.8376\n",
      "Batch: 26\tTraining perplexity: 6.2817\n",
      "Batch: 51\tTraining loss: 1.5600\n",
      "Batch: 51\tTraining perplexity: 4.7589\n",
      "Batch: 76\tTraining loss: 1.3712\n",
      "Batch: 76\tTraining perplexity: 3.9400\n",
      "Batch: 101\tTraining loss: 1.5034\n",
      "Batch: 101\tTraining perplexity: 4.4971\n",
      "Batch: 126\tTraining loss: 1.4460\n",
      "Batch: 126\tTraining perplexity: 4.2462\n",
      "Batch: 151\tTraining loss: 1.3958\n",
      "Batch: 151\tTraining perplexity: 4.0381\n",
      "Batch: 176\tTraining loss: 1.3993\n",
      "Batch: 176\tTraining perplexity: 4.0524\n",
      "Batch: 201\tTraining loss: 1.3903\n",
      "Batch: 201\tTraining perplexity: 4.0159\n",
      "Batch: 226\tTraining loss: 1.2900\n",
      "Batch: 226\tTraining perplexity: 3.6328\n",
      "Batch: 251\tTraining loss: 1.2832\n",
      "Batch: 251\tTraining perplexity: 3.6081\n",
      "Batch: 276\tTraining loss: 1.2119\n",
      "Batch: 276\tTraining perplexity: 3.3598\n",
      "Batch: 301\tTraining loss: 1.2285\n",
      "Batch: 301\tTraining perplexity: 3.4161\n",
      "Batch: 326\tTraining loss: 1.2263\n",
      "Batch: 326\tTraining perplexity: 3.4087\n",
      "Batch: 351\tTraining loss: 1.2740\n",
      "Batch: 351\tTraining perplexity: 3.5751\n",
      "Batch: 376\tTraining loss: 1.2565\n",
      "Batch: 376\tTraining perplexity: 3.5131\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 82.46875\n",
      "\n",
      " Batch average distance : 85.53125\n",
      "\n",
      " Batch average distance : 82.171875\n",
      "\n",
      " Batch average distance : 78.078125\n",
      "\n",
      " Batch average distance : 83.578125\n",
      "\n",
      " Batch average distance : 71.8125\n",
      "\n",
      " Batch average distance : 79.640625\n",
      "\n",
      " Batch average distance : 80.46875\n",
      "\n",
      " Batch average distance : 81.828125\n",
      "\n",
      " Batch average distance : 76.234375\n",
      "\n",
      " Batch average distance : 84.421875\n",
      "\n",
      " Batch average distance : 82.375\n",
      "\n",
      " Batch average distance : 76.8125\n",
      "\n",
      " Batch average distance : 88.359375\n",
      "\n",
      " Batch average distance : 73.453125\n",
      "\n",
      " Batch average distance : 81.671875\n",
      "\n",
      " Batch average distance : 86.015625\n",
      "\n",
      " Batch average distance : 73.83333333333333\n",
      "\n",
      " Epoch average distance : 80.76311030741411\n",
      "*** Training Epoch 2 with teacher forcing 0.1 :***\n",
      "Batch: 1\tTraining loss: 1.1947\n",
      "Batch: 1\tTraining perplexity: 3.3026\n",
      "Batch: 26\tTraining loss: 1.0043\n",
      "Batch: 26\tTraining perplexity: 2.7300\n",
      "Batch: 51\tTraining loss: 1.0440\n",
      "Batch: 51\tTraining perplexity: 2.8406\n",
      "Batch: 76\tTraining loss: 1.1707\n",
      "Batch: 76\tTraining perplexity: 3.2243\n",
      "Batch: 101\tTraining loss: 1.1597\n",
      "Batch: 101\tTraining perplexity: 3.1889\n",
      "Batch: 126\tTraining loss: 1.1160\n",
      "Batch: 126\tTraining perplexity: 3.0525\n",
      "Batch: 151\tTraining loss: 1.1773\n",
      "Batch: 151\tTraining perplexity: 3.2457\n",
      "Batch: 176\tTraining loss: 1.0866\n",
      "Batch: 176\tTraining perplexity: 2.9642\n",
      "Batch: 201\tTraining loss: 1.1376\n",
      "Batch: 201\tTraining perplexity: 3.1193\n",
      "Batch: 226\tTraining loss: 1.1825\n",
      "Batch: 226\tTraining perplexity: 3.2625\n",
      "Batch: 251\tTraining loss: 0.9801\n",
      "Batch: 251\tTraining perplexity: 2.6648\n",
      "Batch: 276\tTraining loss: 1.0252\n",
      "Batch: 276\tTraining perplexity: 2.7876\n",
      "Batch: 301\tTraining loss: 1.0587\n",
      "Batch: 301\tTraining perplexity: 2.8827\n",
      "Batch: 326\tTraining loss: 1.0808\n",
      "Batch: 326\tTraining perplexity: 2.9469\n",
      "Batch: 351\tTraining loss: 0.9698\n",
      "Batch: 351\tTraining perplexity: 2.6373\n",
      "Batch: 376\tTraining loss: 1.0482\n",
      "Batch: 376\tTraining perplexity: 2.8526\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 78.078125\n",
      "\n",
      " Batch average distance : 69.890625\n",
      "\n",
      " Batch average distance : 77.515625\n",
      "\n",
      " Batch average distance : 78.390625\n",
      "\n",
      " Batch average distance : 74.09375\n",
      "\n",
      " Batch average distance : 76.140625\n",
      "\n",
      " Batch average distance : 70.171875\n",
      "\n",
      " Batch average distance : 69.90625\n",
      "\n",
      " Batch average distance : 76.25\n",
      "\n",
      " Batch average distance : 74.796875\n",
      "\n",
      " Batch average distance : 74.5625\n",
      "\n",
      " Batch average distance : 75.140625\n",
      "\n",
      " Batch average distance : 74.71875\n",
      "\n",
      " Batch average distance : 76.328125\n",
      "\n",
      " Batch average distance : 74.6875\n",
      "\n",
      " Batch average distance : 74.609375\n",
      "\n",
      " Batch average distance : 72.234375\n",
      "\n",
      " Batch average distance : 77.88888888888889\n",
      "\n",
      " Epoch average distance : 74.61392405063292\n",
      "*** Training Epoch 3 with teacher forcing 0.1 :***\n",
      "Batch: 1\tTraining loss: 1.0897\n",
      "Batch: 1\tTraining perplexity: 2.9733\n",
      "Batch: 26\tTraining loss: 1.1744\n",
      "Batch: 26\tTraining perplexity: 3.2363\n",
      "Batch: 51\tTraining loss: 1.0475\n",
      "Batch: 51\tTraining perplexity: 2.8506\n",
      "Batch: 76\tTraining loss: 0.9740\n",
      "Batch: 76\tTraining perplexity: 2.6484\n",
      "Batch: 101\tTraining loss: 1.0465\n",
      "Batch: 101\tTraining perplexity: 2.8478\n",
      "Batch: 126\tTraining loss: 1.1142\n",
      "Batch: 126\tTraining perplexity: 3.0473\n",
      "Batch: 151\tTraining loss: 0.9826\n",
      "Batch: 151\tTraining perplexity: 2.6715\n",
      "Batch: 176\tTraining loss: 0.9875\n",
      "Batch: 176\tTraining perplexity: 2.6844\n",
      "Batch: 201\tTraining loss: 0.8777\n",
      "Batch: 201\tTraining perplexity: 2.4054\n",
      "Batch: 226\tTraining loss: 1.1519\n",
      "Batch: 226\tTraining perplexity: 3.1643\n",
      "Batch: 251\tTraining loss: 0.9640\n",
      "Batch: 251\tTraining perplexity: 2.6221\n",
      "Batch: 276\tTraining loss: 1.0172\n",
      "Batch: 276\tTraining perplexity: 2.7655\n",
      "Batch: 301\tTraining loss: 1.0436\n",
      "Batch: 301\tTraining perplexity: 2.8394\n",
      "Batch: 326\tTraining loss: 0.9175\n",
      "Batch: 326\tTraining perplexity: 2.5030\n",
      "Batch: 351\tTraining loss: 0.9000\n",
      "Batch: 351\tTraining perplexity: 2.4596\n",
      "Batch: 376\tTraining loss: 0.9160\n",
      "Batch: 376\tTraining perplexity: 2.4992\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 68.75\n",
      "\n",
      " Batch average distance : 79.421875\n",
      "\n",
      " Batch average distance : 69.40625\n",
      "\n",
      " Batch average distance : 79.21875\n",
      "\n",
      " Batch average distance : 74.703125\n",
      "\n",
      " Batch average distance : 75.890625\n",
      "\n",
      " Batch average distance : 75.5\n",
      "\n",
      " Batch average distance : 80.4375\n",
      "\n",
      " Batch average distance : 72.65625\n",
      "\n",
      " Batch average distance : 72.703125\n",
      "\n",
      " Batch average distance : 69.125\n",
      "\n",
      " Batch average distance : 75.359375\n",
      "\n",
      " Batch average distance : 74.28125\n",
      "\n",
      " Batch average distance : 78.6875\n",
      "\n",
      " Batch average distance : 73.0625\n",
      "\n",
      " Batch average distance : 73.265625\n",
      "\n",
      " Batch average distance : 70.109375\n",
      "\n",
      " Batch average distance : 79.77777777777777\n",
      "\n",
      " Epoch average distance : 74.35895117540687\n",
      "*** Training Epoch 4 with teacher forcing 0.1 :***\n",
      "Batch: 1\tTraining loss: 1.0027\n",
      "Batch: 1\tTraining perplexity: 2.7257\n",
      "Batch: 26\tTraining loss: 0.8976\n",
      "Batch: 26\tTraining perplexity: 2.4537\n",
      "Batch: 51\tTraining loss: 1.1196\n",
      "Batch: 51\tTraining perplexity: 3.0635\n",
      "Batch: 76\tTraining loss: 1.0299\n",
      "Batch: 76\tTraining perplexity: 2.8009\n",
      "Batch: 101\tTraining loss: 0.9895\n",
      "Batch: 101\tTraining perplexity: 2.6900\n",
      "Batch: 126\tTraining loss: 1.1087\n",
      "Batch: 126\tTraining perplexity: 3.0303\n",
      "Batch: 151\tTraining loss: 0.9326\n",
      "Batch: 151\tTraining perplexity: 2.5410\n",
      "Batch: 176\tTraining loss: 0.9007\n",
      "Batch: 176\tTraining perplexity: 2.4614\n",
      "Batch: 201\tTraining loss: 0.9177\n",
      "Batch: 201\tTraining perplexity: 2.5036\n",
      "Batch: 226\tTraining loss: 0.9142\n",
      "Batch: 226\tTraining perplexity: 2.4947\n",
      "Batch: 251\tTraining loss: 1.0226\n",
      "Batch: 251\tTraining perplexity: 2.7805\n",
      "Batch: 276\tTraining loss: 0.9812\n",
      "Batch: 276\tTraining perplexity: 2.6677\n",
      "Batch: 301\tTraining loss: 0.8626\n",
      "Batch: 301\tTraining perplexity: 2.3693\n",
      "Batch: 326\tTraining loss: 1.0725\n",
      "Batch: 326\tTraining perplexity: 2.9226\n",
      "Batch: 351\tTraining loss: 1.0693\n",
      "Batch: 351\tTraining perplexity: 2.9134\n",
      "Batch: 376\tTraining loss: 1.0847\n",
      "Batch: 376\tTraining perplexity: 2.9585\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 78.34375\n",
      "\n",
      " Batch average distance : 68.96875\n",
      "\n",
      " Batch average distance : 72.328125\n",
      "\n",
      " Batch average distance : 70.515625\n",
      "\n",
      " Batch average distance : 72.5625\n",
      "\n",
      " Batch average distance : 72.3125\n",
      "\n",
      " Batch average distance : 70.359375\n",
      "\n",
      " Batch average distance : 72.953125\n",
      "\n",
      " Batch average distance : 77.15625\n",
      "\n",
      " Batch average distance : 72.46875\n",
      "\n",
      " Batch average distance : 71.78125\n",
      "\n",
      " Batch average distance : 76.609375\n",
      "\n",
      " Batch average distance : 76.140625\n",
      "\n",
      " Batch average distance : 64.859375\n",
      "\n",
      " Batch average distance : 75.84375\n",
      "\n",
      " Batch average distance : 72.78125\n",
      "\n",
      " Batch average distance : 78.40625\n",
      "\n",
      " Batch average distance : 76.88888888888889\n",
      "\n",
      " Epoch average distance : 73.25949367088607\n",
      "*** Training Epoch 5 with teacher forcing 0.2 :***\n",
      "Batch: 1\tTraining loss: 1.1144\n",
      "Batch: 1\tTraining perplexity: 3.0477\n",
      "Batch: 26\tTraining loss: 1.0755\n",
      "Batch: 26\tTraining perplexity: 2.9315\n",
      "Batch: 51\tTraining loss: 1.1550\n",
      "Batch: 51\tTraining perplexity: 3.1739\n",
      "Batch: 76\tTraining loss: 1.0663\n",
      "Batch: 76\tTraining perplexity: 2.9047\n",
      "Batch: 101\tTraining loss: 1.0433\n",
      "Batch: 101\tTraining perplexity: 2.8386\n",
      "Batch: 126\tTraining loss: 0.9259\n",
      "Batch: 126\tTraining perplexity: 2.5241\n",
      "Batch: 151\tTraining loss: 0.9535\n",
      "Batch: 151\tTraining perplexity: 2.5947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 176\tTraining loss: 1.0145\n",
      "Batch: 176\tTraining perplexity: 2.7581\n",
      "Batch: 201\tTraining loss: 1.0243\n",
      "Batch: 201\tTraining perplexity: 2.7852\n",
      "Batch: 226\tTraining loss: 0.9984\n",
      "Batch: 226\tTraining perplexity: 2.7139\n",
      "Batch: 251\tTraining loss: 1.0902\n",
      "Batch: 251\tTraining perplexity: 2.9750\n",
      "Batch: 276\tTraining loss: 0.9941\n",
      "Batch: 276\tTraining perplexity: 2.7024\n",
      "Batch: 301\tTraining loss: 0.9822\n",
      "Batch: 301\tTraining perplexity: 2.6703\n",
      "Batch: 326\tTraining loss: 1.1381\n",
      "Batch: 326\tTraining perplexity: 3.1209\n",
      "Batch: 351\tTraining loss: 0.9558\n",
      "Batch: 351\tTraining perplexity: 2.6007\n",
      "Batch: 376\tTraining loss: 0.9817\n",
      "Batch: 376\tTraining perplexity: 2.6689\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 74.8125\n",
      "\n",
      " Batch average distance : 69.59375\n",
      "\n",
      " Batch average distance : 77.578125\n",
      "\n",
      " Batch average distance : 76.375\n",
      "\n",
      " Batch average distance : 72.25\n",
      "\n",
      " Batch average distance : 67.859375\n",
      "\n",
      " Batch average distance : 79.046875\n",
      "\n",
      " Batch average distance : 75.34375\n",
      "\n",
      " Batch average distance : 73.828125\n",
      "\n",
      " Batch average distance : 75.9375\n",
      "\n",
      " Batch average distance : 72.15625\n",
      "\n",
      " Batch average distance : 70.296875\n",
      "\n",
      " Batch average distance : 70.375\n",
      "\n",
      " Batch average distance : 69.28125\n",
      "\n",
      " Batch average distance : 71.984375\n",
      "\n",
      " Batch average distance : 72.15625\n",
      "\n",
      " Batch average distance : 78.71875\n",
      "\n",
      " Batch average distance : 61.166666666666664\n",
      "\n",
      " Epoch average distance : 73.18896925858951\n",
      "*** Training Epoch 6 with teacher forcing 0.2 :***\n",
      "Batch: 1\tTraining loss: 0.9165\n",
      "Batch: 1\tTraining perplexity: 2.5004\n",
      "Batch: 26\tTraining loss: 1.0513\n",
      "Batch: 26\tTraining perplexity: 2.8613\n",
      "Batch: 51\tTraining loss: 0.9868\n",
      "Batch: 51\tTraining perplexity: 2.6825\n",
      "Batch: 76\tTraining loss: 0.9794\n",
      "Batch: 76\tTraining perplexity: 2.6628\n",
      "Batch: 101\tTraining loss: 0.8958\n",
      "Batch: 101\tTraining perplexity: 2.4494\n",
      "Batch: 126\tTraining loss: 1.0585\n",
      "Batch: 126\tTraining perplexity: 2.8820\n",
      "Batch: 151\tTraining loss: 1.1055\n",
      "Batch: 151\tTraining perplexity: 3.0208\n",
      "Batch: 176\tTraining loss: 1.0910\n",
      "Batch: 176\tTraining perplexity: 2.9772\n",
      "Batch: 201\tTraining loss: 0.8562\n",
      "Batch: 201\tTraining perplexity: 2.3542\n",
      "Batch: 226\tTraining loss: 0.9707\n",
      "Batch: 226\tTraining perplexity: 2.6398\n",
      "Batch: 251\tTraining loss: 0.9193\n",
      "Batch: 251\tTraining perplexity: 2.5076\n",
      "Batch: 276\tTraining loss: 1.0321\n",
      "Batch: 276\tTraining perplexity: 2.8071\n",
      "Batch: 301\tTraining loss: 0.8130\n",
      "Batch: 301\tTraining perplexity: 2.2548\n",
      "Batch: 326\tTraining loss: 1.0014\n",
      "Batch: 326\tTraining perplexity: 2.7222\n",
      "Batch: 351\tTraining loss: 1.0659\n",
      "Batch: 351\tTraining perplexity: 2.9033\n",
      "Batch: 376\tTraining loss: 1.0165\n",
      "Batch: 376\tTraining perplexity: 2.7636\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 70.390625\n",
      "\n",
      " Batch average distance : 64.375\n",
      "\n",
      " Batch average distance : 69.921875\n",
      "\n",
      " Batch average distance : 72.140625\n",
      "\n",
      " Batch average distance : 67.9375\n",
      "\n",
      " Batch average distance : 67.796875\n",
      "\n",
      " Batch average distance : 74.515625\n",
      "\n",
      " Batch average distance : 69.578125\n",
      "\n",
      " Batch average distance : 68.34375\n",
      "\n",
      " Batch average distance : 73.453125\n",
      "\n",
      " Batch average distance : 72.5625\n",
      "\n",
      " Batch average distance : 68.53125\n",
      "\n",
      " Batch average distance : 68.578125\n",
      "\n",
      " Batch average distance : 70.28125\n",
      "\n",
      " Batch average distance : 71.25\n",
      "\n",
      " Batch average distance : 74.4375\n",
      "\n",
      " Batch average distance : 72.953125\n",
      "\n",
      " Batch average distance : 71.88888888888889\n",
      "\n",
      " Epoch average distance : 70.4385171790235\n",
      "*** Training Epoch 7 with teacher forcing 0.2 :***\n",
      "Batch: 1\tTraining loss: 0.8147\n",
      "Batch: 1\tTraining perplexity: 2.2585\n",
      "Batch: 26\tTraining loss: 0.9337\n",
      "Batch: 26\tTraining perplexity: 2.5440\n",
      "Batch: 51\tTraining loss: 0.8866\n",
      "Batch: 51\tTraining perplexity: 2.4270\n",
      "Batch: 76\tTraining loss: 0.8859\n",
      "Batch: 76\tTraining perplexity: 2.4251\n",
      "Batch: 101\tTraining loss: 0.9311\n",
      "Batch: 101\tTraining perplexity: 2.5373\n",
      "Batch: 126\tTraining loss: 0.8999\n",
      "Batch: 126\tTraining perplexity: 2.4594\n",
      "Batch: 151\tTraining loss: 1.0183\n",
      "Batch: 151\tTraining perplexity: 2.7686\n",
      "Batch: 176\tTraining loss: 0.8929\n",
      "Batch: 176\tTraining perplexity: 2.4421\n",
      "Batch: 201\tTraining loss: 0.9716\n",
      "Batch: 201\tTraining perplexity: 2.6421\n",
      "Batch: 226\tTraining loss: 0.8840\n",
      "Batch: 226\tTraining perplexity: 2.4206\n",
      "Batch: 251\tTraining loss: 0.9157\n",
      "Batch: 251\tTraining perplexity: 2.4986\n",
      "Batch: 276\tTraining loss: 0.9050\n",
      "Batch: 276\tTraining perplexity: 2.4720\n",
      "Batch: 301\tTraining loss: 0.8580\n",
      "Batch: 301\tTraining perplexity: 2.3585\n",
      "Batch: 326\tTraining loss: 1.0804\n",
      "Batch: 326\tTraining perplexity: 2.9458\n",
      "Batch: 351\tTraining loss: 0.9866\n",
      "Batch: 351\tTraining perplexity: 2.6820\n",
      "Batch: 376\tTraining loss: 1.1248\n",
      "Batch: 376\tTraining perplexity: 3.0796\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 74.015625\n",
      "\n",
      " Batch average distance : 72.859375\n",
      "\n",
      " Batch average distance : 76.046875\n",
      "\n",
      " Batch average distance : 75.5\n",
      "\n",
      " Batch average distance : 71.640625\n",
      "\n",
      " Batch average distance : 77.015625\n",
      "\n",
      " Batch average distance : 65.859375\n",
      "\n",
      " Batch average distance : 74.265625\n",
      "\n",
      " Batch average distance : 69.046875\n",
      "\n",
      " Batch average distance : 69.03125\n",
      "\n",
      " Batch average distance : 73.421875\n",
      "\n",
      " Batch average distance : 72.296875\n",
      "\n",
      " Batch average distance : 73.9375\n",
      "\n",
      " Batch average distance : 73.828125\n",
      "\n",
      " Batch average distance : 71.46875\n",
      "\n",
      " Batch average distance : 71.90625\n",
      "\n",
      " Batch average distance : 69.640625\n",
      "\n",
      " Batch average distance : 62.55555555555556\n",
      "\n",
      " Epoch average distance : 72.29656419529837\n",
      "*** Training Epoch 8 with teacher forcing 0.2 :***\n",
      "Batch: 1\tTraining loss: 0.9980\n",
      "Batch: 1\tTraining perplexity: 2.7127\n",
      "Batch: 26\tTraining loss: 0.9719\n",
      "Batch: 26\tTraining perplexity: 2.6430\n",
      "Batch: 51\tTraining loss: 0.9324\n",
      "Batch: 51\tTraining perplexity: 2.5406\n",
      "Batch: 76\tTraining loss: 1.0250\n",
      "Batch: 76\tTraining perplexity: 2.7872\n",
      "Batch: 101\tTraining loss: 1.0833\n",
      "Batch: 101\tTraining perplexity: 2.9544\n",
      "Batch: 126\tTraining loss: 0.9525\n",
      "Batch: 126\tTraining perplexity: 2.5923\n",
      "Batch: 151\tTraining loss: 1.0109\n",
      "Batch: 151\tTraining perplexity: 2.7481\n",
      "Batch: 176\tTraining loss: 0.9815\n",
      "Batch: 176\tTraining perplexity: 2.6684\n",
      "Batch: 201\tTraining loss: 1.0038\n",
      "Batch: 201\tTraining perplexity: 2.7287\n",
      "Batch: 226\tTraining loss: 1.0357\n",
      "Batch: 226\tTraining perplexity: 2.8171\n",
      "Batch: 251\tTraining loss: 1.0097\n",
      "Batch: 251\tTraining perplexity: 2.7447\n",
      "Batch: 276\tTraining loss: 0.9790\n",
      "Batch: 276\tTraining perplexity: 2.6617\n",
      "Batch: 301\tTraining loss: 0.9792\n",
      "Batch: 301\tTraining perplexity: 2.6624\n",
      "Batch: 326\tTraining loss: 1.0814\n",
      "Batch: 326\tTraining perplexity: 2.9487\n",
      "Batch: 351\tTraining loss: 0.8852\n",
      "Batch: 351\tTraining perplexity: 2.4235\n",
      "Batch: 376\tTraining loss: 1.0719\n",
      "Batch: 376\tTraining perplexity: 2.9209\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 74.03125\n",
      "\n",
      " Batch average distance : 69.8125\n",
      "\n",
      " Batch average distance : 68.953125\n",
      "\n",
      " Batch average distance : 70.828125\n",
      "\n",
      " Batch average distance : 72.296875\n",
      "\n",
      " Batch average distance : 75.4375\n",
      "\n",
      " Batch average distance : 68.1875\n",
      "\n",
      " Batch average distance : 72.9375\n",
      "\n",
      " Batch average distance : 71.578125\n",
      "\n",
      " Batch average distance : 71.578125\n",
      "\n",
      " Batch average distance : 67.046875\n",
      "\n",
      " Batch average distance : 69.109375\n",
      "\n",
      " Batch average distance : 66.625\n",
      "\n",
      " Batch average distance : 79.578125\n",
      "\n",
      " Batch average distance : 69.296875\n",
      "\n",
      " Batch average distance : 67.203125\n",
      "\n",
      " Batch average distance : 66.515625\n",
      "\n",
      " Batch average distance : 78.27777777777777\n",
      "\n",
      " Epoch average distance : 70.77215189873418\n",
      "*** Training Epoch 9 with teacher forcing 0.2 :***\n",
      "Batch: 1\tTraining loss: 0.8896\n",
      "Batch: 1\tTraining perplexity: 2.4342\n",
      "Batch: 26\tTraining loss: 0.9587\n",
      "Batch: 26\tTraining perplexity: 2.6083\n",
      "Batch: 51\tTraining loss: 0.9262\n",
      "Batch: 51\tTraining perplexity: 2.5249\n",
      "Batch: 76\tTraining loss: 0.9351\n",
      "Batch: 76\tTraining perplexity: 2.5475\n",
      "Batch: 101\tTraining loss: 0.9246\n",
      "Batch: 101\tTraining perplexity: 2.5210\n",
      "Batch: 126\tTraining loss: 0.9827\n",
      "Batch: 126\tTraining perplexity: 2.6718\n",
      "Batch: 151\tTraining loss: 1.0190\n",
      "Batch: 151\tTraining perplexity: 2.7703\n",
      "Batch: 176\tTraining loss: 0.9084\n",
      "Batch: 176\tTraining perplexity: 2.4804\n",
      "Batch: 201\tTraining loss: 1.0047\n",
      "Batch: 201\tTraining perplexity: 2.7312\n",
      "Batch: 226\tTraining loss: 0.9029\n",
      "Batch: 226\tTraining perplexity: 2.4668\n",
      "Batch: 251\tTraining loss: 1.0405\n",
      "Batch: 251\tTraining perplexity: 2.8307\n",
      "Batch: 276\tTraining loss: 1.0651\n",
      "Batch: 276\tTraining perplexity: 2.9011\n",
      "Batch: 301\tTraining loss: 0.9840\n",
      "Batch: 301\tTraining perplexity: 2.6750\n",
      "Batch: 326\tTraining loss: 0.7749\n",
      "Batch: 326\tTraining perplexity: 2.1704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 351\tTraining loss: 0.8178\n",
      "Batch: 351\tTraining perplexity: 2.2655\n",
      "Batch: 376\tTraining loss: 0.9446\n",
      "Batch: 376\tTraining perplexity: 2.5717\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 77.25\n",
      "\n",
      " Batch average distance : 65.4375\n",
      "\n",
      " Batch average distance : 68.53125\n",
      "\n",
      " Batch average distance : 72.25\n",
      "\n",
      " Batch average distance : 70.859375\n",
      "\n",
      " Batch average distance : 67.125\n",
      "\n",
      " Batch average distance : 71.34375\n",
      "\n",
      " Batch average distance : 68.3125\n",
      "\n",
      " Batch average distance : 73.828125\n",
      "\n",
      " Batch average distance : 67.515625\n",
      "\n",
      " Batch average distance : 66.390625\n",
      "\n",
      " Batch average distance : 70.59375\n",
      "\n",
      " Batch average distance : 65.375\n",
      "\n",
      " Batch average distance : 69.0\n",
      "\n",
      " Batch average distance : 70.140625\n",
      "\n",
      " Batch average distance : 72.296875\n",
      "\n",
      " Batch average distance : 62.46875\n",
      "\n",
      " Batch average distance : 71.11111111111111\n",
      "\n",
      " Epoch average distance : 69.36528028933093\n",
      "*** Training Epoch 10 with teacher forcing 0.3 :***\n",
      "Batch: 1\tTraining loss: 1.0743\n",
      "Batch: 1\tTraining perplexity: 2.9279\n",
      "Batch: 26\tTraining loss: 1.1176\n",
      "Batch: 26\tTraining perplexity: 3.0576\n",
      "Batch: 51\tTraining loss: 1.1172\n",
      "Batch: 51\tTraining perplexity: 3.0562\n",
      "Batch: 76\tTraining loss: 1.1050\n",
      "Batch: 76\tTraining perplexity: 3.0192\n",
      "Batch: 101\tTraining loss: 1.0626\n",
      "Batch: 101\tTraining perplexity: 2.8940\n",
      "Batch: 126\tTraining loss: 1.1308\n",
      "Batch: 126\tTraining perplexity: 3.0981\n",
      "Batch: 151\tTraining loss: 1.1977\n",
      "Batch: 151\tTraining perplexity: 3.3126\n",
      "Batch: 176\tTraining loss: 1.1668\n",
      "Batch: 176\tTraining perplexity: 3.2118\n",
      "Batch: 201\tTraining loss: 0.9939\n",
      "Batch: 201\tTraining perplexity: 2.7016\n",
      "Batch: 226\tTraining loss: 1.0798\n",
      "Batch: 226\tTraining perplexity: 2.9440\n",
      "Batch: 251\tTraining loss: 1.1188\n",
      "Batch: 251\tTraining perplexity: 3.0612\n",
      "Batch: 276\tTraining loss: 1.0413\n",
      "Batch: 276\tTraining perplexity: 2.8329\n",
      "Batch: 301\tTraining loss: 1.1082\n",
      "Batch: 301\tTraining perplexity: 3.0288\n",
      "Batch: 326\tTraining loss: 1.1304\n",
      "Batch: 326\tTraining perplexity: 3.0969\n",
      "Batch: 351\tTraining loss: 1.0810\n",
      "Batch: 351\tTraining perplexity: 2.9477\n",
      "Batch: 376\tTraining loss: 1.1463\n",
      "Batch: 376\tTraining perplexity: 3.1464\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 70.328125\n",
      "\n",
      " Batch average distance : 78.96875\n",
      "\n",
      " Batch average distance : 66.140625\n",
      "\n",
      " Batch average distance : 71.796875\n",
      "\n",
      " Batch average distance : 68.625\n",
      "\n",
      " Batch average distance : 71.890625\n",
      "\n",
      " Batch average distance : 72.5\n",
      "\n",
      " Batch average distance : 68.828125\n",
      "\n",
      " Batch average distance : 67.765625\n",
      "\n",
      " Batch average distance : 66.703125\n",
      "\n",
      " Batch average distance : 72.375\n",
      "\n",
      " Batch average distance : 61.015625\n",
      "\n",
      " Batch average distance : 74.890625\n",
      "\n",
      " Batch average distance : 72.53125\n",
      "\n",
      " Batch average distance : 68.421875\n",
      "\n",
      " Batch average distance : 69.3125\n",
      "\n",
      " Batch average distance : 71.53125\n",
      "\n",
      " Batch average distance : 78.88888888888889\n",
      "\n",
      " Epoch average distance : 70.35443037974683\n",
      "*** Training Epoch 11 with teacher forcing 0.3 :***\n",
      "Batch: 1\tTraining loss: 1.0673\n",
      "Batch: 1\tTraining perplexity: 2.9076\n",
      "Batch: 26\tTraining loss: 0.9876\n",
      "Batch: 26\tTraining perplexity: 2.6849\n",
      "Batch: 51\tTraining loss: 1.1132\n",
      "Batch: 51\tTraining perplexity: 3.0440\n",
      "Batch: 76\tTraining loss: 1.2259\n",
      "Batch: 76\tTraining perplexity: 3.4071\n",
      "Batch: 101\tTraining loss: 0.9630\n",
      "Batch: 101\tTraining perplexity: 2.6195\n",
      "Batch: 126\tTraining loss: 0.9937\n",
      "Batch: 126\tTraining perplexity: 2.7013\n",
      "Batch: 151\tTraining loss: 0.9853\n",
      "Batch: 151\tTraining perplexity: 2.6787\n",
      "Batch: 176\tTraining loss: 0.9272\n",
      "Batch: 176\tTraining perplexity: 2.5273\n",
      "Batch: 201\tTraining loss: 1.0666\n",
      "Batch: 201\tTraining perplexity: 2.9056\n",
      "Batch: 226\tTraining loss: 0.8274\n",
      "Batch: 226\tTraining perplexity: 2.2875\n",
      "Batch: 251\tTraining loss: 1.0682\n",
      "Batch: 251\tTraining perplexity: 2.9100\n",
      "Batch: 276\tTraining loss: 1.0368\n",
      "Batch: 276\tTraining perplexity: 2.8202\n",
      "Batch: 301\tTraining loss: 1.0943\n",
      "Batch: 301\tTraining perplexity: 2.9870\n",
      "Batch: 326\tTraining loss: 1.1313\n",
      "Batch: 326\tTraining perplexity: 3.0995\n",
      "Batch: 351\tTraining loss: 0.9152\n",
      "Batch: 351\tTraining perplexity: 2.4974\n",
      "Batch: 376\tTraining loss: 0.8490\n",
      "Batch: 376\tTraining perplexity: 2.3373\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 71.375\n",
      "\n",
      " Batch average distance : 65.484375\n",
      "\n",
      " Batch average distance : 67.125\n",
      "\n",
      " Batch average distance : 73.75\n",
      "\n",
      " Batch average distance : 67.359375\n",
      "\n",
      " Batch average distance : 74.609375\n",
      "\n",
      " Batch average distance : 65.171875\n",
      "\n",
      " Batch average distance : 72.40625\n",
      "\n",
      " Batch average distance : 70.578125\n",
      "\n",
      " Batch average distance : 62.71875\n",
      "\n",
      " Batch average distance : 69.75\n",
      "\n",
      " Batch average distance : 68.4375\n",
      "\n",
      " Batch average distance : 65.40625\n",
      "\n",
      " Batch average distance : 66.75\n",
      "\n",
      " Batch average distance : 66.515625\n",
      "\n",
      " Batch average distance : 71.984375\n",
      "\n",
      " Batch average distance : 67.015625\n",
      "\n",
      " Batch average distance : 58.611111111111114\n",
      "\n",
      " Epoch average distance : 68.45117540687161\n",
      "*** Training Epoch 12 with teacher forcing 0.3 :***\n",
      "Batch: 1\tTraining loss: 0.8961\n",
      "Batch: 1\tTraining perplexity: 2.4501\n",
      "Batch: 26\tTraining loss: 0.8975\n",
      "Batch: 26\tTraining perplexity: 2.4534\n",
      "Batch: 51\tTraining loss: 0.9522\n",
      "Batch: 51\tTraining perplexity: 2.5915\n",
      "Batch: 76\tTraining loss: 1.0918\n",
      "Batch: 76\tTraining perplexity: 2.9795\n",
      "Batch: 101\tTraining loss: 0.8673\n",
      "Batch: 101\tTraining perplexity: 2.3804\n",
      "Batch: 126\tTraining loss: 0.8737\n",
      "Batch: 126\tTraining perplexity: 2.3957\n",
      "Batch: 151\tTraining loss: 0.8950\n",
      "Batch: 151\tTraining perplexity: 2.4472\n",
      "Batch: 176\tTraining loss: 0.9993\n",
      "Batch: 176\tTraining perplexity: 2.7164\n",
      "Batch: 201\tTraining loss: 0.9071\n",
      "Batch: 201\tTraining perplexity: 2.4771\n",
      "Batch: 226\tTraining loss: 1.0338\n",
      "Batch: 226\tTraining perplexity: 2.8118\n",
      "Batch: 251\tTraining loss: 0.9863\n",
      "Batch: 251\tTraining perplexity: 2.6812\n",
      "Batch: 276\tTraining loss: 1.0046\n",
      "Batch: 276\tTraining perplexity: 2.7308\n",
      "Batch: 301\tTraining loss: 0.9173\n",
      "Batch: 301\tTraining perplexity: 2.5026\n",
      "Batch: 326\tTraining loss: 0.8842\n",
      "Batch: 326\tTraining perplexity: 2.4212\n",
      "Batch: 351\tTraining loss: 0.8072\n",
      "Batch: 351\tTraining perplexity: 2.2416\n",
      "Batch: 376\tTraining loss: 0.9450\n",
      "Batch: 376\tTraining perplexity: 2.5727\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 64.921875\n",
      "\n",
      " Batch average distance : 62.265625\n",
      "\n",
      " Batch average distance : 68.203125\n",
      "\n",
      " Batch average distance : 69.953125\n",
      "\n",
      " Batch average distance : 68.640625\n",
      "\n",
      " Batch average distance : 63.15625\n",
      "\n",
      " Batch average distance : 72.21875\n",
      "\n",
      " Batch average distance : 63.4375\n",
      "\n",
      " Batch average distance : 64.890625\n",
      "\n",
      " Batch average distance : 67.15625\n",
      "\n",
      " Batch average distance : 66.53125\n",
      "\n",
      " Batch average distance : 62.1875\n",
      "\n",
      " Batch average distance : 70.328125\n",
      "\n",
      " Batch average distance : 63.609375\n",
      "\n",
      " Batch average distance : 65.84375\n",
      "\n",
      " Batch average distance : 66.859375\n",
      "\n",
      " Batch average distance : 72.234375\n",
      "\n",
      " Batch average distance : 64.33333333333333\n",
      "\n",
      " Epoch average distance : 66.57685352622062\n",
      "*** Training Epoch 13 with teacher forcing 0.3 :***\n",
      "Batch: 1\tTraining loss: 1.0052\n",
      "Batch: 1\tTraining perplexity: 2.7325\n",
      "Batch: 26\tTraining loss: 0.7497\n",
      "Batch: 26\tTraining perplexity: 2.1164\n",
      "Batch: 51\tTraining loss: 0.8879\n",
      "Batch: 51\tTraining perplexity: 2.4301\n",
      "Batch: 76\tTraining loss: 0.9562\n",
      "Batch: 76\tTraining perplexity: 2.6018\n",
      "Batch: 101\tTraining loss: 0.9508\n",
      "Batch: 101\tTraining perplexity: 2.5879\n",
      "Batch: 126\tTraining loss: 0.8296\n",
      "Batch: 126\tTraining perplexity: 2.2925\n",
      "Batch: 151\tTraining loss: 0.9907\n",
      "Batch: 151\tTraining perplexity: 2.6931\n",
      "Batch: 176\tTraining loss: 0.9473\n",
      "Batch: 176\tTraining perplexity: 2.5786\n",
      "Batch: 201\tTraining loss: 0.8872\n",
      "Batch: 201\tTraining perplexity: 2.4284\n",
      "Batch: 226\tTraining loss: 0.9279\n",
      "Batch: 226\tTraining perplexity: 2.5292\n",
      "Batch: 251\tTraining loss: 0.7336\n",
      "Batch: 251\tTraining perplexity: 2.0825\n",
      "Batch: 276\tTraining loss: 0.8370\n",
      "Batch: 276\tTraining perplexity: 2.3094\n",
      "Batch: 301\tTraining loss: 0.9988\n",
      "Batch: 301\tTraining perplexity: 2.7151\n",
      "Batch: 326\tTraining loss: 0.8611\n",
      "Batch: 326\tTraining perplexity: 2.3658\n",
      "Batch: 351\tTraining loss: 0.5994\n",
      "Batch: 351\tTraining perplexity: 1.8210\n",
      "Batch: 376\tTraining loss: 0.9317\n",
      "Batch: 376\tTraining perplexity: 2.5387\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 57.9375\n",
      "\n",
      " Batch average distance : 55.4375\n",
      "\n",
      " Batch average distance : 63.109375\n",
      "\n",
      " Batch average distance : 62.4375\n",
      "\n",
      " Batch average distance : 56.65625\n",
      "\n",
      " Batch average distance : 59.046875\n",
      "\n",
      " Batch average distance : 58.765625\n",
      "\n",
      " Batch average distance : 61.21875\n",
      "\n",
      " Batch average distance : 53.09375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Batch average distance : 56.75\n",
      "\n",
      " Batch average distance : 61.515625\n",
      "\n",
      " Batch average distance : 55.796875\n",
      "\n",
      " Batch average distance : 67.03125\n",
      "\n",
      " Batch average distance : 61.0625\n",
      "\n",
      " Batch average distance : 56.28125\n",
      "\n",
      " Batch average distance : 55.40625\n",
      "\n",
      " Batch average distance : 59.46875\n",
      "\n",
      " Batch average distance : 60.55555555555556\n",
      "\n",
      " Epoch average distance : 58.91048824593128\n",
      "*** Training Epoch 14 with teacher forcing 0.3 :***\n",
      "Batch: 1\tTraining loss: 0.8742\n",
      "Batch: 1\tTraining perplexity: 2.3969\n",
      "Batch: 26\tTraining loss: 0.8549\n",
      "Batch: 26\tTraining perplexity: 2.3512\n",
      "Batch: 51\tTraining loss: 0.8284\n",
      "Batch: 51\tTraining perplexity: 2.2897\n",
      "Batch: 76\tTraining loss: 0.6976\n",
      "Batch: 76\tTraining perplexity: 2.0089\n",
      "Batch: 101\tTraining loss: 0.8740\n",
      "Batch: 101\tTraining perplexity: 2.3965\n",
      "Batch: 126\tTraining loss: 0.7294\n",
      "Batch: 126\tTraining perplexity: 2.0739\n",
      "Batch: 151\tTraining loss: 0.6069\n",
      "Batch: 151\tTraining perplexity: 1.8347\n",
      "Batch: 176\tTraining loss: 0.6978\n",
      "Batch: 176\tTraining perplexity: 2.0093\n",
      "Batch: 201\tTraining loss: 0.6486\n",
      "Batch: 201\tTraining perplexity: 1.9128\n",
      "Batch: 226\tTraining loss: 0.8113\n",
      "Batch: 226\tTraining perplexity: 2.2509\n",
      "Batch: 251\tTraining loss: 0.7058\n",
      "Batch: 251\tTraining perplexity: 2.0254\n",
      "Batch: 276\tTraining loss: 0.6469\n",
      "Batch: 276\tTraining perplexity: 1.9096\n",
      "Batch: 301\tTraining loss: 0.5998\n",
      "Batch: 301\tTraining perplexity: 1.8217\n",
      "Batch: 326\tTraining loss: 0.7025\n",
      "Batch: 326\tTraining perplexity: 2.0188\n",
      "Batch: 351\tTraining loss: 0.7562\n",
      "Batch: 351\tTraining perplexity: 2.1302\n",
      "Batch: 376\tTraining loss: 0.6458\n",
      "Batch: 376\tTraining perplexity: 1.9076\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 54.8125\n",
      "\n",
      " Batch average distance : 50.8125\n",
      "\n",
      " Batch average distance : 52.28125\n",
      "\n",
      " Batch average distance : 54.140625\n",
      "\n",
      " Batch average distance : 52.890625\n",
      "\n",
      " Batch average distance : 51.515625\n",
      "\n",
      " Batch average distance : 50.25\n",
      "\n",
      " Batch average distance : 52.625\n",
      "\n",
      " Batch average distance : 49.34375\n",
      "\n",
      " Batch average distance : 51.375\n",
      "\n",
      " Batch average distance : 50.390625\n",
      "\n",
      " Batch average distance : 55.21875\n",
      "\n",
      " Batch average distance : 53.453125\n",
      "\n",
      " Batch average distance : 47.640625\n",
      "\n",
      " Batch average distance : 47.734375\n",
      "\n",
      " Batch average distance : 51.875\n",
      "\n",
      " Batch average distance : 50.40625\n",
      "\n",
      " Batch average distance : 56.0\n",
      "\n",
      " Epoch average distance : 51.64647377938517\n",
      "*** Training Epoch 15 with teacher forcing 0.4 :***\n",
      "Batch: 1\tTraining loss: 0.6303\n",
      "Batch: 1\tTraining perplexity: 1.8782\n",
      "Batch: 26\tTraining loss: 0.6565\n",
      "Batch: 26\tTraining perplexity: 1.9280\n",
      "Batch: 51\tTraining loss: 0.8389\n",
      "Batch: 51\tTraining perplexity: 2.3139\n",
      "Batch: 76\tTraining loss: 0.7570\n",
      "Batch: 76\tTraining perplexity: 2.1320\n",
      "Batch: 101\tTraining loss: 0.7708\n",
      "Batch: 101\tTraining perplexity: 2.1614\n",
      "Batch: 126\tTraining loss: 0.7833\n",
      "Batch: 126\tTraining perplexity: 2.1888\n",
      "Batch: 151\tTraining loss: 0.7631\n",
      "Batch: 151\tTraining perplexity: 2.1449\n",
      "Batch: 176\tTraining loss: 0.7792\n",
      "Batch: 176\tTraining perplexity: 2.1796\n",
      "Batch: 201\tTraining loss: 0.6578\n",
      "Batch: 201\tTraining perplexity: 1.9304\n",
      "Batch: 226\tTraining loss: 0.6720\n",
      "Batch: 226\tTraining perplexity: 1.9582\n",
      "Batch: 251\tTraining loss: 0.5535\n",
      "Batch: 251\tTraining perplexity: 1.7392\n",
      "Batch: 276\tTraining loss: 0.6749\n",
      "Batch: 276\tTraining perplexity: 1.9638\n",
      "Batch: 301\tTraining loss: 0.7402\n",
      "Batch: 301\tTraining perplexity: 2.0964\n",
      "Batch: 326\tTraining loss: 0.7289\n",
      "Batch: 326\tTraining perplexity: 2.0728\n",
      "Batch: 351\tTraining loss: 0.7651\n",
      "Batch: 351\tTraining perplexity: 2.1492\n",
      "Batch: 376\tTraining loss: 0.6726\n",
      "Batch: 376\tTraining perplexity: 1.9592\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 41.328125\n",
      "\n",
      " Batch average distance : 40.734375\n",
      "\n",
      " Batch average distance : 42.65625\n",
      "\n",
      " Batch average distance : 47.296875\n",
      "\n",
      " Batch average distance : 37.21875\n",
      "\n",
      " Batch average distance : 40.25\n",
      "\n",
      " Batch average distance : 41.53125\n",
      "\n",
      " Batch average distance : 40.40625\n",
      "\n",
      " Batch average distance : 41.453125\n",
      "\n",
      " Batch average distance : 41.375\n",
      "\n",
      " Batch average distance : 43.984375\n",
      "\n",
      " Batch average distance : 40.875\n",
      "\n",
      " Batch average distance : 42.984375\n",
      "\n",
      " Batch average distance : 39.421875\n",
      "\n",
      " Batch average distance : 42.375\n",
      "\n",
      " Batch average distance : 40.078125\n",
      "\n",
      " Batch average distance : 41.1875\n",
      "\n",
      " Batch average distance : 44.166666666666664\n",
      "\n",
      " Epoch average distance : 41.52350813743219\n",
      "*** Training Epoch 16 with teacher forcing 0.4 :***\n",
      "Batch: 1\tTraining loss: 0.6372\n",
      "Batch: 1\tTraining perplexity: 1.8912\n",
      "Batch: 26\tTraining loss: 0.7247\n",
      "Batch: 26\tTraining perplexity: 2.0642\n",
      "Batch: 51\tTraining loss: 0.6228\n",
      "Batch: 51\tTraining perplexity: 1.8642\n",
      "Batch: 76\tTraining loss: 0.5408\n",
      "Batch: 76\tTraining perplexity: 1.7174\n",
      "Batch: 101\tTraining loss: 0.8337\n",
      "Batch: 101\tTraining perplexity: 2.3019\n",
      "Batch: 126\tTraining loss: 0.5493\n",
      "Batch: 126\tTraining perplexity: 1.7321\n",
      "Batch: 151\tTraining loss: 0.6625\n",
      "Batch: 151\tTraining perplexity: 1.9396\n",
      "Batch: 176\tTraining loss: 0.5980\n",
      "Batch: 176\tTraining perplexity: 1.8185\n",
      "Batch: 201\tTraining loss: 0.6329\n",
      "Batch: 201\tTraining perplexity: 1.8830\n",
      "Batch: 226\tTraining loss: 0.6259\n",
      "Batch: 226\tTraining perplexity: 1.8698\n",
      "Batch: 251\tTraining loss: 0.6719\n",
      "Batch: 251\tTraining perplexity: 1.9579\n",
      "Batch: 276\tTraining loss: 0.5913\n",
      "Batch: 276\tTraining perplexity: 1.8064\n",
      "Batch: 301\tTraining loss: 0.5991\n",
      "Batch: 301\tTraining perplexity: 1.8204\n",
      "Batch: 326\tTraining loss: 0.6920\n",
      "Batch: 326\tTraining perplexity: 1.9977\n",
      "Batch: 351\tTraining loss: 0.5516\n",
      "Batch: 351\tTraining perplexity: 1.7360\n",
      "Batch: 376\tTraining loss: 0.5993\n",
      "Batch: 376\tTraining perplexity: 1.8208\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 33.84375\n",
      "\n",
      " Batch average distance : 35.203125\n",
      "\n",
      " Batch average distance : 33.875\n",
      "\n",
      " Batch average distance : 40.609375\n",
      "\n",
      " Batch average distance : 39.140625\n",
      "\n",
      " Batch average distance : 31.84375\n",
      "\n",
      " Batch average distance : 32.359375\n",
      "\n",
      " Batch average distance : 36.40625\n",
      "\n",
      " Batch average distance : 37.640625\n",
      "\n",
      " Batch average distance : 34.3125\n",
      "\n",
      " Batch average distance : 34.21875\n",
      "\n",
      " Batch average distance : 39.375\n",
      "\n",
      " Batch average distance : 37.578125\n",
      "\n",
      " Batch average distance : 35.734375\n",
      "\n",
      " Batch average distance : 33.5\n",
      "\n",
      " Batch average distance : 35.046875\n",
      "\n",
      " Batch average distance : 31.28125\n",
      "\n",
      " Batch average distance : 37.166666666666664\n",
      "\n",
      " Epoch average distance : 35.438517179023506\n",
      "*** Training Epoch 17 with teacher forcing 0.4 :***\n",
      "Batch: 1\tTraining loss: 0.4908\n",
      "Batch: 1\tTraining perplexity: 1.6336\n",
      "Batch: 26\tTraining loss: 0.5793\n",
      "Batch: 26\tTraining perplexity: 1.7848\n",
      "Batch: 51\tTraining loss: 0.5474\n",
      "Batch: 51\tTraining perplexity: 1.7287\n",
      "Batch: 76\tTraining loss: 0.5135\n",
      "Batch: 76\tTraining perplexity: 1.6711\n",
      "Batch: 101\tTraining loss: 0.5292\n",
      "Batch: 101\tTraining perplexity: 1.6976\n",
      "Batch: 126\tTraining loss: 0.5146\n",
      "Batch: 126\tTraining perplexity: 1.6729\n",
      "Batch: 151\tTraining loss: 0.5416\n",
      "Batch: 151\tTraining perplexity: 1.7188\n",
      "Batch: 176\tTraining loss: 0.5052\n",
      "Batch: 176\tTraining perplexity: 1.6573\n",
      "Batch: 201\tTraining loss: 0.4741\n",
      "Batch: 201\tTraining perplexity: 1.6066\n",
      "Batch: 226\tTraining loss: 0.5069\n",
      "Batch: 226\tTraining perplexity: 1.6601\n",
      "Batch: 251\tTraining loss: 0.5929\n",
      "Batch: 251\tTraining perplexity: 1.8093\n",
      "Batch: 276\tTraining loss: 0.5649\n",
      "Batch: 276\tTraining perplexity: 1.7593\n",
      "Batch: 301\tTraining loss: 0.6261\n",
      "Batch: 301\tTraining perplexity: 1.8704\n",
      "Batch: 326\tTraining loss: 0.4807\n",
      "Batch: 326\tTraining perplexity: 1.6172\n",
      "Batch: 351\tTraining loss: 0.4758\n",
      "Batch: 351\tTraining perplexity: 1.6093\n",
      "Batch: 376\tTraining loss: 0.5450\n",
      "Batch: 376\tTraining perplexity: 1.7246\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 34.875\n",
      "\n",
      " Batch average distance : 30.4375\n",
      "\n",
      " Batch average distance : 34.1875\n",
      "\n",
      " Batch average distance : 28.546875\n",
      "\n",
      " Batch average distance : 30.484375\n",
      "\n",
      " Batch average distance : 28.390625\n",
      "\n",
      " Batch average distance : 35.6875\n",
      "\n",
      " Batch average distance : 31.71875\n",
      "\n",
      " Batch average distance : 28.375\n",
      "\n",
      " Batch average distance : 31.890625\n",
      "\n",
      " Batch average distance : 29.546875\n",
      "\n",
      " Batch average distance : 28.296875\n",
      "\n",
      " Batch average distance : 28.921875\n",
      "\n",
      " Batch average distance : 29.40625\n",
      "\n",
      " Batch average distance : 33.4375\n",
      "\n",
      " Batch average distance : 30.390625\n",
      "\n",
      " Batch average distance : 30.6875\n",
      "\n",
      " Batch average distance : 29.0\n",
      "\n",
      " Epoch average distance : 30.867992766726942\n",
      "*** Training Epoch 18 with teacher forcing 0.4 :***\n",
      "Batch: 1\tTraining loss: 0.4899\n",
      "Batch: 1\tTraining perplexity: 1.6321\n",
      "Batch: 26\tTraining loss: 0.4175\n",
      "Batch: 26\tTraining perplexity: 1.5182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 51\tTraining loss: 0.5513\n",
      "Batch: 51\tTraining perplexity: 1.7355\n",
      "Batch: 76\tTraining loss: 0.5269\n",
      "Batch: 76\tTraining perplexity: 1.6937\n",
      "Batch: 101\tTraining loss: 0.4243\n",
      "Batch: 101\tTraining perplexity: 1.5285\n",
      "Batch: 126\tTraining loss: 0.5311\n",
      "Batch: 126\tTraining perplexity: 1.7009\n",
      "Batch: 151\tTraining loss: 0.4519\n",
      "Batch: 151\tTraining perplexity: 1.5713\n",
      "Batch: 176\tTraining loss: 0.4324\n",
      "Batch: 176\tTraining perplexity: 1.5409\n",
      "Batch: 201\tTraining loss: 0.5739\n",
      "Batch: 201\tTraining perplexity: 1.7752\n",
      "Batch: 226\tTraining loss: 0.4738\n",
      "Batch: 226\tTraining perplexity: 1.6060\n",
      "Batch: 251\tTraining loss: 0.5079\n",
      "Batch: 251\tTraining perplexity: 1.6618\n",
      "Batch: 276\tTraining loss: 0.4104\n",
      "Batch: 276\tTraining perplexity: 1.5074\n",
      "Batch: 301\tTraining loss: 0.4564\n",
      "Batch: 301\tTraining perplexity: 1.5783\n",
      "Batch: 326\tTraining loss: 0.5161\n",
      "Batch: 326\tTraining perplexity: 1.6755\n",
      "Batch: 351\tTraining loss: 0.4272\n",
      "Batch: 351\tTraining perplexity: 1.5330\n",
      "Batch: 376\tTraining loss: 0.5888\n",
      "Batch: 376\tTraining perplexity: 1.8018\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 24.53125\n",
      "\n",
      " Batch average distance : 28.171875\n",
      "\n",
      " Batch average distance : 27.21875\n",
      "\n",
      " Batch average distance : 28.6875\n",
      "\n",
      " Batch average distance : 28.53125\n",
      "\n",
      " Batch average distance : 29.25\n",
      "\n",
      " Batch average distance : 27.921875\n",
      "\n",
      " Batch average distance : 24.671875\n",
      "\n",
      " Batch average distance : 32.265625\n",
      "\n",
      " Batch average distance : 23.609375\n",
      "\n",
      " Batch average distance : 26.40625\n",
      "\n",
      " Batch average distance : 27.03125\n",
      "\n",
      " Batch average distance : 29.140625\n",
      "\n",
      " Batch average distance : 27.125\n",
      "\n",
      " Batch average distance : 25.453125\n",
      "\n",
      " Batch average distance : 28.171875\n",
      "\n",
      " Batch average distance : 30.546875\n",
      "\n",
      " Batch average distance : 20.944444444444443\n",
      "\n",
      " Epoch average distance : 27.464737793851718\n",
      "*** Training Epoch 19 with teacher forcing 0.4 :***\n",
      "Batch: 1\tTraining loss: 0.4389\n",
      "Batch: 1\tTraining perplexity: 1.5509\n",
      "Batch: 26\tTraining loss: 0.4242\n",
      "Batch: 26\tTraining perplexity: 1.5284\n",
      "Batch: 51\tTraining loss: 0.4666\n",
      "Batch: 51\tTraining perplexity: 1.5946\n",
      "Batch: 76\tTraining loss: 0.4328\n",
      "Batch: 76\tTraining perplexity: 1.5415\n",
      "Batch: 101\tTraining loss: 0.3999\n",
      "Batch: 101\tTraining perplexity: 1.4917\n",
      "Batch: 126\tTraining loss: 0.3938\n",
      "Batch: 126\tTraining perplexity: 1.4826\n",
      "Batch: 151\tTraining loss: 0.5317\n",
      "Batch: 151\tTraining perplexity: 1.7018\n",
      "Batch: 176\tTraining loss: 0.3760\n",
      "Batch: 176\tTraining perplexity: 1.4565\n",
      "Batch: 201\tTraining loss: 0.3844\n",
      "Batch: 201\tTraining perplexity: 1.4687\n",
      "Batch: 226\tTraining loss: 0.4601\n",
      "Batch: 226\tTraining perplexity: 1.5842\n",
      "Batch: 251\tTraining loss: 0.3979\n",
      "Batch: 251\tTraining perplexity: 1.4887\n",
      "Batch: 276\tTraining loss: 0.4024\n",
      "Batch: 276\tTraining perplexity: 1.4953\n",
      "Batch: 301\tTraining loss: 0.3781\n",
      "Batch: 301\tTraining perplexity: 1.4595\n",
      "Batch: 326\tTraining loss: 0.3919\n",
      "Batch: 326\tTraining perplexity: 1.4798\n",
      "Batch: 351\tTraining loss: 0.3633\n",
      "Batch: 351\tTraining perplexity: 1.4380\n",
      "Batch: 376\tTraining loss: 0.3669\n",
      "Batch: 376\tTraining perplexity: 1.4432\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 22.234375\n",
      "\n",
      " Batch average distance : 28.109375\n",
      "\n",
      " Batch average distance : 26.171875\n",
      "\n",
      " Batch average distance : 28.03125\n",
      "\n",
      " Batch average distance : 21.1875\n",
      "\n",
      " Batch average distance : 27.15625\n",
      "\n",
      " Batch average distance : 27.53125\n",
      "\n",
      " Batch average distance : 22.0\n",
      "\n",
      " Batch average distance : 22.234375\n",
      "\n",
      " Batch average distance : 26.53125\n",
      "\n",
      " Batch average distance : 23.421875\n",
      "\n",
      " Batch average distance : 27.03125\n",
      "\n",
      " Batch average distance : 23.6875\n",
      "\n",
      " Batch average distance : 23.1875\n",
      "\n",
      " Batch average distance : 26.84375\n",
      "\n",
      " Batch average distance : 28.921875\n",
      "\n",
      " Batch average distance : 23.8125\n",
      "\n",
      " Batch average distance : 22.11111111111111\n",
      "\n",
      " Epoch average distance : 25.132007233273058\n",
      "*** Training Epoch 20 with teacher forcing 0.5 :***\n",
      "Batch: 1\tTraining loss: 0.4907\n",
      "Batch: 1\tTraining perplexity: 1.6334\n",
      "Batch: 26\tTraining loss: 0.4547\n",
      "Batch: 26\tTraining perplexity: 1.5758\n",
      "Batch: 51\tTraining loss: 0.5829\n",
      "Batch: 51\tTraining perplexity: 1.7913\n",
      "Batch: 76\tTraining loss: 0.5254\n",
      "Batch: 76\tTraining perplexity: 1.6912\n",
      "Batch: 101\tTraining loss: 0.5450\n",
      "Batch: 101\tTraining perplexity: 1.7246\n",
      "Batch: 126\tTraining loss: 0.4874\n",
      "Batch: 126\tTraining perplexity: 1.6281\n",
      "Batch: 151\tTraining loss: 0.3684\n",
      "Batch: 151\tTraining perplexity: 1.4454\n",
      "Batch: 176\tTraining loss: 0.5068\n",
      "Batch: 176\tTraining perplexity: 1.6600\n",
      "Batch: 201\tTraining loss: 0.4420\n",
      "Batch: 201\tTraining perplexity: 1.5559\n",
      "Batch: 226\tTraining loss: 0.3991\n",
      "Batch: 226\tTraining perplexity: 1.4905\n",
      "Batch: 251\tTraining loss: 0.3916\n",
      "Batch: 251\tTraining perplexity: 1.4794\n",
      "Batch: 276\tTraining loss: 0.4225\n",
      "Batch: 276\tTraining perplexity: 1.5258\n",
      "Batch: 301\tTraining loss: 0.5986\n",
      "Batch: 301\tTraining perplexity: 1.8196\n",
      "Batch: 326\tTraining loss: 0.5004\n",
      "Batch: 326\tTraining perplexity: 1.6495\n",
      "Batch: 351\tTraining loss: 0.4872\n",
      "Batch: 351\tTraining perplexity: 1.6277\n",
      "Batch: 376\tTraining loss: 0.4429\n",
      "Batch: 376\tTraining perplexity: 1.5571\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 20.515625\n",
      "\n",
      " Batch average distance : 21.40625\n",
      "\n",
      " Batch average distance : 24.03125\n",
      "\n",
      " Batch average distance : 22.109375\n",
      "\n",
      " Batch average distance : 20.640625\n",
      "\n",
      " Batch average distance : 21.15625\n",
      "\n",
      " Batch average distance : 24.296875\n",
      "\n",
      " Batch average distance : 19.390625\n",
      "\n",
      " Batch average distance : 27.765625\n",
      "\n",
      " Batch average distance : 23.890625\n",
      "\n",
      " Batch average distance : 23.59375\n",
      "\n",
      " Batch average distance : 22.3125\n",
      "\n",
      " Batch average distance : 24.421875\n",
      "\n",
      " Batch average distance : 24.046875\n",
      "\n",
      " Batch average distance : 24.0\n",
      "\n",
      " Batch average distance : 21.734375\n",
      "\n",
      " Batch average distance : 22.1875\n",
      "\n",
      " Batch average distance : 25.166666666666668\n",
      "\n",
      " Epoch average distance : 22.83273056057866\n",
      "*** Training Epoch 21 with teacher forcing 0.5 :***\n",
      "Batch: 1\tTraining loss: 0.4618\n",
      "Batch: 1\tTraining perplexity: 1.5870\n",
      "Batch: 26\tTraining loss: 0.4528\n",
      "Batch: 26\tTraining perplexity: 1.5727\n",
      "Batch: 51\tTraining loss: 0.3771\n",
      "Batch: 51\tTraining perplexity: 1.4580\n",
      "Batch: 76\tTraining loss: 0.4222\n",
      "Batch: 76\tTraining perplexity: 1.5253\n",
      "Batch: 101\tTraining loss: 0.4974\n",
      "Batch: 101\tTraining perplexity: 1.6445\n",
      "Batch: 126\tTraining loss: 0.3464\n",
      "Batch: 126\tTraining perplexity: 1.4140\n",
      "Batch: 151\tTraining loss: 0.4584\n",
      "Batch: 151\tTraining perplexity: 1.5816\n",
      "Batch: 176\tTraining loss: 0.3838\n",
      "Batch: 176\tTraining perplexity: 1.4679\n",
      "Batch: 201\tTraining loss: 0.5584\n",
      "Batch: 201\tTraining perplexity: 1.7479\n",
      "Batch: 226\tTraining loss: 0.3980\n",
      "Batch: 226\tTraining perplexity: 1.4888\n",
      "Batch: 251\tTraining loss: 0.5519\n",
      "Batch: 251\tTraining perplexity: 1.7366\n",
      "Batch: 276\tTraining loss: 0.4132\n",
      "Batch: 276\tTraining perplexity: 1.5116\n",
      "Batch: 301\tTraining loss: 0.3791\n",
      "Batch: 301\tTraining perplexity: 1.4609\n",
      "Batch: 326\tTraining loss: 0.5016\n",
      "Batch: 326\tTraining perplexity: 1.6513\n",
      "Batch: 351\tTraining loss: 0.3516\n",
      "Batch: 351\tTraining perplexity: 1.4214\n",
      "Batch: 376\tTraining loss: 0.4285\n",
      "Batch: 376\tTraining perplexity: 1.5349\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 25.09375\n",
      "\n",
      " Batch average distance : 18.71875\n",
      "\n",
      " Batch average distance : 22.375\n",
      "\n",
      " Batch average distance : 20.125\n",
      "\n",
      " Batch average distance : 21.53125\n",
      "\n",
      " Batch average distance : 21.953125\n",
      "\n",
      " Batch average distance : 21.828125\n",
      "\n",
      " Batch average distance : 19.765625\n",
      "\n",
      " Batch average distance : 22.484375\n",
      "\n",
      " Batch average distance : 24.890625\n",
      "\n",
      " Batch average distance : 22.859375\n",
      "\n",
      " Batch average distance : 20.46875\n",
      "\n",
      " Batch average distance : 24.421875\n",
      "\n",
      " Batch average distance : 19.921875\n",
      "\n",
      " Batch average distance : 20.625\n",
      "\n",
      " Batch average distance : 19.265625\n",
      "\n",
      " Batch average distance : 19.734375\n",
      "\n",
      " Batch average distance : 20.38888888888889\n",
      "\n",
      " Epoch average distance : 21.514466546112114\n",
      "*** Training Epoch 22 with teacher forcing 0.5 :***\n",
      "Batch: 1\tTraining loss: 0.4669\n",
      "Batch: 1\tTraining perplexity: 1.5951\n",
      "Batch: 26\tTraining loss: 0.4457\n",
      "Batch: 26\tTraining perplexity: 1.5616\n",
      "Batch: 51\tTraining loss: 0.4335\n",
      "Batch: 51\tTraining perplexity: 1.5427\n",
      "Batch: 76\tTraining loss: 0.4325\n",
      "Batch: 76\tTraining perplexity: 1.5411\n",
      "Batch: 101\tTraining loss: 0.3582\n",
      "Batch: 101\tTraining perplexity: 1.4308\n",
      "Batch: 126\tTraining loss: 0.4348\n",
      "Batch: 126\tTraining perplexity: 1.5447\n",
      "Batch: 151\tTraining loss: 0.3205\n",
      "Batch: 151\tTraining perplexity: 1.3778\n",
      "Batch: 176\tTraining loss: 0.3747\n",
      "Batch: 176\tTraining perplexity: 1.4545\n",
      "Batch: 201\tTraining loss: 0.3873\n",
      "Batch: 201\tTraining perplexity: 1.4731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 226\tTraining loss: 0.3945\n",
      "Batch: 226\tTraining perplexity: 1.4836\n",
      "Batch: 251\tTraining loss: 0.4524\n",
      "Batch: 251\tTraining perplexity: 1.5721\n",
      "Batch: 276\tTraining loss: 0.3727\n",
      "Batch: 276\tTraining perplexity: 1.4517\n",
      "Batch: 301\tTraining loss: 0.4107\n",
      "Batch: 301\tTraining perplexity: 1.5078\n",
      "Batch: 326\tTraining loss: 0.4042\n",
      "Batch: 326\tTraining perplexity: 1.4981\n",
      "Batch: 351\tTraining loss: 0.4985\n",
      "Batch: 351\tTraining perplexity: 1.6462\n",
      "Batch: 376\tTraining loss: 0.4475\n",
      "Batch: 376\tTraining perplexity: 1.5644\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 18.609375\n",
      "\n",
      " Batch average distance : 20.171875\n",
      "\n",
      " Batch average distance : 19.875\n",
      "\n",
      " Batch average distance : 18.109375\n",
      "\n",
      " Batch average distance : 23.515625\n",
      "\n",
      " Batch average distance : 20.15625\n",
      "\n",
      " Batch average distance : 17.390625\n",
      "\n",
      " Batch average distance : 21.859375\n",
      "\n",
      " Batch average distance : 22.65625\n",
      "\n",
      " Batch average distance : 22.109375\n",
      "\n",
      " Batch average distance : 20.046875\n",
      "\n",
      " Batch average distance : 25.875\n",
      "\n",
      " Batch average distance : 20.078125\n",
      "\n",
      " Batch average distance : 18.4375\n",
      "\n",
      " Batch average distance : 19.015625\n",
      "\n",
      " Batch average distance : 19.125\n",
      "\n",
      " Batch average distance : 22.0\n",
      "\n",
      " Batch average distance : 21.333333333333332\n",
      "\n",
      " Epoch average distance : 20.544303797468356\n",
      "*** Training Epoch 23 with teacher forcing 0.5 :***\n",
      "Batch: 1\tTraining loss: 0.3551\n",
      "Batch: 1\tTraining perplexity: 1.4263\n",
      "Batch: 26\tTraining loss: 0.4483\n",
      "Batch: 26\tTraining perplexity: 1.5656\n",
      "Batch: 51\tTraining loss: 0.4351\n",
      "Batch: 51\tTraining perplexity: 1.5452\n",
      "Batch: 76\tTraining loss: 0.4348\n",
      "Batch: 76\tTraining perplexity: 1.5447\n",
      "Batch: 101\tTraining loss: 0.3398\n",
      "Batch: 101\tTraining perplexity: 1.4047\n",
      "Batch: 126\tTraining loss: 0.4874\n",
      "Batch: 126\tTraining perplexity: 1.6280\n",
      "Batch: 151\tTraining loss: 0.3729\n",
      "Batch: 151\tTraining perplexity: 1.4520\n",
      "Batch: 176\tTraining loss: 0.3870\n",
      "Batch: 176\tTraining perplexity: 1.4725\n",
      "Batch: 201\tTraining loss: 0.3953\n",
      "Batch: 201\tTraining perplexity: 1.4849\n",
      "Batch: 226\tTraining loss: 0.3227\n",
      "Batch: 226\tTraining perplexity: 1.3809\n",
      "Batch: 251\tTraining loss: 0.3332\n",
      "Batch: 251\tTraining perplexity: 1.3954\n",
      "Batch: 276\tTraining loss: 0.3872\n",
      "Batch: 276\tTraining perplexity: 1.4728\n",
      "Batch: 301\tTraining loss: 0.3168\n",
      "Batch: 301\tTraining perplexity: 1.3727\n",
      "Batch: 326\tTraining loss: 0.3057\n",
      "Batch: 326\tTraining perplexity: 1.3576\n",
      "Batch: 351\tTraining loss: 0.3204\n",
      "Batch: 351\tTraining perplexity: 1.3777\n",
      "Batch: 376\tTraining loss: 0.3559\n",
      "Batch: 376\tTraining perplexity: 1.4275\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 20.5\n",
      "\n",
      " Batch average distance : 19.171875\n",
      "\n",
      " Batch average distance : 20.171875\n",
      "\n",
      " Batch average distance : 19.515625\n",
      "\n",
      " Batch average distance : 19.578125\n",
      "\n",
      " Batch average distance : 23.0625\n",
      "\n",
      " Batch average distance : 20.046875\n",
      "\n",
      " Batch average distance : 16.984375\n",
      "\n",
      " Batch average distance : 17.34375\n",
      "\n",
      " Batch average distance : 20.875\n",
      "\n",
      " Batch average distance : 19.0625\n",
      "\n",
      " Batch average distance : 19.921875\n",
      "\n",
      " Batch average distance : 20.96875\n",
      "\n",
      " Batch average distance : 21.640625\n",
      "\n",
      " Batch average distance : 19.96875\n",
      "\n",
      " Batch average distance : 18.828125\n",
      "\n",
      " Batch average distance : 18.9375\n",
      "\n",
      " Batch average distance : 17.444444444444443\n",
      "\n",
      " Epoch average distance : 19.760397830018082\n",
      "*** Training Epoch 24 with teacher forcing 0.5 :***\n",
      "Batch: 1\tTraining loss: 0.3675\n",
      "Batch: 1\tTraining perplexity: 1.4441\n",
      "Batch: 26\tTraining loss: 0.3180\n",
      "Batch: 26\tTraining perplexity: 1.3744\n",
      "Batch: 51\tTraining loss: 0.2519\n",
      "Batch: 51\tTraining perplexity: 1.2865\n",
      "Batch: 76\tTraining loss: 0.3428\n",
      "Batch: 76\tTraining perplexity: 1.4089\n",
      "Batch: 101\tTraining loss: 0.3914\n",
      "Batch: 101\tTraining perplexity: 1.4790\n",
      "Batch: 126\tTraining loss: 0.3325\n",
      "Batch: 126\tTraining perplexity: 1.3945\n",
      "Batch: 151\tTraining loss: 0.3340\n",
      "Batch: 151\tTraining perplexity: 1.3966\n",
      "Batch: 176\tTraining loss: 0.2947\n",
      "Batch: 176\tTraining perplexity: 1.3427\n",
      "Batch: 201\tTraining loss: 0.4926\n",
      "Batch: 201\tTraining perplexity: 1.6366\n",
      "Batch: 226\tTraining loss: 0.3577\n",
      "Batch: 226\tTraining perplexity: 1.4300\n",
      "Batch: 251\tTraining loss: 0.4049\n",
      "Batch: 251\tTraining perplexity: 1.4992\n",
      "Batch: 276\tTraining loss: 0.3504\n",
      "Batch: 276\tTraining perplexity: 1.4196\n",
      "Batch: 301\tTraining loss: 0.3113\n",
      "Batch: 301\tTraining perplexity: 1.3651\n",
      "Batch: 326\tTraining loss: 0.3087\n",
      "Batch: 326\tTraining perplexity: 1.3616\n",
      "Batch: 351\tTraining loss: 0.3726\n",
      "Batch: 351\tTraining perplexity: 1.4515\n",
      "Batch: 376\tTraining loss: 0.2682\n",
      "Batch: 376\tTraining perplexity: 1.3077\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 19.265625\n",
      "\n",
      " Batch average distance : 20.4375\n",
      "\n",
      " Batch average distance : 15.21875\n",
      "\n",
      " Batch average distance : 18.921875\n",
      "\n",
      " Batch average distance : 21.40625\n",
      "\n",
      " Batch average distance : 19.171875\n",
      "\n",
      " Batch average distance : 18.8125\n",
      "\n",
      " Batch average distance : 20.875\n",
      "\n",
      " Batch average distance : 14.21875\n",
      "\n",
      " Batch average distance : 21.5625\n",
      "\n",
      " Batch average distance : 16.015625\n",
      "\n",
      " Batch average distance : 17.953125\n",
      "\n",
      " Batch average distance : 18.21875\n",
      "\n",
      " Batch average distance : 15.640625\n",
      "\n",
      " Batch average distance : 19.078125\n",
      "\n",
      " Batch average distance : 18.796875\n",
      "\n",
      " Batch average distance : 18.40625\n",
      "\n",
      " Batch average distance : 14.5\n",
      "\n",
      " Epoch average distance : 18.40596745027125\n",
      "*** Training Epoch 25 with teacher forcing 0.5 :***\n",
      "Batch: 1\tTraining loss: 0.3503\n",
      "Batch: 1\tTraining perplexity: 1.4196\n",
      "Batch: 26\tTraining loss: 0.2754\n",
      "Batch: 26\tTraining perplexity: 1.3171\n",
      "Batch: 51\tTraining loss: 0.3803\n",
      "Batch: 51\tTraining perplexity: 1.4628\n",
      "Batch: 76\tTraining loss: 0.2785\n",
      "Batch: 76\tTraining perplexity: 1.3212\n",
      "Batch: 101\tTraining loss: 0.4585\n",
      "Batch: 101\tTraining perplexity: 1.5817\n",
      "Batch: 126\tTraining loss: 0.4239\n",
      "Batch: 126\tTraining perplexity: 1.5279\n",
      "Batch: 151\tTraining loss: 0.3586\n",
      "Batch: 151\tTraining perplexity: 1.4314\n",
      "Batch: 176\tTraining loss: 0.3406\n",
      "Batch: 176\tTraining perplexity: 1.4057\n",
      "Batch: 201\tTraining loss: 0.2667\n",
      "Batch: 201\tTraining perplexity: 1.3056\n",
      "Batch: 226\tTraining loss: 0.3724\n",
      "Batch: 226\tTraining perplexity: 1.4512\n",
      "Batch: 251\tTraining loss: 0.2714\n",
      "Batch: 251\tTraining perplexity: 1.3117\n",
      "Batch: 276\tTraining loss: 0.3349\n",
      "Batch: 276\tTraining perplexity: 1.3978\n",
      "Batch: 301\tTraining loss: 0.3430\n",
      "Batch: 301\tTraining perplexity: 1.4091\n",
      "Batch: 326\tTraining loss: 0.2981\n",
      "Batch: 326\tTraining perplexity: 1.3473\n",
      "Batch: 351\tTraining loss: 0.2986\n",
      "Batch: 351\tTraining perplexity: 1.3480\n",
      "Batch: 376\tTraining loss: 0.3154\n",
      "Batch: 376\tTraining perplexity: 1.3709\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 15.5\n",
      "\n",
      " Batch average distance : 20.46875\n",
      "\n",
      " Batch average distance : 17.953125\n",
      "\n",
      " Batch average distance : 16.296875\n",
      "\n",
      " Batch average distance : 15.734375\n",
      "\n",
      " Batch average distance : 18.390625\n",
      "\n",
      " Batch average distance : 16.578125\n",
      "\n",
      " Batch average distance : 14.953125\n",
      "\n",
      " Batch average distance : 20.375\n",
      "\n",
      " Batch average distance : 16.03125\n",
      "\n",
      " Batch average distance : 15.609375\n",
      "\n",
      " Batch average distance : 18.578125\n",
      "\n",
      " Batch average distance : 15.890625\n",
      "\n",
      " Batch average distance : 16.609375\n",
      "\n",
      " Batch average distance : 20.421875\n",
      "\n",
      " Batch average distance : 17.5625\n",
      "\n",
      " Batch average distance : 20.859375\n",
      "\n",
      " Batch average distance : 13.055555555555555\n",
      "\n",
      " Epoch average distance : 17.445750452079565\n",
      "*** Training Epoch 26 with teacher forcing 0.5 :***\n",
      "Batch: 1\tTraining loss: 0.4170\n",
      "Batch: 1\tTraining perplexity: 1.5174\n",
      "Batch: 26\tTraining loss: 0.2843\n",
      "Batch: 26\tTraining perplexity: 1.3289\n",
      "Batch: 51\tTraining loss: 0.3116\n",
      "Batch: 51\tTraining perplexity: 1.3656\n",
      "Batch: 76\tTraining loss: 0.3330\n",
      "Batch: 76\tTraining perplexity: 1.3951\n",
      "Batch: 101\tTraining loss: 0.3045\n",
      "Batch: 101\tTraining perplexity: 1.3560\n",
      "Batch: 126\tTraining loss: 0.3073\n",
      "Batch: 126\tTraining perplexity: 1.3598\n",
      "Batch: 151\tTraining loss: 0.3472\n",
      "Batch: 151\tTraining perplexity: 1.4151\n",
      "Batch: 176\tTraining loss: 0.3296\n",
      "Batch: 176\tTraining perplexity: 1.3904\n",
      "Batch: 201\tTraining loss: 0.2540\n",
      "Batch: 201\tTraining perplexity: 1.2891\n",
      "Batch: 226\tTraining loss: 0.3292\n",
      "Batch: 226\tTraining perplexity: 1.3898\n",
      "Batch: 251\tTraining loss: 0.3347\n",
      "Batch: 251\tTraining perplexity: 1.3975\n",
      "Batch: 276\tTraining loss: 0.2939\n",
      "Batch: 276\tTraining perplexity: 1.3417\n",
      "Batch: 301\tTraining loss: 0.2713\n",
      "Batch: 301\tTraining perplexity: 1.3117\n",
      "Batch: 326\tTraining loss: 0.3688\n",
      "Batch: 326\tTraining perplexity: 1.4460\n",
      "Batch: 351\tTraining loss: 0.3594\n",
      "Batch: 351\tTraining perplexity: 1.4324\n",
      "Batch: 376\tTraining loss: 0.2808\n",
      "Batch: 376\tTraining perplexity: 1.3242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 18.390625\n",
      "\n",
      " Batch average distance : 19.015625\n",
      "\n",
      " Batch average distance : 18.546875\n",
      "\n",
      " Batch average distance : 17.046875\n",
      "\n",
      " Batch average distance : 20.375\n",
      "\n",
      " Batch average distance : 17.265625\n",
      "\n",
      " Batch average distance : 14.671875\n",
      "\n",
      " Batch average distance : 19.890625\n",
      "\n",
      " Batch average distance : 19.03125\n",
      "\n",
      " Batch average distance : 17.265625\n",
      "\n",
      " Batch average distance : 16.8125\n",
      "\n",
      " Batch average distance : 15.8125\n",
      "\n",
      " Batch average distance : 15.4375\n",
      "\n",
      " Batch average distance : 17.375\n",
      "\n",
      " Batch average distance : 16.859375\n",
      "\n",
      " Batch average distance : 17.609375\n",
      "\n",
      " Batch average distance : 13.53125\n",
      "\n",
      " Batch average distance : 14.277777777777779\n",
      "\n",
      " Epoch average distance : 17.299276672694393\n",
      "*** Training Epoch 27 with teacher forcing 0.5 :***\n",
      "Batch: 1\tTraining loss: 0.3120\n",
      "Batch: 1\tTraining perplexity: 1.3662\n",
      "Batch: 26\tTraining loss: 0.2819\n",
      "Batch: 26\tTraining perplexity: 1.3257\n",
      "Batch: 51\tTraining loss: 0.4038\n",
      "Batch: 51\tTraining perplexity: 1.4976\n",
      "Batch: 76\tTraining loss: 0.2511\n",
      "Batch: 76\tTraining perplexity: 1.2855\n",
      "Batch: 101\tTraining loss: 0.3825\n",
      "Batch: 101\tTraining perplexity: 1.4660\n",
      "Batch: 126\tTraining loss: 0.2933\n",
      "Batch: 126\tTraining perplexity: 1.3409\n",
      "Batch: 151\tTraining loss: 0.2750\n",
      "Batch: 151\tTraining perplexity: 1.3166\n",
      "Batch: 176\tTraining loss: 0.2103\n",
      "Batch: 176\tTraining perplexity: 1.2340\n",
      "Batch: 201\tTraining loss: 0.2976\n",
      "Batch: 201\tTraining perplexity: 1.3467\n",
      "Batch: 226\tTraining loss: 0.2762\n",
      "Batch: 226\tTraining perplexity: 1.3181\n",
      "Batch: 251\tTraining loss: 0.2666\n",
      "Batch: 251\tTraining perplexity: 1.3056\n",
      "Batch: 276\tTraining loss: 0.2902\n",
      "Batch: 276\tTraining perplexity: 1.3367\n",
      "Batch: 301\tTraining loss: 0.3218\n",
      "Batch: 301\tTraining perplexity: 1.3796\n",
      "Batch: 326\tTraining loss: 0.3438\n",
      "Batch: 326\tTraining perplexity: 1.4104\n",
      "Batch: 351\tTraining loss: 0.2935\n",
      "Batch: 351\tTraining perplexity: 1.3411\n",
      "Batch: 376\tTraining loss: 0.2818\n",
      "Batch: 376\tTraining perplexity: 1.3255\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 16.59375\n",
      "\n",
      " Batch average distance : 14.484375\n",
      "\n",
      " Batch average distance : 14.921875\n",
      "\n",
      " Batch average distance : 20.859375\n",
      "\n",
      " Batch average distance : 16.265625\n",
      "\n",
      " Batch average distance : 15.28125\n",
      "\n",
      " Batch average distance : 15.546875\n",
      "\n",
      " Batch average distance : 17.859375\n",
      "\n",
      " Batch average distance : 17.296875\n",
      "\n",
      " Batch average distance : 13.96875\n",
      "\n",
      " Batch average distance : 16.3125\n",
      "\n",
      " Batch average distance : 18.5\n",
      "\n",
      " Batch average distance : 14.25\n",
      "\n",
      " Batch average distance : 16.359375\n",
      "\n",
      " Batch average distance : 19.078125\n",
      "\n",
      " Batch average distance : 16.609375\n",
      "\n",
      " Batch average distance : 17.34375\n",
      "\n",
      " Batch average distance : 13.944444444444445\n",
      "\n",
      " Epoch average distance : 16.518083182640144\n",
      "*** Training Epoch 28 with teacher forcing 0.5 :***\n",
      "Batch: 1\tTraining loss: 0.2748\n",
      "Batch: 1\tTraining perplexity: 1.3163\n",
      "Batch: 26\tTraining loss: 0.3098\n",
      "Batch: 26\tTraining perplexity: 1.3632\n",
      "Batch: 51\tTraining loss: 0.2435\n",
      "Batch: 51\tTraining perplexity: 1.2757\n",
      "Batch: 76\tTraining loss: 0.2225\n",
      "Batch: 76\tTraining perplexity: 1.2492\n",
      "Batch: 101\tTraining loss: 0.2670\n",
      "Batch: 101\tTraining perplexity: 1.3061\n",
      "Batch: 126\tTraining loss: 0.3436\n",
      "Batch: 126\tTraining perplexity: 1.4101\n",
      "Batch: 151\tTraining loss: 0.2855\n",
      "Batch: 151\tTraining perplexity: 1.3305\n",
      "Batch: 176\tTraining loss: 0.2700\n",
      "Batch: 176\tTraining perplexity: 1.3100\n",
      "Batch: 201\tTraining loss: 0.2726\n",
      "Batch: 201\tTraining perplexity: 1.3134\n",
      "Batch: 226\tTraining loss: 0.2626\n",
      "Batch: 226\tTraining perplexity: 1.3004\n",
      "Batch: 251\tTraining loss: 0.2956\n",
      "Batch: 251\tTraining perplexity: 1.3439\n",
      "Batch: 276\tTraining loss: 0.3146\n",
      "Batch: 276\tTraining perplexity: 1.3697\n",
      "Batch: 301\tTraining loss: 0.2734\n",
      "Batch: 301\tTraining perplexity: 1.3144\n",
      "Batch: 326\tTraining loss: 0.2676\n",
      "Batch: 326\tTraining perplexity: 1.3068\n",
      "Batch: 351\tTraining loss: 0.2695\n",
      "Batch: 351\tTraining perplexity: 1.3094\n",
      "Batch: 376\tTraining loss: 0.2698\n",
      "Batch: 376\tTraining perplexity: 1.3097\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 17.46875\n",
      "\n",
      " Batch average distance : 16.0625\n",
      "\n",
      " Batch average distance : 17.515625\n",
      "\n",
      " Batch average distance : 16.671875\n",
      "\n",
      " Batch average distance : 16.0625\n",
      "\n",
      " Batch average distance : 13.765625\n",
      "\n",
      " Batch average distance : 17.359375\n",
      "\n",
      " Batch average distance : 14.609375\n",
      "\n",
      " Batch average distance : 15.8125\n",
      "\n",
      " Batch average distance : 17.15625\n",
      "\n",
      " Batch average distance : 14.5625\n",
      "\n",
      " Batch average distance : 14.765625\n",
      "\n",
      " Batch average distance : 16.828125\n",
      "\n",
      " Batch average distance : 19.1875\n",
      "\n",
      " Batch average distance : 15.421875\n",
      "\n",
      " Batch average distance : 13.171875\n",
      "\n",
      " Batch average distance : 14.359375\n",
      "\n",
      " Batch average distance : 16.944444444444443\n",
      "\n",
      " Epoch average distance : 15.94484629294756\n",
      "*** Training Epoch 29 with teacher forcing 0.5 :***\n",
      "Batch: 1\tTraining loss: 0.2625\n",
      "Batch: 1\tTraining perplexity: 1.3001\n",
      "Batch: 26\tTraining loss: 0.2510\n",
      "Batch: 26\tTraining perplexity: 1.2854\n",
      "Batch: 51\tTraining loss: 0.3705\n",
      "Batch: 51\tTraining perplexity: 1.4485\n",
      "Batch: 76\tTraining loss: 0.3235\n",
      "Batch: 76\tTraining perplexity: 1.3820\n",
      "Batch: 101\tTraining loss: 0.2667\n",
      "Batch: 101\tTraining perplexity: 1.3057\n",
      "Batch: 126\tTraining loss: 0.2848\n",
      "Batch: 126\tTraining perplexity: 1.3295\n",
      "Batch: 151\tTraining loss: 0.3186\n",
      "Batch: 151\tTraining perplexity: 1.3752\n",
      "Batch: 176\tTraining loss: 0.2778\n",
      "Batch: 176\tTraining perplexity: 1.3202\n",
      "Batch: 201\tTraining loss: 0.2624\n",
      "Batch: 201\tTraining perplexity: 1.3001\n",
      "Batch: 226\tTraining loss: 0.2465\n",
      "Batch: 226\tTraining perplexity: 1.2795\n",
      "Batch: 251\tTraining loss: 0.2433\n",
      "Batch: 251\tTraining perplexity: 1.2755\n",
      "Batch: 276\tTraining loss: 0.2576\n",
      "Batch: 276\tTraining perplexity: 1.2938\n",
      "Batch: 301\tTraining loss: 0.2235\n",
      "Batch: 301\tTraining perplexity: 1.2504\n",
      "Batch: 326\tTraining loss: 0.2727\n",
      "Batch: 326\tTraining perplexity: 1.3135\n",
      "Batch: 351\tTraining loss: 0.2699\n",
      "Batch: 351\tTraining perplexity: 1.3099\n",
      "Batch: 376\tTraining loss: 0.2204\n",
      "Batch: 376\tTraining perplexity: 1.2466\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 13.625\n",
      "\n",
      " Batch average distance : 16.84375\n",
      "\n",
      " Batch average distance : 15.40625\n",
      "\n",
      " Batch average distance : 15.015625\n",
      "\n",
      " Batch average distance : 19.125\n",
      "\n",
      " Batch average distance : 18.765625\n",
      "\n",
      " Batch average distance : 16.015625\n",
      "\n",
      " Batch average distance : 12.484375\n",
      "\n",
      " Batch average distance : 18.125\n",
      "\n",
      " Batch average distance : 16.46875\n",
      "\n",
      " Batch average distance : 14.609375\n",
      "\n",
      " Batch average distance : 17.984375\n",
      "\n",
      " Batch average distance : 16.0625\n",
      "\n",
      " Batch average distance : 20.125\n",
      "\n",
      " Batch average distance : 15.0\n",
      "\n",
      " Batch average distance : 14.046875\n",
      "\n",
      " Batch average distance : 13.234375\n",
      "\n",
      " Batch average distance : 15.666666666666666\n",
      "\n",
      " Epoch average distance : 16.04882459312839\n",
      "*** Training Epoch 30 with teacher forcing 0.5 :***\n",
      "Batch: 1\tTraining loss: 0.2075\n",
      "Batch: 1\tTraining perplexity: 1.2306\n",
      "Batch: 26\tTraining loss: 0.3655\n",
      "Batch: 26\tTraining perplexity: 1.4413\n",
      "Batch: 51\tTraining loss: 0.3512\n",
      "Batch: 51\tTraining perplexity: 1.4208\n",
      "Batch: 76\tTraining loss: 0.2479\n",
      "Batch: 76\tTraining perplexity: 1.2813\n",
      "Batch: 101\tTraining loss: 0.2327\n",
      "Batch: 101\tTraining perplexity: 1.2620\n",
      "Batch: 126\tTraining loss: 0.2403\n",
      "Batch: 126\tTraining perplexity: 1.2716\n",
      "Batch: 151\tTraining loss: 0.3572\n",
      "Batch: 151\tTraining perplexity: 1.4294\n",
      "Batch: 176\tTraining loss: 0.2094\n",
      "Batch: 176\tTraining perplexity: 1.2329\n",
      "Batch: 201\tTraining loss: 0.1985\n",
      "Batch: 201\tTraining perplexity: 1.2196\n",
      "Batch: 226\tTraining loss: 0.2278\n",
      "Batch: 226\tTraining perplexity: 1.2558\n",
      "Batch: 251\tTraining loss: 0.2415\n",
      "Batch: 251\tTraining perplexity: 1.2732\n",
      "Batch: 276\tTraining loss: 0.2895\n",
      "Batch: 276\tTraining perplexity: 1.3358\n",
      "Batch: 301\tTraining loss: 0.2161\n",
      "Batch: 301\tTraining perplexity: 1.2412\n",
      "Batch: 326\tTraining loss: 0.2716\n",
      "Batch: 326\tTraining perplexity: 1.3121\n",
      "Batch: 351\tTraining loss: 0.3538\n",
      "Batch: 351\tTraining perplexity: 1.4245\n",
      "Batch: 376\tTraining loss: 0.2570\n",
      "Batch: 376\tTraining perplexity: 1.2930\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 16.359375\n",
      "\n",
      " Batch average distance : 18.296875\n",
      "\n",
      " Batch average distance : 12.765625\n",
      "\n",
      " Batch average distance : 19.234375\n",
      "\n",
      " Batch average distance : 14.046875\n",
      "\n",
      " Batch average distance : 14.6875\n",
      "\n",
      " Batch average distance : 13.203125\n",
      "\n",
      " Batch average distance : 19.78125\n",
      "\n",
      " Batch average distance : 19.28125\n",
      "\n",
      " Batch average distance : 15.296875\n",
      "\n",
      " Batch average distance : 14.296875\n",
      "\n",
      " Batch average distance : 13.296875\n",
      "\n",
      " Batch average distance : 15.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Batch average distance : 14.859375\n",
      "\n",
      " Batch average distance : 15.15625\n",
      "\n",
      " Batch average distance : 18.484375\n",
      "\n",
      " Batch average distance : 13.8125\n",
      "\n",
      " Batch average distance : 11.555555555555555\n",
      "\n",
      " Epoch average distance : 15.742314647377938\n",
      "*** Training Epoch 31 with teacher forcing 0.5 :***\n",
      "Batch: 1\tTraining loss: 0.2080\n",
      "Batch: 1\tTraining perplexity: 1.2312\n",
      "Batch: 26\tTraining loss: 0.2018\n",
      "Batch: 26\tTraining perplexity: 1.2236\n",
      "Batch: 51\tTraining loss: 0.2088\n",
      "Batch: 51\tTraining perplexity: 1.2322\n",
      "Batch: 76\tTraining loss: 0.2185\n",
      "Batch: 76\tTraining perplexity: 1.2442\n",
      "Batch: 101\tTraining loss: 0.1883\n",
      "Batch: 101\tTraining perplexity: 1.2072\n",
      "Batch: 126\tTraining loss: 0.2183\n",
      "Batch: 126\tTraining perplexity: 1.2439\n",
      "Batch: 151\tTraining loss: 0.1951\n",
      "Batch: 151\tTraining perplexity: 1.2154\n",
      "Batch: 176\tTraining loss: 0.2463\n",
      "Batch: 176\tTraining perplexity: 1.2793\n",
      "Batch: 201\tTraining loss: 0.2224\n",
      "Batch: 201\tTraining perplexity: 1.2491\n",
      "Batch: 226\tTraining loss: 0.2598\n",
      "Batch: 226\tTraining perplexity: 1.2967\n",
      "Batch: 251\tTraining loss: 0.3466\n",
      "Batch: 251\tTraining perplexity: 1.4142\n",
      "Batch: 276\tTraining loss: 0.2361\n",
      "Batch: 276\tTraining perplexity: 1.2662\n",
      "Batch: 301\tTraining loss: 0.2436\n",
      "Batch: 301\tTraining perplexity: 1.2759\n",
      "Batch: 326\tTraining loss: 0.2886\n",
      "Batch: 326\tTraining perplexity: 1.3345\n",
      "Batch: 351\tTraining loss: 0.2631\n",
      "Batch: 351\tTraining perplexity: 1.3010\n",
      "Batch: 376\tTraining loss: 0.3054\n",
      "Batch: 376\tTraining perplexity: 1.3571\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 13.0\n",
      "\n",
      " Batch average distance : 14.609375\n",
      "\n",
      " Batch average distance : 15.546875\n",
      "\n",
      " Batch average distance : 15.03125\n",
      "\n",
      " Batch average distance : 15.921875\n",
      "\n",
      " Batch average distance : 13.890625\n",
      "\n",
      " Batch average distance : 16.296875\n",
      "\n",
      " Batch average distance : 14.234375\n",
      "\n",
      " Batch average distance : 15.078125\n",
      "\n",
      " Batch average distance : 15.53125\n",
      "\n",
      " Batch average distance : 13.796875\n",
      "\n",
      " Batch average distance : 15.234375\n",
      "\n",
      " Batch average distance : 17.25\n",
      "\n",
      " Batch average distance : 16.421875\n",
      "\n",
      " Batch average distance : 15.34375\n",
      "\n",
      " Batch average distance : 13.15625\n",
      "\n",
      " Batch average distance : 11.859375\n",
      "\n",
      " Batch average distance : 23.27777777777778\n",
      "\n",
      " Epoch average distance : 14.972875226039783\n",
      "*** Training Epoch 32 with teacher forcing 0.5 :***\n",
      "Batch: 1\tTraining loss: 0.2008\n",
      "Batch: 1\tTraining perplexity: 1.2224\n",
      "Batch: 26\tTraining loss: 0.2437\n",
      "Batch: 26\tTraining perplexity: 1.2760\n",
      "Batch: 51\tTraining loss: 0.2431\n",
      "Batch: 51\tTraining perplexity: 1.2752\n",
      "Batch: 76\tTraining loss: 0.2581\n",
      "Batch: 76\tTraining perplexity: 1.2944\n",
      "Batch: 101\tTraining loss: 0.2948\n",
      "Batch: 101\tTraining perplexity: 1.3429\n",
      "Batch: 126\tTraining loss: 0.2088\n",
      "Batch: 126\tTraining perplexity: 1.2322\n",
      "Batch: 151\tTraining loss: 0.1892\n",
      "Batch: 151\tTraining perplexity: 1.2082\n",
      "Batch: 176\tTraining loss: 0.2384\n",
      "Batch: 176\tTraining perplexity: 1.2692\n",
      "Batch: 201\tTraining loss: 0.2055\n",
      "Batch: 201\tTraining perplexity: 1.2281\n",
      "Batch: 226\tTraining loss: 0.1977\n",
      "Batch: 226\tTraining perplexity: 1.2186\n",
      "Batch: 251\tTraining loss: 0.3076\n",
      "Batch: 251\tTraining perplexity: 1.3602\n",
      "Batch: 276\tTraining loss: 0.1892\n",
      "Batch: 276\tTraining perplexity: 1.2083\n",
      "Batch: 301\tTraining loss: 0.2457\n",
      "Batch: 301\tTraining perplexity: 1.2785\n",
      "Batch: 326\tTraining loss: 0.2110\n",
      "Batch: 326\tTraining perplexity: 1.2349\n",
      "Batch: 351\tTraining loss: 0.2516\n",
      "Batch: 351\tTraining perplexity: 1.2861\n",
      "Batch: 376\tTraining loss: 0.2266\n",
      "Batch: 376\tTraining perplexity: 1.2543\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 16.546875\n",
      "\n",
      " Batch average distance : 12.96875\n",
      "\n",
      " Batch average distance : 16.765625\n",
      "\n",
      " Batch average distance : 11.78125\n",
      "\n",
      " Batch average distance : 16.09375\n",
      "\n",
      " Batch average distance : 14.3125\n",
      "\n",
      " Batch average distance : 16.5\n",
      "\n",
      " Batch average distance : 16.453125\n",
      "\n",
      " Batch average distance : 13.0625\n",
      "\n",
      " Batch average distance : 17.015625\n",
      "\n",
      " Batch average distance : 16.09375\n",
      "\n",
      " Batch average distance : 14.96875\n",
      "\n",
      " Batch average distance : 16.125\n",
      "\n",
      " Batch average distance : 14.96875\n",
      "\n",
      " Batch average distance : 11.9375\n",
      "\n",
      " Batch average distance : 14.1875\n",
      "\n",
      " Batch average distance : 17.6875\n",
      "\n",
      " Batch average distance : 8.38888888888889\n",
      "\n",
      " Epoch average distance : 15.035262206148282\n",
      "*** Training Epoch 33 with teacher forcing 0.5 :***\n",
      "Batch: 1\tTraining loss: 0.2222\n",
      "Batch: 1\tTraining perplexity: 1.2488\n",
      "Batch: 26\tTraining loss: 0.2744\n",
      "Batch: 26\tTraining perplexity: 1.3158\n",
      "Batch: 51\tTraining loss: 0.2594\n",
      "Batch: 51\tTraining perplexity: 1.2962\n",
      "Batch: 76\tTraining loss: 0.2233\n",
      "Batch: 76\tTraining perplexity: 1.2502\n",
      "Batch: 101\tTraining loss: 0.2141\n",
      "Batch: 101\tTraining perplexity: 1.2387\n",
      "Batch: 126\tTraining loss: 0.2345\n",
      "Batch: 126\tTraining perplexity: 1.2643\n",
      "Batch: 151\tTraining loss: 0.2235\n",
      "Batch: 151\tTraining perplexity: 1.2505\n",
      "Batch: 176\tTraining loss: 0.1867\n",
      "Batch: 176\tTraining perplexity: 1.2053\n",
      "Batch: 201\tTraining loss: 0.2319\n",
      "Batch: 201\tTraining perplexity: 1.2609\n",
      "Batch: 226\tTraining loss: 0.2319\n",
      "Batch: 226\tTraining perplexity: 1.2610\n",
      "Batch: 251\tTraining loss: 0.1725\n",
      "Batch: 251\tTraining perplexity: 1.1882\n",
      "Batch: 276\tTraining loss: 0.2037\n",
      "Batch: 276\tTraining perplexity: 1.2259\n",
      "Batch: 301\tTraining loss: 0.2542\n",
      "Batch: 301\tTraining perplexity: 1.2894\n",
      "Batch: 326\tTraining loss: 0.2643\n",
      "Batch: 326\tTraining perplexity: 1.3025\n",
      "Batch: 351\tTraining loss: 0.3176\n",
      "Batch: 351\tTraining perplexity: 1.3738\n",
      "Batch: 376\tTraining loss: 0.2646\n",
      "Batch: 376\tTraining perplexity: 1.3029\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 13.125\n",
      "\n",
      " Batch average distance : 15.46875\n",
      "\n",
      " Batch average distance : 16.078125\n",
      "\n",
      " Batch average distance : 13.296875\n",
      "\n",
      " Batch average distance : 15.015625\n",
      "\n",
      " Batch average distance : 16.34375\n",
      "\n",
      " Batch average distance : 13.203125\n",
      "\n",
      " Batch average distance : 17.390625\n",
      "\n",
      " Batch average distance : 15.421875\n",
      "\n",
      " Batch average distance : 14.25\n",
      "\n",
      " Batch average distance : 11.703125\n",
      "\n",
      " Batch average distance : 15.390625\n",
      "\n",
      " Batch average distance : 13.484375\n",
      "\n",
      " Batch average distance : 14.09375\n",
      "\n",
      " Batch average distance : 13.578125\n",
      "\n",
      " Batch average distance : 11.09375\n",
      "\n",
      " Batch average distance : 14.203125\n",
      "\n",
      " Batch average distance : 19.333333333333332\n",
      "\n",
      " Epoch average distance : 14.384267631103073\n",
      "*** Training Epoch 34 with teacher forcing 0.5 :***\n",
      "Batch: 1\tTraining loss: 0.2290\n",
      "Batch: 1\tTraining perplexity: 1.2574\n",
      "Batch: 26\tTraining loss: 0.2523\n",
      "Batch: 26\tTraining perplexity: 1.2870\n",
      "Batch: 51\tTraining loss: 0.2598\n",
      "Batch: 51\tTraining perplexity: 1.2967\n",
      "Batch: 76\tTraining loss: 0.2161\n",
      "Batch: 76\tTraining perplexity: 1.2412\n",
      "Batch: 101\tTraining loss: 0.2276\n",
      "Batch: 101\tTraining perplexity: 1.2555\n",
      "Batch: 126\tTraining loss: 0.1639\n",
      "Batch: 126\tTraining perplexity: 1.1781\n",
      "Batch: 151\tTraining loss: 0.2194\n",
      "Batch: 151\tTraining perplexity: 1.2454\n",
      "Batch: 176\tTraining loss: 0.2266\n",
      "Batch: 176\tTraining perplexity: 1.2544\n",
      "Batch: 201\tTraining loss: 0.1744\n",
      "Batch: 201\tTraining perplexity: 1.1905\n",
      "Batch: 226\tTraining loss: 0.2542\n",
      "Batch: 226\tTraining perplexity: 1.2894\n",
      "Batch: 251\tTraining loss: 0.1773\n",
      "Batch: 251\tTraining perplexity: 1.1940\n",
      "Batch: 276\tTraining loss: 0.2020\n",
      "Batch: 276\tTraining perplexity: 1.2238\n",
      "Batch: 301\tTraining loss: 0.2061\n",
      "Batch: 301\tTraining perplexity: 1.2289\n",
      "Batch: 326\tTraining loss: 0.2209\n",
      "Batch: 326\tTraining perplexity: 1.2473\n",
      "Batch: 351\tTraining loss: 0.2142\n",
      "Batch: 351\tTraining perplexity: 1.2389\n",
      "Batch: 376\tTraining loss: 0.1674\n",
      "Batch: 376\tTraining perplexity: 1.1823\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 15.609375\n",
      "\n",
      " Batch average distance : 13.796875\n",
      "\n",
      " Batch average distance : 13.125\n",
      "\n",
      " Batch average distance : 16.75\n",
      "\n",
      " Batch average distance : 15.453125\n",
      "\n",
      " Batch average distance : 14.15625\n",
      "\n",
      " Batch average distance : 14.78125\n",
      "\n",
      " Batch average distance : 14.578125\n",
      "\n",
      " Batch average distance : 16.9375\n",
      "\n",
      " Batch average distance : 14.875\n",
      "\n",
      " Batch average distance : 11.625\n",
      "\n",
      " Batch average distance : 14.75\n",
      "\n",
      " Batch average distance : 12.796875\n",
      "\n",
      " Batch average distance : 13.40625\n",
      "\n",
      " Batch average distance : 15.8125\n",
      "\n",
      " Batch average distance : 17.234375\n",
      "\n",
      " Batch average distance : 14.65625\n",
      "\n",
      " Batch average distance : 9.777777777777779\n",
      "\n",
      " Epoch average distance : 14.645569620253164\n",
      "*** Training Epoch 35 with teacher forcing 0.5 :***\n",
      "Batch: 1\tTraining loss: 0.1926\n",
      "Batch: 1\tTraining perplexity: 1.2124\n",
      "Batch: 26\tTraining loss: 0.2040\n",
      "Batch: 26\tTraining perplexity: 1.2263\n",
      "Batch: 51\tTraining loss: 0.2327\n",
      "Batch: 51\tTraining perplexity: 1.2620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 76\tTraining loss: 0.1691\n",
      "Batch: 76\tTraining perplexity: 1.1842\n",
      "Batch: 101\tTraining loss: 0.2214\n",
      "Batch: 101\tTraining perplexity: 1.2479\n",
      "Batch: 126\tTraining loss: 0.1868\n",
      "Batch: 126\tTraining perplexity: 1.2054\n",
      "Batch: 151\tTraining loss: 0.2710\n",
      "Batch: 151\tTraining perplexity: 1.3113\n",
      "Batch: 176\tTraining loss: 0.2076\n",
      "Batch: 176\tTraining perplexity: 1.2307\n",
      "Batch: 201\tTraining loss: 0.1725\n",
      "Batch: 201\tTraining perplexity: 1.1882\n",
      "Batch: 226\tTraining loss: 0.2745\n",
      "Batch: 226\tTraining perplexity: 1.3159\n",
      "Batch: 251\tTraining loss: 0.2144\n",
      "Batch: 251\tTraining perplexity: 1.2391\n",
      "Batch: 276\tTraining loss: 0.3005\n",
      "Batch: 276\tTraining perplexity: 1.3505\n",
      "Batch: 301\tTraining loss: 0.2443\n",
      "Batch: 301\tTraining perplexity: 1.2768\n",
      "Batch: 326\tTraining loss: 0.2388\n",
      "Batch: 326\tTraining perplexity: 1.2697\n",
      "Batch: 351\tTraining loss: 0.2491\n",
      "Batch: 351\tTraining perplexity: 1.2828\n",
      "Batch: 376\tTraining loss: 0.1661\n",
      "Batch: 376\tTraining perplexity: 1.1807\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 14.3125\n",
      "\n",
      " Batch average distance : 15.65625\n",
      "\n",
      " Batch average distance : 17.203125\n",
      "\n",
      " Batch average distance : 15.734375\n",
      "\n",
      " Batch average distance : 14.65625\n",
      "\n",
      " Batch average distance : 13.46875\n",
      "\n",
      " Batch average distance : 13.671875\n",
      "\n",
      " Batch average distance : 12.265625\n",
      "\n",
      " Batch average distance : 16.21875\n",
      "\n",
      " Batch average distance : 12.09375\n",
      "\n",
      " Batch average distance : 14.1875\n",
      "\n",
      " Batch average distance : 15.71875\n",
      "\n",
      " Batch average distance : 11.765625\n",
      "\n",
      " Batch average distance : 14.4375\n",
      "\n",
      " Batch average distance : 13.421875\n",
      "\n",
      " Batch average distance : 15.703125\n",
      "\n",
      " Batch average distance : 14.765625\n",
      "\n",
      " Batch average distance : 14.11111111111111\n",
      "\n",
      " Epoch average distance : 14.423146473779385\n",
      "*** Training Epoch 36 with teacher forcing 0.5 :***\n",
      "Batch: 1\tTraining loss: 0.1804\n",
      "Batch: 1\tTraining perplexity: 1.1977\n",
      "Batch: 26\tTraining loss: 0.2132\n",
      "Batch: 26\tTraining perplexity: 1.2376\n",
      "Batch: 51\tTraining loss: 0.1257\n",
      "Batch: 51\tTraining perplexity: 1.1339\n",
      "Batch: 76\tTraining loss: 0.1949\n",
      "Batch: 76\tTraining perplexity: 1.2151\n",
      "Batch: 101\tTraining loss: 0.2648\n",
      "Batch: 101\tTraining perplexity: 1.3032\n",
      "Batch: 126\tTraining loss: 0.1816\n",
      "Batch: 126\tTraining perplexity: 1.1991\n",
      "Batch: 151\tTraining loss: 0.1917\n",
      "Batch: 151\tTraining perplexity: 1.2113\n",
      "Batch: 176\tTraining loss: 0.1618\n",
      "Batch: 176\tTraining perplexity: 1.1756\n",
      "Batch: 201\tTraining loss: 0.1715\n",
      "Batch: 201\tTraining perplexity: 1.1871\n",
      "Batch: 226\tTraining loss: 0.2067\n",
      "Batch: 226\tTraining perplexity: 1.2296\n",
      "Batch: 251\tTraining loss: 0.2184\n",
      "Batch: 251\tTraining perplexity: 1.2441\n",
      "Batch: 276\tTraining loss: 0.1893\n",
      "Batch: 276\tTraining perplexity: 1.2084\n",
      "Batch: 301\tTraining loss: 0.1687\n",
      "Batch: 301\tTraining perplexity: 1.1837\n",
      "Batch: 326\tTraining loss: 0.2368\n",
      "Batch: 326\tTraining perplexity: 1.2672\n",
      "Batch: 351\tTraining loss: 0.1925\n",
      "Batch: 351\tTraining perplexity: 1.2123\n",
      "Batch: 376\tTraining loss: 0.1603\n",
      "Batch: 376\tTraining perplexity: 1.1739\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 12.609375\n",
      "\n",
      " Batch average distance : 12.96875\n",
      "\n",
      " Batch average distance : 11.546875\n",
      "\n",
      " Batch average distance : 14.6875\n",
      "\n",
      " Batch average distance : 15.609375\n",
      "\n",
      " Batch average distance : 13.28125\n",
      "\n",
      " Batch average distance : 13.390625\n",
      "\n",
      " Batch average distance : 15.25\n",
      "\n",
      " Batch average distance : 14.90625\n",
      "\n",
      " Batch average distance : 14.265625\n",
      "\n",
      " Batch average distance : 14.921875\n",
      "\n",
      " Batch average distance : 16.46875\n",
      "\n",
      " Batch average distance : 15.03125\n",
      "\n",
      " Batch average distance : 11.21875\n",
      "\n",
      " Batch average distance : 12.25\n",
      "\n",
      " Batch average distance : 11.84375\n",
      "\n",
      " Batch average distance : 16.3125\n",
      "\n",
      " Batch average distance : 13.333333333333334\n",
      "\n",
      " Epoch average distance : 13.905967450271248\n",
      "*** Training Epoch 37 with teacher forcing 0.5 :***\n",
      "Batch: 1\tTraining loss: 0.1520\n",
      "Batch: 1\tTraining perplexity: 1.1641\n",
      "Batch: 26\tTraining loss: 0.2011\n",
      "Batch: 26\tTraining perplexity: 1.2228\n",
      "Batch: 51\tTraining loss: 0.2247\n",
      "Batch: 51\tTraining perplexity: 1.2520\n",
      "Batch: 76\tTraining loss: 0.1699\n",
      "Batch: 76\tTraining perplexity: 1.1851\n",
      "Batch: 101\tTraining loss: 0.1580\n",
      "Batch: 101\tTraining perplexity: 1.1712\n",
      "Batch: 126\tTraining loss: 0.2464\n",
      "Batch: 126\tTraining perplexity: 1.2795\n",
      "Batch: 151\tTraining loss: 0.1723\n",
      "Batch: 151\tTraining perplexity: 1.1881\n",
      "Batch: 176\tTraining loss: 0.2003\n",
      "Batch: 176\tTraining perplexity: 1.2217\n",
      "Batch: 201\tTraining loss: 0.1881\n",
      "Batch: 201\tTraining perplexity: 1.2070\n",
      "Batch: 226\tTraining loss: 0.2533\n",
      "Batch: 226\tTraining perplexity: 1.2883\n",
      "Batch: 251\tTraining loss: 0.2931\n",
      "Batch: 251\tTraining perplexity: 1.3406\n",
      "Batch: 276\tTraining loss: 0.2103\n",
      "Batch: 276\tTraining perplexity: 1.2340\n",
      "Batch: 301\tTraining loss: 0.1377\n",
      "Batch: 301\tTraining perplexity: 1.1476\n",
      "Batch: 326\tTraining loss: 0.2517\n",
      "Batch: 326\tTraining perplexity: 1.2862\n",
      "Batch: 351\tTraining loss: 0.2076\n",
      "Batch: 351\tTraining perplexity: 1.2307\n",
      "Batch: 376\tTraining loss: 0.2013\n",
      "Batch: 376\tTraining perplexity: 1.2230\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 12.71875\n",
      "\n",
      " Batch average distance : 14.109375\n",
      "\n",
      " Batch average distance : 10.796875\n",
      "\n",
      " Batch average distance : 14.53125\n",
      "\n",
      " Batch average distance : 13.984375\n",
      "\n",
      " Batch average distance : 16.515625\n",
      "\n",
      " Batch average distance : 11.375\n",
      "\n",
      " Batch average distance : 12.984375\n",
      "\n",
      " Batch average distance : 11.75\n",
      "\n",
      " Batch average distance : 14.0\n",
      "\n",
      " Batch average distance : 15.109375\n",
      "\n",
      " Batch average distance : 16.4375\n",
      "\n",
      " Batch average distance : 15.0\n",
      "\n",
      " Batch average distance : 13.171875\n",
      "\n",
      " Batch average distance : 14.390625\n",
      "\n",
      " Batch average distance : 14.171875\n",
      "\n",
      " Batch average distance : 17.71875\n",
      "\n",
      " Batch average distance : 15.944444444444445\n",
      "\n",
      " Epoch average distance : 14.075949367088608\n",
      "*** Training Epoch 38 with teacher forcing 0.5 :***\n",
      "Batch: 1\tTraining loss: 0.1579\n",
      "Batch: 1\tTraining perplexity: 1.1711\n",
      "Batch: 26\tTraining loss: 0.1432\n",
      "Batch: 26\tTraining perplexity: 1.1540\n",
      "Batch: 51\tTraining loss: 0.2051\n",
      "Batch: 51\tTraining perplexity: 1.2276\n",
      "Batch: 76\tTraining loss: 0.1678\n",
      "Batch: 76\tTraining perplexity: 1.1827\n",
      "Batch: 101\tTraining loss: 0.2512\n",
      "Batch: 101\tTraining perplexity: 1.2856\n",
      "Batch: 126\tTraining loss: 0.1731\n",
      "Batch: 126\tTraining perplexity: 1.1889\n",
      "Batch: 151\tTraining loss: 0.2410\n",
      "Batch: 151\tTraining perplexity: 1.2725\n",
      "Batch: 176\tTraining loss: 0.2793\n",
      "Batch: 176\tTraining perplexity: 1.3222\n",
      "Batch: 201\tTraining loss: 0.1706\n",
      "Batch: 201\tTraining perplexity: 1.1860\n",
      "Batch: 226\tTraining loss: 0.1941\n",
      "Batch: 226\tTraining perplexity: 1.2142\n",
      "Batch: 251\tTraining loss: 0.1636\n",
      "Batch: 251\tTraining perplexity: 1.1777\n",
      "Batch: 276\tTraining loss: 0.1865\n",
      "Batch: 276\tTraining perplexity: 1.2050\n",
      "Batch: 301\tTraining loss: 0.2065\n",
      "Batch: 301\tTraining perplexity: 1.2293\n",
      "Batch: 326\tTraining loss: 0.1790\n",
      "Batch: 326\tTraining perplexity: 1.1960\n",
      "Batch: 351\tTraining loss: 0.1913\n",
      "Batch: 351\tTraining perplexity: 1.2108\n",
      "Batch: 376\tTraining loss: 0.2253\n",
      "Batch: 376\tTraining perplexity: 1.2527\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 13.25\n",
      "\n",
      " Batch average distance : 12.21875\n",
      "\n",
      " Batch average distance : 11.921875\n",
      "\n",
      " Batch average distance : 15.921875\n",
      "\n",
      " Batch average distance : 13.96875\n",
      "\n",
      " Batch average distance : 15.421875\n",
      "\n",
      " Batch average distance : 14.5\n",
      "\n",
      " Batch average distance : 12.609375\n",
      "\n",
      " Batch average distance : 16.953125\n",
      "\n",
      " Batch average distance : 12.328125\n",
      "\n",
      " Batch average distance : 15.9375\n",
      "\n",
      " Batch average distance : 11.71875\n",
      "\n",
      " Batch average distance : 15.015625\n",
      "\n",
      " Batch average distance : 14.90625\n",
      "\n",
      " Batch average distance : 16.25\n",
      "\n",
      " Batch average distance : 13.640625\n",
      "\n",
      " Batch average distance : 14.765625\n",
      "\n",
      " Batch average distance : 13.555555555555555\n",
      "\n",
      " Epoch average distance : 14.185352622061483\n",
      "*** Training Epoch 39 with teacher forcing 0.5 :***\n",
      "Batch: 1\tTraining loss: 0.1885\n",
      "Batch: 1\tTraining perplexity: 1.2074\n",
      "Batch: 26\tTraining loss: 0.2087\n",
      "Batch: 26\tTraining perplexity: 1.2320\n",
      "Batch: 51\tTraining loss: 0.1382\n",
      "Batch: 51\tTraining perplexity: 1.1483\n",
      "Batch: 76\tTraining loss: 0.2292\n",
      "Batch: 76\tTraining perplexity: 1.2576\n",
      "Batch: 101\tTraining loss: 0.1805\n",
      "Batch: 101\tTraining perplexity: 1.1979\n",
      "Batch: 126\tTraining loss: 0.2092\n",
      "Batch: 126\tTraining perplexity: 1.2327\n",
      "Batch: 151\tTraining loss: 0.1625\n",
      "Batch: 151\tTraining perplexity: 1.1765\n",
      "Batch: 176\tTraining loss: 0.1319\n",
      "Batch: 176\tTraining perplexity: 1.1410\n",
      "Batch: 201\tTraining loss: 0.2344\n",
      "Batch: 201\tTraining perplexity: 1.2642\n",
      "Batch: 226\tTraining loss: 0.1883\n",
      "Batch: 226\tTraining perplexity: 1.2071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 251\tTraining loss: 0.2185\n",
      "Batch: 251\tTraining perplexity: 1.2443\n",
      "Batch: 276\tTraining loss: 0.1975\n",
      "Batch: 276\tTraining perplexity: 1.2183\n",
      "Batch: 301\tTraining loss: 0.1856\n",
      "Batch: 301\tTraining perplexity: 1.2039\n",
      "Batch: 326\tTraining loss: 0.1806\n",
      "Batch: 326\tTraining perplexity: 1.1979\n",
      "Batch: 351\tTraining loss: 0.1586\n",
      "Batch: 351\tTraining perplexity: 1.1719\n",
      "Batch: 376\tTraining loss: 0.1643\n",
      "Batch: 376\tTraining perplexity: 1.1786\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 17.65625\n",
      "\n",
      " Batch average distance : 13.578125\n",
      "\n",
      " Batch average distance : 14.125\n",
      "\n",
      " Batch average distance : 13.46875\n",
      "\n",
      " Batch average distance : 15.96875\n",
      "\n",
      " Batch average distance : 14.484375\n",
      "\n",
      " Batch average distance : 13.75\n",
      "\n",
      " Batch average distance : 15.1875\n",
      "\n",
      " Batch average distance : 13.203125\n",
      "\n",
      " Batch average distance : 11.953125\n",
      "\n",
      " Batch average distance : 15.515625\n",
      "\n",
      " Batch average distance : 13.421875\n",
      "\n",
      " Batch average distance : 12.953125\n",
      "\n",
      " Batch average distance : 12.65625\n",
      "\n",
      " Batch average distance : 14.265625\n",
      "\n",
      " Batch average distance : 18.515625\n",
      "\n",
      " Batch average distance : 13.078125\n",
      "\n",
      " Batch average distance : 12.88888888888889\n",
      "\n",
      " Epoch average distance : 14.316455696202532\n",
      "*** Training Epoch 40 with teacher forcing 0.5 :***\n",
      "Batch: 1\tTraining loss: 0.1566\n",
      "Batch: 1\tTraining perplexity: 1.1696\n",
      "Batch: 26\tTraining loss: 0.1275\n",
      "Batch: 26\tTraining perplexity: 1.1360\n",
      "Batch: 51\tTraining loss: 0.1284\n",
      "Batch: 51\tTraining perplexity: 1.1370\n",
      "Batch: 76\tTraining loss: 0.1195\n",
      "Batch: 76\tTraining perplexity: 1.1269\n",
      "Batch: 101\tTraining loss: 0.2043\n",
      "Batch: 101\tTraining perplexity: 1.2266\n",
      "Batch: 126\tTraining loss: 0.1441\n",
      "Batch: 126\tTraining perplexity: 1.1550\n",
      "Batch: 151\tTraining loss: 0.2142\n",
      "Batch: 151\tTraining perplexity: 1.2388\n",
      "Batch: 176\tTraining loss: 0.1370\n",
      "Batch: 176\tTraining perplexity: 1.1468\n",
      "Batch: 201\tTraining loss: 0.2145\n",
      "Batch: 201\tTraining perplexity: 1.2392\n",
      "Batch: 226\tTraining loss: 0.2036\n",
      "Batch: 226\tTraining perplexity: 1.2259\n",
      "Batch: 251\tTraining loss: 0.1729\n",
      "Batch: 251\tTraining perplexity: 1.1888\n",
      "Batch: 276\tTraining loss: 0.1826\n",
      "Batch: 276\tTraining perplexity: 1.2003\n",
      "Batch: 301\tTraining loss: 0.1826\n",
      "Batch: 301\tTraining perplexity: 1.2004\n",
      "Batch: 326\tTraining loss: 0.1377\n",
      "Batch: 326\tTraining perplexity: 1.1476\n",
      "Batch: 351\tTraining loss: 0.1421\n",
      "Batch: 351\tTraining perplexity: 1.1526\n",
      "Batch: 376\tTraining loss: 0.1538\n",
      "Batch: 376\tTraining perplexity: 1.1663\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 12.875\n",
      "\n",
      " Batch average distance : 17.203125\n",
      "\n",
      " Batch average distance : 12.765625\n",
      "\n",
      " Batch average distance : 15.671875\n",
      "\n",
      " Batch average distance : 12.46875\n",
      "\n",
      " Batch average distance : 15.609375\n",
      "\n",
      " Batch average distance : 12.9375\n",
      "\n",
      " Batch average distance : 14.421875\n",
      "\n",
      " Batch average distance : 13.1875\n",
      "\n",
      " Batch average distance : 13.140625\n",
      "\n",
      " Batch average distance : 14.296875\n",
      "\n",
      " Batch average distance : 14.578125\n",
      "\n",
      " Batch average distance : 16.015625\n",
      "\n",
      " Batch average distance : 12.484375\n",
      "\n",
      " Batch average distance : 10.859375\n",
      "\n",
      " Batch average distance : 11.421875\n",
      "\n",
      " Batch average distance : 12.8125\n",
      "\n",
      " Batch average distance : 11.666666666666666\n",
      "\n",
      " Epoch average distance : 13.658227848101266\n",
      "*** Training Epoch 41 with teacher forcing 0.5 :***\n",
      "Batch: 1\tTraining loss: 0.1674\n",
      "Batch: 1\tTraining perplexity: 1.1822\n",
      "Batch: 26\tTraining loss: 0.1976\n",
      "Batch: 26\tTraining perplexity: 1.2185\n",
      "Batch: 51\tTraining loss: 0.2008\n",
      "Batch: 51\tTraining perplexity: 1.2224\n",
      "Batch: 76\tTraining loss: 0.1104\n",
      "Batch: 76\tTraining perplexity: 1.1167\n",
      "Batch: 101\tTraining loss: 0.2111\n",
      "Batch: 101\tTraining perplexity: 1.2351\n",
      "Batch: 126\tTraining loss: 0.1318\n",
      "Batch: 126\tTraining perplexity: 1.1409\n",
      "Batch: 151\tTraining loss: 0.1496\n",
      "Batch: 151\tTraining perplexity: 1.1614\n",
      "Batch: 176\tTraining loss: 0.1320\n",
      "Batch: 176\tTraining perplexity: 1.1411\n",
      "Batch: 201\tTraining loss: 0.1531\n",
      "Batch: 201\tTraining perplexity: 1.1655\n",
      "Batch: 226\tTraining loss: 0.1677\n",
      "Batch: 226\tTraining perplexity: 1.1826\n",
      "Batch: 251\tTraining loss: 0.2017\n",
      "Batch: 251\tTraining perplexity: 1.2235\n",
      "Batch: 276\tTraining loss: 0.1657\n",
      "Batch: 276\tTraining perplexity: 1.1803\n",
      "Batch: 301\tTraining loss: 0.1313\n",
      "Batch: 301\tTraining perplexity: 1.1403\n",
      "Batch: 326\tTraining loss: 0.1475\n",
      "Batch: 326\tTraining perplexity: 1.1590\n",
      "Batch: 351\tTraining loss: 0.1362\n",
      "Batch: 351\tTraining perplexity: 1.1459\n",
      "Batch: 376\tTraining loss: 0.1262\n",
      "Batch: 376\tTraining perplexity: 1.1345\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 17.59375\n",
      "\n",
      " Batch average distance : 14.75\n",
      "\n",
      " Batch average distance : 10.703125\n",
      "\n",
      " Batch average distance : 12.421875\n",
      "\n",
      " Batch average distance : 11.1875\n",
      "\n",
      " Batch average distance : 10.640625\n",
      "\n",
      " Batch average distance : 13.40625\n",
      "\n",
      " Batch average distance : 14.40625\n",
      "\n",
      " Batch average distance : 15.703125\n",
      "\n",
      " Batch average distance : 12.0625\n",
      "\n",
      " Batch average distance : 14.34375\n",
      "\n",
      " Batch average distance : 13.65625\n",
      "\n",
      " Batch average distance : 11.859375\n",
      "\n",
      " Batch average distance : 11.78125\n",
      "\n",
      " Batch average distance : 15.203125\n",
      "\n",
      " Batch average distance : 15.375\n",
      "\n",
      " Batch average distance : 15.546875\n",
      "\n",
      " Batch average distance : 6.388888888888889\n",
      "\n",
      " Epoch average distance : 13.450271247739602\n",
      "*** Training Epoch 42 with teacher forcing 0.5 :***\n",
      "Batch: 1\tTraining loss: 0.1403\n",
      "Batch: 1\tTraining perplexity: 1.1507\n",
      "Batch: 26\tTraining loss: 0.2292\n",
      "Batch: 26\tTraining perplexity: 1.2576\n",
      "Batch: 51\tTraining loss: 0.1499\n",
      "Batch: 51\tTraining perplexity: 1.1618\n",
      "Batch: 76\tTraining loss: 0.1450\n",
      "Batch: 76\tTraining perplexity: 1.1561\n",
      "Batch: 101\tTraining loss: 0.1351\n",
      "Batch: 101\tTraining perplexity: 1.1446\n",
      "Batch: 126\tTraining loss: 0.1308\n",
      "Batch: 126\tTraining perplexity: 1.1398\n",
      "Batch: 151\tTraining loss: 0.1832\n",
      "Batch: 151\tTraining perplexity: 1.2011\n",
      "Batch: 176\tTraining loss: 0.2042\n",
      "Batch: 176\tTraining perplexity: 1.2265\n",
      "Batch: 201\tTraining loss: 0.1851\n",
      "Batch: 201\tTraining perplexity: 1.2034\n",
      "Batch: 226\tTraining loss: 0.1543\n",
      "Batch: 226\tTraining perplexity: 1.1669\n",
      "Batch: 251\tTraining loss: 0.1869\n",
      "Batch: 251\tTraining perplexity: 1.2055\n",
      "Batch: 276\tTraining loss: 0.1626\n",
      "Batch: 276\tTraining perplexity: 1.1765\n",
      "Batch: 301\tTraining loss: 0.1020\n",
      "Batch: 301\tTraining perplexity: 1.1073\n",
      "Batch: 326\tTraining loss: 0.1572\n",
      "Batch: 326\tTraining perplexity: 1.1702\n",
      "Batch: 351\tTraining loss: 0.1901\n",
      "Batch: 351\tTraining perplexity: 1.2094\n",
      "Batch: 376\tTraining loss: 0.1979\n",
      "Batch: 376\tTraining perplexity: 1.2189\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 17.296875\n",
      "\n",
      " Batch average distance : 14.515625\n",
      "\n",
      " Batch average distance : 14.15625\n",
      "\n",
      " Batch average distance : 15.5\n",
      "\n",
      " Batch average distance : 15.03125\n",
      "\n",
      " Batch average distance : 11.671875\n",
      "\n",
      " Batch average distance : 17.75\n",
      "\n",
      " Batch average distance : 10.921875\n",
      "\n",
      " Batch average distance : 14.25\n",
      "\n",
      " Batch average distance : 13.65625\n",
      "\n",
      " Batch average distance : 12.5625\n",
      "\n",
      " Batch average distance : 14.546875\n",
      "\n",
      " Batch average distance : 15.5\n",
      "\n",
      " Batch average distance : 13.6875\n",
      "\n",
      " Batch average distance : 13.609375\n",
      "\n",
      " Batch average distance : 15.421875\n",
      "\n",
      " Batch average distance : 13.234375\n",
      "\n",
      " Batch average distance : 16.22222222222222\n",
      "\n",
      " Epoch average distance : 14.343580470162749\n",
      "*** Training Epoch 43 with teacher forcing 0.5 :***\n",
      "Batch: 1\tTraining loss: 0.1087\n",
      "Batch: 1\tTraining perplexity: 1.1148\n",
      "Batch: 26\tTraining loss: 0.1428\n",
      "Batch: 26\tTraining perplexity: 1.1535\n",
      "Batch: 51\tTraining loss: 0.1775\n",
      "Batch: 51\tTraining perplexity: 1.1942\n",
      "Batch: 76\tTraining loss: 0.1259\n",
      "Batch: 76\tTraining perplexity: 1.1342\n",
      "Batch: 101\tTraining loss: 0.1661\n",
      "Batch: 101\tTraining perplexity: 1.1807\n",
      "Batch: 126\tTraining loss: 0.1473\n",
      "Batch: 126\tTraining perplexity: 1.1587\n",
      "Batch: 151\tTraining loss: 0.1563\n",
      "Batch: 151\tTraining perplexity: 1.1692\n",
      "Batch: 176\tTraining loss: 0.1611\n",
      "Batch: 176\tTraining perplexity: 1.1748\n",
      "Batch: 201\tTraining loss: 0.1581\n",
      "Batch: 201\tTraining perplexity: 1.1712\n",
      "Batch: 226\tTraining loss: 0.1813\n",
      "Batch: 226\tTraining perplexity: 1.1988\n",
      "Batch: 251\tTraining loss: 0.1164\n",
      "Batch: 251\tTraining perplexity: 1.1234\n",
      "Batch: 276\tTraining loss: 0.1638\n",
      "Batch: 276\tTraining perplexity: 1.1780\n",
      "Batch: 301\tTraining loss: 0.1695\n",
      "Batch: 301\tTraining perplexity: 1.1847\n",
      "Batch: 326\tTraining loss: 0.1058\n",
      "Batch: 326\tTraining perplexity: 1.1116\n",
      "Batch: 351\tTraining loss: 0.1713\n",
      "Batch: 351\tTraining perplexity: 1.1869\n",
      "Batch: 376\tTraining loss: 0.1014\n",
      "Batch: 376\tTraining perplexity: 1.1067\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 12.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Batch average distance : 11.640625\n",
      "\n",
      " Batch average distance : 13.4375\n",
      "\n",
      " Batch average distance : 12.1875\n",
      "\n",
      " Batch average distance : 15.03125\n",
      "\n",
      " Batch average distance : 12.5\n",
      "\n",
      " Batch average distance : 13.6875\n",
      "\n",
      " Batch average distance : 11.796875\n",
      "\n",
      " Batch average distance : 12.25\n",
      "\n",
      " Batch average distance : 14.25\n",
      "\n",
      " Batch average distance : 12.375\n",
      "\n",
      " Batch average distance : 13.6875\n",
      "\n",
      " Batch average distance : 13.765625\n",
      "\n",
      " Batch average distance : 14.390625\n",
      "\n",
      " Batch average distance : 14.765625\n",
      "\n",
      " Batch average distance : 11.109375\n",
      "\n",
      " Batch average distance : 14.84375\n",
      "\n",
      " Batch average distance : 14.055555555555555\n",
      "\n",
      " Epoch average distance : 13.217902350813743\n",
      "*** Training Epoch 44 with teacher forcing 0.5 :***\n",
      "Batch: 1\tTraining loss: 0.1273\n",
      "Batch: 1\tTraining perplexity: 1.1357\n",
      "Batch: 26\tTraining loss: 0.1599\n",
      "Batch: 26\tTraining perplexity: 1.1733\n",
      "Batch: 51\tTraining loss: 0.1332\n",
      "Batch: 51\tTraining perplexity: 1.1425\n",
      "Batch: 76\tTraining loss: 0.1204\n",
      "Batch: 76\tTraining perplexity: 1.1280\n",
      "Batch: 101\tTraining loss: 0.2400\n",
      "Batch: 101\tTraining perplexity: 1.2713\n",
      "Batch: 126\tTraining loss: 0.1466\n",
      "Batch: 126\tTraining perplexity: 1.1579\n",
      "Batch: 151\tTraining loss: 0.1550\n",
      "Batch: 151\tTraining perplexity: 1.1677\n",
      "Batch: 176\tTraining loss: 0.1598\n",
      "Batch: 176\tTraining perplexity: 1.1733\n",
      "Batch: 201\tTraining loss: 0.1566\n",
      "Batch: 201\tTraining perplexity: 1.1695\n",
      "Batch: 226\tTraining loss: 0.1924\n",
      "Batch: 226\tTraining perplexity: 1.2122\n",
      "Batch: 251\tTraining loss: 0.1629\n",
      "Batch: 251\tTraining perplexity: 1.1770\n",
      "Batch: 276\tTraining loss: 0.1460\n",
      "Batch: 276\tTraining perplexity: 1.1572\n",
      "Batch: 301\tTraining loss: 0.1269\n",
      "Batch: 301\tTraining perplexity: 1.1353\n",
      "Batch: 326\tTraining loss: 0.1327\n",
      "Batch: 326\tTraining perplexity: 1.1420\n",
      "Batch: 351\tTraining loss: 0.1499\n",
      "Batch: 351\tTraining perplexity: 1.1617\n",
      "Batch: 376\tTraining loss: 0.1351\n",
      "Batch: 376\tTraining perplexity: 1.1446\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 13.0\n",
      "\n",
      " Batch average distance : 13.0625\n",
      "\n",
      " Batch average distance : 13.0\n",
      "\n",
      " Batch average distance : 14.15625\n",
      "\n",
      " Batch average distance : 13.265625\n",
      "\n",
      " Batch average distance : 13.171875\n",
      "\n",
      " Batch average distance : 15.96875\n",
      "\n",
      " Batch average distance : 15.53125\n",
      "\n",
      " Batch average distance : 13.21875\n",
      "\n",
      " Batch average distance : 11.546875\n",
      "\n",
      " Batch average distance : 12.71875\n",
      "\n",
      " Batch average distance : 13.84375\n",
      "\n",
      " Batch average distance : 14.46875\n",
      "\n",
      " Batch average distance : 10.0\n",
      "\n",
      " Batch average distance : 12.171875\n",
      "\n",
      " Batch average distance : 9.6875\n",
      "\n",
      " Batch average distance : 12.9375\n",
      "\n",
      " Batch average distance : 10.666666666666666\n",
      "\n",
      " Epoch average distance : 13.005424954792044\n",
      "*** Training Epoch 45 with teacher forcing 0.5 :***\n",
      "Batch: 1\tTraining loss: 0.1050\n",
      "Batch: 1\tTraining perplexity: 1.1107\n",
      "Batch: 26\tTraining loss: 0.1515\n",
      "Batch: 26\tTraining perplexity: 1.1636\n",
      "Batch: 51\tTraining loss: 0.1170\n",
      "Batch: 51\tTraining perplexity: 1.1241\n",
      "Batch: 76\tTraining loss: 0.1075\n",
      "Batch: 76\tTraining perplexity: 1.1135\n",
      "Batch: 101\tTraining loss: 0.1117\n",
      "Batch: 101\tTraining perplexity: 1.1181\n",
      "Batch: 126\tTraining loss: 0.1181\n",
      "Batch: 126\tTraining perplexity: 1.1253\n",
      "Batch: 151\tTraining loss: 0.1016\n",
      "Batch: 151\tTraining perplexity: 1.1069\n",
      "Batch: 176\tTraining loss: 0.1295\n",
      "Batch: 176\tTraining perplexity: 1.1383\n",
      "Batch: 201\tTraining loss: 0.2294\n",
      "Batch: 201\tTraining perplexity: 1.2578\n",
      "Batch: 226\tTraining loss: 0.1270\n",
      "Batch: 226\tTraining perplexity: 1.1354\n",
      "Batch: 251\tTraining loss: 0.1198\n",
      "Batch: 251\tTraining perplexity: 1.1273\n",
      "Batch: 276\tTraining loss: 0.2045\n",
      "Batch: 276\tTraining perplexity: 1.2269\n",
      "Batch: 301\tTraining loss: 0.1184\n",
      "Batch: 301\tTraining perplexity: 1.1257\n",
      "Batch: 326\tTraining loss: 0.1435\n",
      "Batch: 326\tTraining perplexity: 1.1543\n",
      "Batch: 351\tTraining loss: 0.1335\n",
      "Batch: 351\tTraining perplexity: 1.1428\n",
      "Batch: 376\tTraining loss: 0.1102\n",
      "Batch: 376\tTraining perplexity: 1.1165\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 12.46875\n",
      "\n",
      " Batch average distance : 11.671875\n",
      "\n",
      " Batch average distance : 12.484375\n",
      "\n",
      " Batch average distance : 13.625\n",
      "\n",
      " Batch average distance : 16.390625\n",
      "\n",
      " Batch average distance : 11.0625\n",
      "\n",
      " Batch average distance : 13.59375\n",
      "\n",
      " Batch average distance : 13.03125\n",
      "\n",
      " Batch average distance : 11.03125\n",
      "\n",
      " Batch average distance : 11.359375\n",
      "\n",
      " Batch average distance : 12.09375\n",
      "\n",
      " Batch average distance : 12.5625\n",
      "\n",
      " Batch average distance : 10.84375\n",
      "\n",
      " Batch average distance : 11.90625\n",
      "\n",
      " Batch average distance : 14.046875\n",
      "\n",
      " Batch average distance : 11.78125\n",
      "\n",
      " Batch average distance : 15.359375\n",
      "\n",
      " Batch average distance : 13.38888888888889\n",
      "\n",
      " Epoch average distance : 12.677215189873417\n",
      "*** Training Epoch 46 with teacher forcing 0.5 :***\n",
      "Batch: 1\tTraining loss: 0.1286\n",
      "Batch: 1\tTraining perplexity: 1.1373\n",
      "Batch: 26\tTraining loss: 0.1123\n",
      "Batch: 26\tTraining perplexity: 1.1189\n",
      "Batch: 51\tTraining loss: 0.1448\n",
      "Batch: 51\tTraining perplexity: 1.1558\n",
      "Batch: 76\tTraining loss: 0.1593\n",
      "Batch: 76\tTraining perplexity: 1.1727\n",
      "Batch: 101\tTraining loss: 0.1282\n",
      "Batch: 101\tTraining perplexity: 1.1368\n",
      "Batch: 126\tTraining loss: 0.1344\n",
      "Batch: 126\tTraining perplexity: 1.1439\n",
      "Batch: 151\tTraining loss: 0.1362\n",
      "Batch: 151\tTraining perplexity: 1.1459\n",
      "Batch: 176\tTraining loss: 0.0859\n",
      "Batch: 176\tTraining perplexity: 1.0897\n",
      "Batch: 201\tTraining loss: 0.0968\n",
      "Batch: 201\tTraining perplexity: 1.1016\n",
      "Batch: 226\tTraining loss: 0.1008\n",
      "Batch: 226\tTraining perplexity: 1.1061\n",
      "Batch: 251\tTraining loss: 0.1367\n",
      "Batch: 251\tTraining perplexity: 1.1464\n",
      "Batch: 276\tTraining loss: 0.1048\n",
      "Batch: 276\tTraining perplexity: 1.1105\n",
      "Batch: 301\tTraining loss: 0.1097\n",
      "Batch: 301\tTraining perplexity: 1.1159\n",
      "Batch: 326\tTraining loss: 0.1199\n",
      "Batch: 326\tTraining perplexity: 1.1274\n",
      "Batch: 351\tTraining loss: 0.1859\n",
      "Batch: 351\tTraining perplexity: 1.2044\n",
      "Batch: 376\tTraining loss: 0.1372\n",
      "Batch: 376\tTraining perplexity: 1.1470\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 15.203125\n",
      "\n",
      " Batch average distance : 11.0625\n",
      "\n",
      " Batch average distance : 13.0\n",
      "\n",
      " Batch average distance : 13.453125\n",
      "\n",
      " Batch average distance : 12.796875\n",
      "\n",
      " Batch average distance : 14.921875\n",
      "\n",
      " Batch average distance : 11.515625\n",
      "\n",
      " Batch average distance : 12.953125\n",
      "\n",
      " Batch average distance : 15.109375\n",
      "\n",
      " Batch average distance : 11.03125\n",
      "\n",
      " Batch average distance : 14.109375\n",
      "\n",
      " Batch average distance : 12.78125\n",
      "\n",
      " Batch average distance : 11.609375\n",
      "\n",
      " Batch average distance : 12.296875\n",
      "\n",
      " Batch average distance : 10.34375\n",
      "\n",
      " Batch average distance : 12.71875\n",
      "\n",
      " Batch average distance : 12.03125\n",
      "\n",
      " Batch average distance : 11.11111111111111\n",
      "\n",
      " Epoch average distance : 12.734177215189874\n",
      "*** Training Epoch 47 with teacher forcing 0.5 :***\n",
      "Batch: 1\tTraining loss: 0.1154\n",
      "Batch: 1\tTraining perplexity: 1.1223\n",
      "Batch: 26\tTraining loss: 0.1531\n",
      "Batch: 26\tTraining perplexity: 1.1655\n",
      "Batch: 51\tTraining loss: 0.1189\n",
      "Batch: 51\tTraining perplexity: 1.1263\n",
      "Batch: 76\tTraining loss: 0.1916\n",
      "Batch: 76\tTraining perplexity: 1.2112\n",
      "Batch: 101\tTraining loss: 0.1858\n",
      "Batch: 101\tTraining perplexity: 1.2042\n",
      "Batch: 126\tTraining loss: 0.1234\n",
      "Batch: 126\tTraining perplexity: 1.1314\n",
      "Batch: 151\tTraining loss: 0.1742\n",
      "Batch: 151\tTraining perplexity: 1.1903\n",
      "Batch: 176\tTraining loss: 0.0948\n",
      "Batch: 176\tTraining perplexity: 1.0994\n",
      "Batch: 201\tTraining loss: 0.1352\n",
      "Batch: 201\tTraining perplexity: 1.1448\n",
      "Batch: 226\tTraining loss: 0.1900\n",
      "Batch: 226\tTraining perplexity: 1.2092\n",
      "Batch: 251\tTraining loss: 0.1170\n",
      "Batch: 251\tTraining perplexity: 1.1241\n",
      "Batch: 276\tTraining loss: 0.1924\n",
      "Batch: 276\tTraining perplexity: 1.2121\n",
      "Batch: 301\tTraining loss: 0.1163\n",
      "Batch: 301\tTraining perplexity: 1.1233\n",
      "Batch: 326\tTraining loss: 0.1032\n",
      "Batch: 326\tTraining perplexity: 1.1087\n",
      "Batch: 351\tTraining loss: 0.1831\n",
      "Batch: 351\tTraining perplexity: 1.2010\n",
      "Batch: 376\tTraining loss: 0.1287\n",
      "Batch: 376\tTraining perplexity: 1.1373\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 13.625\n",
      "\n",
      " Batch average distance : 13.453125\n",
      "\n",
      " Batch average distance : 13.0\n",
      "\n",
      " Batch average distance : 12.328125\n",
      "\n",
      " Batch average distance : 12.8125\n",
      "\n",
      " Batch average distance : 14.5625\n",
      "\n",
      " Batch average distance : 11.53125\n",
      "\n",
      " Batch average distance : 14.046875\n",
      "\n",
      " Batch average distance : 14.0625\n",
      "\n",
      " Batch average distance : 11.96875\n",
      "\n",
      " Batch average distance : 12.546875\n",
      "\n",
      " Batch average distance : 15.1875\n",
      "\n",
      " Batch average distance : 9.203125\n",
      "\n",
      " Batch average distance : 13.15625\n",
      "\n",
      " Batch average distance : 15.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Batch average distance : 16.6875\n",
      "\n",
      " Batch average distance : 10.53125\n",
      "\n",
      " Batch average distance : 12.333333333333334\n",
      "\n",
      " Epoch average distance : 13.181735985533454\n",
      "*** Training Epoch 48 with teacher forcing 0.5 :***\n",
      "Batch: 1\tTraining loss: 0.1184\n",
      "Batch: 1\tTraining perplexity: 1.1257\n",
      "Batch: 26\tTraining loss: 0.0958\n",
      "Batch: 26\tTraining perplexity: 1.1005\n",
      "Batch: 51\tTraining loss: 0.1036\n",
      "Batch: 51\tTraining perplexity: 1.1092\n",
      "Batch: 76\tTraining loss: 0.1009\n",
      "Batch: 76\tTraining perplexity: 1.1061\n",
      "Batch: 101\tTraining loss: 0.1055\n",
      "Batch: 101\tTraining perplexity: 1.1113\n",
      "Batch: 126\tTraining loss: 0.1181\n",
      "Batch: 126\tTraining perplexity: 1.1253\n",
      "Batch: 151\tTraining loss: 0.1486\n",
      "Batch: 151\tTraining perplexity: 1.1602\n",
      "Batch: 176\tTraining loss: 0.1666\n",
      "Batch: 176\tTraining perplexity: 1.1813\n",
      "Batch: 201\tTraining loss: 0.1535\n",
      "Batch: 201\tTraining perplexity: 1.1659\n",
      "Batch: 226\tTraining loss: 0.1236\n",
      "Batch: 226\tTraining perplexity: 1.1316\n",
      "Batch: 251\tTraining loss: 0.1265\n",
      "Batch: 251\tTraining perplexity: 1.1348\n",
      "Batch: 276\tTraining loss: 0.1329\n",
      "Batch: 276\tTraining perplexity: 1.1421\n",
      "Batch: 301\tTraining loss: 0.1553\n",
      "Batch: 301\tTraining perplexity: 1.1680\n",
      "Batch: 326\tTraining loss: 0.1425\n",
      "Batch: 326\tTraining perplexity: 1.1532\n",
      "Batch: 351\tTraining loss: 0.1662\n",
      "Batch: 351\tTraining perplexity: 1.1808\n",
      "Batch: 376\tTraining loss: 0.1034\n",
      "Batch: 376\tTraining perplexity: 1.1090\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 12.6875\n",
      "\n",
      " Batch average distance : 13.53125\n",
      "\n",
      " Batch average distance : 14.546875\n",
      "\n",
      " Batch average distance : 13.484375\n",
      "\n",
      " Batch average distance : 14.59375\n",
      "\n",
      " Batch average distance : 14.046875\n",
      "\n",
      " Batch average distance : 11.921875\n",
      "\n",
      " Batch average distance : 11.953125\n",
      "\n",
      " Batch average distance : 12.9375\n",
      "\n",
      " Batch average distance : 13.734375\n",
      "\n",
      " Batch average distance : 11.90625\n",
      "\n",
      " Batch average distance : 11.0625\n",
      "\n",
      " Batch average distance : 11.765625\n",
      "\n",
      " Batch average distance : 12.3125\n",
      "\n",
      " Batch average distance : 11.140625\n",
      "\n",
      " Batch average distance : 13.3125\n",
      "\n",
      " Batch average distance : 12.671875\n",
      "\n",
      " Batch average distance : 10.88888888888889\n",
      "\n",
      " Epoch average distance : 12.769439421338156\n",
      "*** Training Epoch 49 with teacher forcing 0.5 :***\n",
      "Batch: 1\tTraining loss: 0.0996\n",
      "Batch: 1\tTraining perplexity: 1.1047\n",
      "Batch: 26\tTraining loss: 0.1405\n",
      "Batch: 26\tTraining perplexity: 1.1508\n",
      "Batch: 51\tTraining loss: 0.1208\n",
      "Batch: 51\tTraining perplexity: 1.1284\n",
      "Batch: 76\tTraining loss: 0.1878\n",
      "Batch: 76\tTraining perplexity: 1.2066\n",
      "Batch: 101\tTraining loss: 0.1171\n",
      "Batch: 101\tTraining perplexity: 1.1242\n",
      "Batch: 126\tTraining loss: 0.1115\n",
      "Batch: 126\tTraining perplexity: 1.1180\n",
      "Batch: 151\tTraining loss: 0.1430\n",
      "Batch: 151\tTraining perplexity: 1.1538\n",
      "Batch: 176\tTraining loss: 0.1014\n",
      "Batch: 176\tTraining perplexity: 1.1067\n",
      "Batch: 201\tTraining loss: 0.1029\n",
      "Batch: 201\tTraining perplexity: 1.1084\n",
      "Batch: 226\tTraining loss: 0.1176\n",
      "Batch: 226\tTraining perplexity: 1.1248\n",
      "Batch: 251\tTraining loss: 0.0953\n",
      "Batch: 251\tTraining perplexity: 1.1000\n",
      "Batch: 276\tTraining loss: 0.1369\n",
      "Batch: 276\tTraining perplexity: 1.1467\n",
      "Batch: 301\tTraining loss: 0.0992\n",
      "Batch: 301\tTraining perplexity: 1.1043\n",
      "Batch: 326\tTraining loss: 0.1523\n",
      "Batch: 326\tTraining perplexity: 1.1645\n",
      "Batch: 351\tTraining loss: 0.0963\n",
      "Batch: 351\tTraining perplexity: 1.1011\n",
      "Batch: 376\tTraining loss: 0.0921\n",
      "Batch: 376\tTraining perplexity: 1.0965\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 13.09375\n",
      "\n",
      " Batch average distance : 13.0625\n",
      "\n",
      " Batch average distance : 10.78125\n",
      "\n",
      " Batch average distance : 12.390625\n",
      "\n",
      " Batch average distance : 14.828125\n",
      "\n",
      " Batch average distance : 12.59375\n",
      "\n",
      " Batch average distance : 12.1875\n",
      "\n",
      " Batch average distance : 13.109375\n",
      "\n",
      " Batch average distance : 16.40625\n",
      "\n",
      " Batch average distance : 12.953125\n",
      "\n",
      " Batch average distance : 13.25\n",
      "\n",
      " Batch average distance : 12.09375\n",
      "\n",
      " Batch average distance : 12.34375\n",
      "\n",
      " Batch average distance : 14.15625\n",
      "\n",
      " Batch average distance : 13.609375\n",
      "\n",
      " Batch average distance : 13.71875\n",
      "\n",
      " Batch average distance : 11.140625\n",
      "\n",
      " Batch average distance : 6.666666666666667\n",
      "\n",
      " Epoch average distance : 12.938517179023508\n",
      "*** Training Epoch 50 with teacher forcing 0.5 :***\n",
      "Batch: 1\tTraining loss: 0.1047\n",
      "Batch: 1\tTraining perplexity: 1.1104\n",
      "Batch: 26\tTraining loss: 0.1326\n",
      "Batch: 26\tTraining perplexity: 1.1418\n",
      "Batch: 51\tTraining loss: 0.0889\n",
      "Batch: 51\tTraining perplexity: 1.0930\n",
      "Batch: 76\tTraining loss: 0.0833\n",
      "Batch: 76\tTraining perplexity: 1.0868\n",
      "Batch: 101\tTraining loss: 0.1136\n",
      "Batch: 101\tTraining perplexity: 1.1203\n",
      "Batch: 126\tTraining loss: 0.1106\n",
      "Batch: 126\tTraining perplexity: 1.1170\n",
      "Batch: 151\tTraining loss: 0.0834\n",
      "Batch: 151\tTraining perplexity: 1.0870\n",
      "Batch: 176\tTraining loss: 0.1087\n",
      "Batch: 176\tTraining perplexity: 1.1148\n",
      "Batch: 201\tTraining loss: 0.1294\n",
      "Batch: 201\tTraining perplexity: 1.1382\n",
      "Batch: 226\tTraining loss: 0.1479\n",
      "Batch: 226\tTraining perplexity: 1.1594\n",
      "Batch: 251\tTraining loss: 0.1192\n",
      "Batch: 251\tTraining perplexity: 1.1266\n",
      "Batch: 276\tTraining loss: 0.1670\n",
      "Batch: 276\tTraining perplexity: 1.1817\n",
      "Batch: 301\tTraining loss: 0.1122\n",
      "Batch: 301\tTraining perplexity: 1.1188\n",
      "Batch: 326\tTraining loss: 0.1403\n",
      "Batch: 326\tTraining perplexity: 1.1506\n",
      "Batch: 351\tTraining loss: 0.0993\n",
      "Batch: 351\tTraining perplexity: 1.1044\n",
      "Batch: 376\tTraining loss: 0.1596\n",
      "Batch: 376\tTraining perplexity: 1.1730\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 12.90625\n",
      "\n",
      " Batch average distance : 15.9375\n",
      "\n",
      " Batch average distance : 12.5625\n",
      "\n",
      " Batch average distance : 14.453125\n",
      "\n",
      " Batch average distance : 13.296875\n",
      "\n",
      " Batch average distance : 11.75\n",
      "\n",
      " Batch average distance : 12.8125\n",
      "\n",
      " Batch average distance : 10.875\n",
      "\n",
      " Batch average distance : 16.09375\n",
      "\n",
      " Batch average distance : 13.765625\n",
      "\n",
      " Batch average distance : 16.375\n",
      "\n",
      " Batch average distance : 10.265625\n",
      "\n",
      " Batch average distance : 11.203125\n",
      "\n",
      " Batch average distance : 12.96875\n",
      "\n",
      " Batch average distance : 13.109375\n",
      "\n",
      " Batch average distance : 11.4375\n",
      "\n",
      " Batch average distance : 11.0\n",
      "\n",
      " Batch average distance : 18.61111111111111\n",
      "\n",
      " Epoch average distance : 13.080470162748643\n",
      "*** Training Epoch 51 with teacher forcing 0.5 :***\n",
      "Batch: 1\tTraining loss: 0.0854\n",
      "Batch: 1\tTraining perplexity: 1.0891\n",
      "Batch: 26\tTraining loss: 0.0964\n",
      "Batch: 26\tTraining perplexity: 1.1012\n",
      "Batch: 51\tTraining loss: 0.0843\n",
      "Batch: 51\tTraining perplexity: 1.0879\n",
      "Batch: 76\tTraining loss: 0.0929\n",
      "Batch: 76\tTraining perplexity: 1.0974\n",
      "Batch: 101\tTraining loss: 0.1151\n",
      "Batch: 101\tTraining perplexity: 1.1219\n",
      "Batch: 126\tTraining loss: 0.1114\n",
      "Batch: 126\tTraining perplexity: 1.1178\n",
      "Batch: 151\tTraining loss: 0.1395\n",
      "Batch: 151\tTraining perplexity: 1.1497\n",
      "Batch: 176\tTraining loss: 0.0791\n",
      "Batch: 176\tTraining perplexity: 1.0823\n",
      "Batch: 201\tTraining loss: 0.1166\n",
      "Batch: 201\tTraining perplexity: 1.1237\n",
      "Batch: 226\tTraining loss: 0.0853\n",
      "Batch: 226\tTraining perplexity: 1.0891\n",
      "Batch: 251\tTraining loss: 0.0676\n",
      "Batch: 251\tTraining perplexity: 1.0700\n",
      "Batch: 276\tTraining loss: 0.2783\n",
      "Batch: 276\tTraining perplexity: 1.3209\n",
      "Batch: 301\tTraining loss: 0.1001\n",
      "Batch: 301\tTraining perplexity: 1.1053\n",
      "Batch: 326\tTraining loss: 0.0928\n",
      "Batch: 326\tTraining perplexity: 1.0973\n",
      "Batch: 351\tTraining loss: 0.1116\n",
      "Batch: 351\tTraining perplexity: 1.1181\n",
      "Batch: 376\tTraining loss: 0.1352\n",
      "Batch: 376\tTraining perplexity: 1.1448\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 11.46875\n",
      "\n",
      " Batch average distance : 11.65625\n",
      "\n",
      " Batch average distance : 14.46875\n",
      "\n",
      " Batch average distance : 14.46875\n",
      "\n",
      " Batch average distance : 15.3125\n",
      "\n",
      " Batch average distance : 10.546875\n",
      "\n",
      " Batch average distance : 9.203125\n",
      "\n",
      " Batch average distance : 11.96875\n",
      "\n",
      " Batch average distance : 13.53125\n",
      "\n",
      " Batch average distance : 11.3125\n",
      "\n",
      " Batch average distance : 11.515625\n",
      "\n",
      " Batch average distance : 10.328125\n",
      "\n",
      " Batch average distance : 12.65625\n",
      "\n",
      " Batch average distance : 14.703125\n",
      "\n",
      " Batch average distance : 12.578125\n",
      "\n",
      " Batch average distance : 12.28125\n",
      "\n",
      " Batch average distance : 15.34375\n",
      "\n",
      " Batch average distance : 7.444444444444445\n",
      "\n",
      " Epoch average distance : 12.466546112115733\n",
      "*** Training Epoch 52 with teacher forcing 0.5 :***\n",
      "Batch: 1\tTraining loss: 0.0712\n",
      "Batch: 1\tTraining perplexity: 1.0738\n",
      "Batch: 26\tTraining loss: 0.0754\n",
      "Batch: 26\tTraining perplexity: 1.0783\n",
      "Batch: 51\tTraining loss: 0.1143\n",
      "Batch: 51\tTraining perplexity: 1.1211\n",
      "Batch: 76\tTraining loss: 0.1337\n",
      "Batch: 76\tTraining perplexity: 1.1431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 101\tTraining loss: 0.1012\n",
      "Batch: 101\tTraining perplexity: 1.1065\n",
      "Batch: 126\tTraining loss: 0.1134\n",
      "Batch: 126\tTraining perplexity: 1.1200\n",
      "Batch: 151\tTraining loss: 0.0834\n",
      "Batch: 151\tTraining perplexity: 1.0870\n",
      "Batch: 176\tTraining loss: 0.1087\n",
      "Batch: 176\tTraining perplexity: 1.1148\n",
      "Batch: 201\tTraining loss: 0.1531\n",
      "Batch: 201\tTraining perplexity: 1.1654\n",
      "Batch: 226\tTraining loss: 0.0876\n",
      "Batch: 226\tTraining perplexity: 1.0915\n",
      "Batch: 251\tTraining loss: 0.0945\n",
      "Batch: 251\tTraining perplexity: 1.0991\n",
      "Batch: 276\tTraining loss: 0.0865\n",
      "Batch: 276\tTraining perplexity: 1.0903\n",
      "Batch: 301\tTraining loss: 0.1351\n",
      "Batch: 301\tTraining perplexity: 1.1446\n",
      "Batch: 326\tTraining loss: 0.1294\n",
      "Batch: 326\tTraining perplexity: 1.1381\n",
      "Batch: 351\tTraining loss: 0.1062\n",
      "Batch: 351\tTraining perplexity: 1.1121\n",
      "Batch: 376\tTraining loss: 0.1394\n",
      "Batch: 376\tTraining perplexity: 1.1496\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 12.5625\n",
      "\n",
      " Batch average distance : 13.328125\n",
      "\n",
      " Batch average distance : 14.140625\n",
      "\n",
      " Batch average distance : 15.0\n",
      "\n",
      " Batch average distance : 13.28125\n",
      "\n",
      " Batch average distance : 11.234375\n",
      "\n",
      " Batch average distance : 9.953125\n",
      "\n",
      " Batch average distance : 13.859375\n",
      "\n",
      " Batch average distance : 11.296875\n",
      "\n",
      " Batch average distance : 10.65625\n",
      "\n",
      " Batch average distance : 10.984375\n",
      "\n",
      " Batch average distance : 14.359375\n",
      "\n",
      " Batch average distance : 12.1875\n",
      "\n",
      " Batch average distance : 15.34375\n",
      "\n",
      " Batch average distance : 11.5\n",
      "\n",
      " Batch average distance : 15.015625\n",
      "\n",
      " Batch average distance : 12.75\n",
      "\n",
      " Batch average distance : 15.11111111111111\n",
      "\n",
      " Epoch average distance : 12.829113924050633\n",
      "*** Training Epoch 53 with teacher forcing 0.5 :***\n",
      "Batch: 1\tTraining loss: 0.1077\n",
      "Batch: 1\tTraining perplexity: 1.1137\n",
      "Batch: 26\tTraining loss: 0.1073\n",
      "Batch: 26\tTraining perplexity: 1.1133\n",
      "Batch: 51\tTraining loss: 0.0705\n",
      "Batch: 51\tTraining perplexity: 1.0730\n",
      "Batch: 76\tTraining loss: 0.0753\n",
      "Batch: 76\tTraining perplexity: 1.0783\n",
      "Batch: 101\tTraining loss: 0.0923\n",
      "Batch: 101\tTraining perplexity: 1.0967\n",
      "Batch: 126\tTraining loss: 0.1054\n",
      "Batch: 126\tTraining perplexity: 1.1112\n",
      "Batch: 151\tTraining loss: 0.0715\n",
      "Batch: 151\tTraining perplexity: 1.0741\n",
      "Batch: 176\tTraining loss: 0.1111\n",
      "Batch: 176\tTraining perplexity: 1.1175\n",
      "Batch: 201\tTraining loss: 0.1013\n",
      "Batch: 201\tTraining perplexity: 1.1066\n",
      "Batch: 226\tTraining loss: 0.1012\n",
      "Batch: 226\tTraining perplexity: 1.1064\n",
      "Batch: 251\tTraining loss: 0.1699\n",
      "Batch: 251\tTraining perplexity: 1.1852\n",
      "Batch: 276\tTraining loss: 0.0809\n",
      "Batch: 276\tTraining perplexity: 1.0843\n",
      "Batch: 301\tTraining loss: 0.0864\n",
      "Batch: 301\tTraining perplexity: 1.0902\n",
      "Batch: 326\tTraining loss: 0.1014\n",
      "Batch: 326\tTraining perplexity: 1.1067\n",
      "Batch: 351\tTraining loss: 0.1229\n",
      "Batch: 351\tTraining perplexity: 1.1308\n",
      "Batch: 376\tTraining loss: 0.1010\n",
      "Batch: 376\tTraining perplexity: 1.1062\n",
      "****** Start validation ******\n",
      "\n",
      " Batch average distance : 12.578125\n",
      "\n",
      " Batch average distance : 11.625\n",
      "\n",
      " Batch average distance : 9.75\n",
      "\n",
      " Batch average distance : 13.140625\n",
      "\n",
      " Batch average distance : 16.171875\n",
      "\n",
      " Batch average distance : 13.375\n",
      "\n",
      " Batch average distance : 13.46875\n",
      "\n",
      " Batch average distance : 12.578125\n",
      "\n",
      " Batch average distance : 14.859375\n",
      "\n",
      " Batch average distance : 12.984375\n",
      "\n",
      " Batch average distance : 11.21875\n",
      "\n",
      " Batch average distance : 14.421875\n",
      "\n",
      " Batch average distance : 12.921875\n",
      "\n",
      " Batch average distance : 13.34375\n",
      "\n",
      " Batch average distance : 15.1875\n",
      "\n",
      " Batch average distance : 12.5\n",
      "\n",
      " Batch average distance : 10.75\n",
      "\n",
      " Batch average distance : 14.0\n",
      "\n",
      " Epoch average distance : 13.009041591320072\n",
      "*** Training Epoch 54 with teacher forcing 0.5 :***\n",
      "Batch: 1\tTraining loss: 0.1006\n",
      "Batch: 1\tTraining perplexity: 1.1059\n",
      "Batch: 26\tTraining loss: 0.0710\n",
      "Batch: 26\tTraining perplexity: 1.0736\n",
      "Batch: 51\tTraining loss: 0.0981\n",
      "Batch: 51\tTraining perplexity: 1.1030\n",
      "Batch: 76\tTraining loss: 0.0990\n",
      "Batch: 76\tTraining perplexity: 1.1041\n",
      "Batch: 101\tTraining loss: 0.0816\n",
      "Batch: 101\tTraining perplexity: 1.0850\n",
      "Batch: 126\tTraining loss: 0.1353\n",
      "Batch: 126\tTraining perplexity: 1.1448\n",
      "Batch: 151\tTraining loss: 0.0830\n",
      "Batch: 151\tTraining perplexity: 1.0865\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-5d023908b6b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m66\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-75-3aac448f9d00>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, num_epochs, criterion, optimizer)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mmasked_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mmasked_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, train_loader, 66, criterion, optimizer, one_epoch=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Training Epoch 1 with teacher forcing 0.5 :***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel_launcher.py:55: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1\tTraining loss: 0.0481\n",
      "Batch: 1\tTraining perplexity: 1.0493\n",
      "Batch: 50\tTraining loss: 0.0773\n",
      "Batch: 50\tTraining perplexity: 1.0804\n",
      "Batch: 99\tTraining loss: 0.0794\n",
      "Batch: 99\tTraining perplexity: 1.0827\n",
      "Batch: 148\tTraining loss: 0.0834\n",
      "Batch: 148\tTraining perplexity: 1.0869\n",
      "Batch: 197\tTraining loss: 0.0718\n",
      "Batch: 197\tTraining perplexity: 1.0745\n",
      "Batch: 246\tTraining loss: 0.0990\n",
      "Batch: 246\tTraining perplexity: 1.1041\n",
      "Batch: 295\tTraining loss: 0.0674\n",
      "Batch: 295\tTraining perplexity: 1.0697\n",
      "Batch: 344\tTraining loss: 0.0664\n",
      "Batch: 344\tTraining perplexity: 1.0687\n",
      "****** Start validation ******\n",
      "\n",
      " Epoch average distance : 12.021699819168173\n"
     ]
    }
   ],
   "source": [
    "numEpochs = 1\n",
    "learningRate = 3e-5\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)\n",
    "train(model, train_loader, numEpochs, criterion, optimizer, one_epoch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Training Epoch 1 with teacher forcing 0.5 :***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel_launcher.py:55: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1\tTraining loss: 0.0433\n",
      "Batch: 1\tTraining perplexity: 1.0442\n",
      "Batch: 50\tTraining loss: 0.0532\n",
      "Batch: 50\tTraining perplexity: 1.0547\n",
      "Batch: 99\tTraining loss: 0.0604\n",
      "Batch: 99\tTraining perplexity: 1.0622\n",
      "Batch: 148\tTraining loss: 0.0683\n",
      "Batch: 148\tTraining perplexity: 1.0707\n",
      "Batch: 197\tTraining loss: 0.0525\n",
      "Batch: 197\tTraining perplexity: 1.0539\n",
      "Batch: 246\tTraining loss: 0.0688\n",
      "Batch: 246\tTraining perplexity: 1.0712\n",
      "Batch: 295\tTraining loss: 0.0651\n",
      "Batch: 295\tTraining perplexity: 1.0672\n",
      "Batch: 344\tTraining loss: 0.0719\n",
      "Batch: 344\tTraining perplexity: 1.0746\n",
      "****** Start validation ******\n",
      "\n",
      " Epoch average distance : 12.105786618444846\n"
     ]
    }
   ],
   "source": [
    "numEpochs = 1\n",
    "learningRate = 3e-5\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)\n",
    "train(model, train_loader, numEpochs, criterion, optimizer, one_epoch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Training Epoch 1 with teacher forcing 0.6 :***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel_launcher.py:55: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 49\tTraining loss: 0.1011\n",
      "Batch: 49\tTraining perplexity: 1.1064\n",
      "Batch: 99\tTraining loss: 0.0613\n",
      "Batch: 99\tTraining perplexity: 1.0632\n",
      "Batch: 149\tTraining loss: 0.1271\n",
      "Batch: 149\tTraining perplexity: 1.1355\n",
      "Batch: 199\tTraining loss: 0.0808\n",
      "Batch: 199\tTraining perplexity: 1.0841\n",
      "Batch: 249\tTraining loss: 0.0652\n",
      "Batch: 249\tTraining perplexity: 1.0674\n",
      "Batch: 299\tTraining loss: 0.0687\n",
      "Batch: 299\tTraining perplexity: 1.0711\n",
      "Batch: 349\tTraining loss: 0.0610\n",
      "Batch: 349\tTraining perplexity: 1.0629\n",
      "****** Start validation ******\n",
      "\n",
      " Epoch average distance : 12.12748643761302\n"
     ]
    }
   ],
   "source": [
    "numEpochs = 1\n",
    "learningRate = 3e-5\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)\n",
    "train(model, train_loader, numEpochs, criterion, optimizer, one_epoch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Training Epoch 1 with teacher forcing 0.6 :***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel_launcher.py:55: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 50\tTraining loss: 0.0643\n",
      "Batch: 50\tTraining perplexity: 1.0664\n",
      "Batch: 100\tTraining loss: 0.0581\n",
      "Batch: 100\tTraining perplexity: 1.0598\n",
      "Batch: 150\tTraining loss: 0.0754\n",
      "Batch: 150\tTraining perplexity: 1.0783\n",
      "Batch: 200\tTraining loss: 0.0752\n",
      "Batch: 200\tTraining perplexity: 1.0781\n",
      "Batch: 250\tTraining loss: 0.0633\n",
      "Batch: 250\tTraining perplexity: 1.0654\n",
      "Batch: 300\tTraining loss: 0.0776\n",
      "Batch: 300\tTraining perplexity: 1.0806\n",
      "Batch: 350\tTraining loss: 0.1298\n",
      "Batch: 350\tTraining perplexity: 1.1386\n",
      "****** Start validation ******\n",
      "\n",
      " Epoch average distance : 12.269439421338156\n",
      "*** Training Epoch 2 with teacher forcing 0.6 :***\n",
      "Batch: 50\tTraining loss: 0.0575\n",
      "Batch: 50\tTraining perplexity: 1.0592\n",
      "Batch: 100\tTraining loss: 0.0774\n",
      "Batch: 100\tTraining perplexity: 1.0805\n",
      "Batch: 150\tTraining loss: 0.0596\n",
      "Batch: 150\tTraining perplexity: 1.0615\n",
      "Batch: 200\tTraining loss: 0.0490\n",
      "Batch: 200\tTraining perplexity: 1.0502\n",
      "Batch: 250\tTraining loss: 0.0626\n",
      "Batch: 250\tTraining perplexity: 1.0646\n",
      "Batch: 300\tTraining loss: 0.1291\n",
      "Batch: 300\tTraining perplexity: 1.1378\n",
      "Batch: 350\tTraining loss: 0.0927\n",
      "Batch: 350\tTraining perplexity: 1.0971\n",
      "****** Start validation ******\n",
      "\n",
      " Epoch average distance : 12.216094032549728\n",
      "*** Training Epoch 3 with teacher forcing 0.6 :***\n",
      "Batch: 50\tTraining loss: 0.0828\n",
      "Batch: 50\tTraining perplexity: 1.0864\n",
      "Batch: 100\tTraining loss: 0.0699\n",
      "Batch: 100\tTraining perplexity: 1.0724\n",
      "Batch: 150\tTraining loss: 0.0732\n",
      "Batch: 150\tTraining perplexity: 1.0760\n",
      "Batch: 200\tTraining loss: 0.0699\n",
      "Batch: 200\tTraining perplexity: 1.0724\n",
      "Batch: 250\tTraining loss: 0.0559\n",
      "Batch: 250\tTraining perplexity: 1.0575\n",
      "Batch: 300\tTraining loss: 0.0772\n",
      "Batch: 300\tTraining perplexity: 1.0802\n",
      "Batch: 350\tTraining loss: 0.0537\n",
      "Batch: 350\tTraining perplexity: 1.0551\n",
      "****** Start validation ******\n",
      "\n",
      " Epoch average distance : 12.403254972875226\n",
      "*** Training Epoch 4 with teacher forcing 0.6 :***\n",
      "Batch: 50\tTraining loss: 0.0473\n",
      "Batch: 50\tTraining perplexity: 1.0484\n",
      "Batch: 100\tTraining loss: 0.0380\n",
      "Batch: 100\tTraining perplexity: 1.0388\n",
      "Batch: 150\tTraining loss: 0.1049\n",
      "Batch: 150\tTraining perplexity: 1.1106\n",
      "Batch: 200\tTraining loss: 0.0650\n",
      "Batch: 200\tTraining perplexity: 1.0672\n",
      "Batch: 250\tTraining loss: 0.0571\n",
      "Batch: 250\tTraining perplexity: 1.0587\n",
      "Batch: 300\tTraining loss: 0.0456\n",
      "Batch: 300\tTraining perplexity: 1.0467\n",
      "Batch: 350\tTraining loss: 0.0445\n",
      "Batch: 350\tTraining perplexity: 1.0455\n",
      "****** Start validation ******\n",
      "\n",
      " Epoch average distance : 12.236889692585896\n",
      "*** Training Epoch 5 with teacher forcing 0.6 :***\n",
      "Batch: 50\tTraining loss: 0.1136\n",
      "Batch: 50\tTraining perplexity: 1.1204\n",
      "Batch: 100\tTraining loss: 0.1884\n",
      "Batch: 100\tTraining perplexity: 1.2073\n",
      "Batch: 150\tTraining loss: 0.0653\n",
      "Batch: 150\tTraining perplexity: 1.0675\n",
      "Batch: 200\tTraining loss: 0.0455\n",
      "Batch: 200\tTraining perplexity: 1.0465\n",
      "Batch: 250\tTraining loss: 0.1221\n",
      "Batch: 250\tTraining perplexity: 1.1299\n",
      "Batch: 300\tTraining loss: 0.0629\n",
      "Batch: 300\tTraining perplexity: 1.0650\n",
      "Batch: 350\tTraining loss: 0.0868\n",
      "Batch: 350\tTraining perplexity: 1.0906\n",
      "****** Start validation ******\n",
      "\n",
      " Epoch average distance : 12.354430379746836\n",
      "*** Training Epoch 6 with teacher forcing 0.6 :***\n",
      "Batch: 50\tTraining loss: 0.0885\n",
      "Batch: 50\tTraining perplexity: 1.0926\n",
      "Batch: 100\tTraining loss: 0.0453\n",
      "Batch: 100\tTraining perplexity: 1.0464\n",
      "Batch: 150\tTraining loss: 0.0484\n",
      "Batch: 150\tTraining perplexity: 1.0496\n",
      "Batch: 200\tTraining loss: 0.0476\n",
      "Batch: 200\tTraining perplexity: 1.0488\n",
      "Batch: 250\tTraining loss: 0.0369\n",
      "Batch: 250\tTraining perplexity: 1.0376\n",
      "Batch: 300\tTraining loss: 0.0761\n",
      "Batch: 300\tTraining perplexity: 1.0790\n",
      "Batch: 350\tTraining loss: 0.0530\n",
      "Batch: 350\tTraining perplexity: 1.0544\n",
      "****** Start validation ******\n",
      "\n",
      " Epoch average distance : 12.491862567811935\n",
      "*** Training Epoch 7 with teacher forcing 0.6 :***\n",
      "Batch: 50\tTraining loss: 0.0467\n",
      "Batch: 50\tTraining perplexity: 1.0478\n",
      "Batch: 100\tTraining loss: 0.0624\n",
      "Batch: 100\tTraining perplexity: 1.0644\n",
      "Batch: 150\tTraining loss: 0.0821\n",
      "Batch: 150\tTraining perplexity: 1.0856\n",
      "Batch: 200\tTraining loss: 0.0749\n",
      "Batch: 200\tTraining perplexity: 1.0778\n",
      "Batch: 250\tTraining loss: 0.1090\n",
      "Batch: 250\tTraining perplexity: 1.1152\n",
      "Batch: 300\tTraining loss: 0.0474\n",
      "Batch: 300\tTraining perplexity: 1.0485\n",
      "Batch: 350\tTraining loss: 0.0606\n",
      "Batch: 350\tTraining perplexity: 1.0625\n",
      "****** Start validation ******\n",
      "\n",
      " Epoch average distance : 12.396021699819169\n",
      "*** Training Epoch 8 with teacher forcing 0.6 :***\n",
      "Batch: 50\tTraining loss: 0.0583\n",
      "Batch: 50\tTraining perplexity: 1.0601\n",
      "Batch: 100\tTraining loss: 0.0688\n",
      "Batch: 100\tTraining perplexity: 1.0713\n",
      "Batch: 150\tTraining loss: 0.0690\n",
      "Batch: 150\tTraining perplexity: 1.0714\n",
      "Batch: 200\tTraining loss: 0.0568\n",
      "Batch: 200\tTraining perplexity: 1.0584\n",
      "Batch: 250\tTraining loss: 0.0636\n",
      "Batch: 250\tTraining perplexity: 1.0656\n",
      "Batch: 300\tTraining loss: 0.0547\n",
      "Batch: 300\tTraining perplexity: 1.0562\n",
      "Batch: 350\tTraining loss: 0.1245\n",
      "Batch: 350\tTraining perplexity: 1.1326\n",
      "****** Start validation ******\n",
      "\n",
      " Epoch average distance : 12.374321880650994\n",
      "*** Training Epoch 9 with teacher forcing 0.6 :***\n",
      "Batch: 50\tTraining loss: 0.0832\n",
      "Batch: 50\tTraining perplexity: 1.0867\n",
      "Batch: 100\tTraining loss: 0.0506\n",
      "Batch: 100\tTraining perplexity: 1.0519\n",
      "Batch: 150\tTraining loss: 0.0586\n",
      "Batch: 150\tTraining perplexity: 1.0604\n",
      "Batch: 200\tTraining loss: 0.0571\n",
      "Batch: 200\tTraining perplexity: 1.0587\n",
      "Batch: 250\tTraining loss: 0.1060\n",
      "Batch: 250\tTraining perplexity: 1.1119\n",
      "Batch: 300\tTraining loss: 0.0746\n",
      "Batch: 300\tTraining perplexity: 1.0774\n",
      "Batch: 350\tTraining loss: 0.0495\n",
      "Batch: 350\tTraining perplexity: 1.0508\n",
      "****** Start validation ******\n",
      "\n",
      " Epoch average distance : 12.632007233273056\n",
      "*** Training Epoch 10 with teacher forcing 0.6 :***\n",
      "Batch: 50\tTraining loss: 0.0429\n",
      "Batch: 50\tTraining perplexity: 1.0439\n",
      "Batch: 100\tTraining loss: 0.0789\n",
      "Batch: 100\tTraining perplexity: 1.0821\n",
      "Batch: 150\tTraining loss: 0.0538\n",
      "Batch: 150\tTraining perplexity: 1.0552\n",
      "Batch: 200\tTraining loss: 0.0691\n",
      "Batch: 200\tTraining perplexity: 1.0716\n",
      "Batch: 250\tTraining loss: 0.0648\n",
      "Batch: 250\tTraining perplexity: 1.0669\n",
      "Batch: 300\tTraining loss: 0.0869\n",
      "Batch: 300\tTraining perplexity: 1.0908\n",
      "Batch: 350\tTraining loss: 0.0970\n",
      "Batch: 350\tTraining perplexity: 1.1019\n",
      "****** Start validation ******\n",
      "\n",
      " Epoch average distance : 12.638336347197107\n"
     ]
    }
   ],
   "source": [
    "numEpochs = 10\n",
    "learningRate = 3e-5\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)\n",
    "train(model, train_loader, numEpochs, criterion, optimizer, one_epoch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ye0l7_j0RCJc"
   },
   "source": [
    "# ***Moving Forward....***\n",
    "\n",
    "We have provided a skeleton to begin with, so that you have a clear picture when writing the code. Apart from the methods given here following methods should be implemented.\n",
    "  \n",
    "*    Validation and test methods.\n",
    "*    Methods to convert indexes to characters\n",
    "*    Methods for calculating the perplexity/Levenstine distance for gauging the training routine.\n",
    "*    For visualizing the gradient flow (refer to FAQs) and attention graph (refer to Recitation-9) methods are already given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GNOzubmNXDLu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Seq2Seq. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type pBLSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Decoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Attention. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, 'hw4_epoch53.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** Testing ******\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "generated_sentence = []\n",
    "\n",
    "def predict(model, test_loader):\n",
    "    \n",
    "    teacher_forcing_para=None\n",
    "  \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        \n",
    "        print('****** Testing ******')\n",
    "\n",
    "        for (batch_num, collate_output) in enumerate(test_loader):\n",
    "\n",
    "            speech_input, speech_len = collate_output\n",
    "            speech_input = speech_input.to(device)\n",
    "            predictions = model(teacher_forcing_para, speech_input, speech_len ,text_input=None, train=False)\n",
    "          \n",
    "            for i in range(len(speech_len)):\n",
    "                pred = torch.argmax(predictions[i], dim=1).cpu().numpy()\n",
    "                pred_word = index_to_char(pred)\n",
    "                generated_sentence.append(pred_word)\n",
    "        assert(len(generated_sentence) == 523)\n",
    "                \n",
    "predict(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "result = pd.DataFrame()\n",
    "result['Id'] = list(range(523))\n",
    "result['Predicted'] = generated_sentence\n",
    "result.to_csv('zichenli_hw4p2_4.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "zichenli_hw4p2_baseline.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
