{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZAZVG_nv87K8"
   },
   "source": [
    "5 LSTM layer, bi-directional, 4 linear layer, 512 hidden size, add dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 335,
     "status": "ok",
     "timestamp": 1573314833557,
     "user": {
      "displayName": "Zichen Li",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mArxVcsqOTvpJJhNNsbsrzoYSAdI2qr04Wp2U6LtA=s64",
      "userId": "09393985127784237998"
     },
     "user_tz": 300
    },
    "id": "hjRHKPpHc1VV",
    "outputId": "efda9971-c919-4069-efde-32ac8948f73b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n"
     ]
    }
   ],
   "source": [
    "PHONEME_LIST = [\n",
    "    \"+BREATH+\",\n",
    "    \"+COUGH+\",\n",
    "    \"+NOISE+\",\n",
    "    \"+SMACK+\",\n",
    "    \"+UH+\",\n",
    "    \"+UM+\",\n",
    "    \"AA\",\n",
    "    \"AE\",\n",
    "    \"AH\",\n",
    "    \"AO\",\n",
    "    \"AW\",\n",
    "    \"AY\",\n",
    "    \"B\",\n",
    "    \"CH\",\n",
    "    \"D\",\n",
    "    \"DH\",\n",
    "    \"EH\",\n",
    "    \"ER\",\n",
    "    \"EY\",\n",
    "    \"F\",\n",
    "    \"G\",\n",
    "    \"HH\",\n",
    "    \"IH\",\n",
    "    \"IY\",\n",
    "    \"JH\",\n",
    "    \"K\",\n",
    "    \"L\",\n",
    "    \"M\",\n",
    "    \"N\",\n",
    "    \"NG\",\n",
    "    \"OW\",\n",
    "    \"OY\",\n",
    "    \"P\",\n",
    "    \"R\",\n",
    "    \"S\",\n",
    "    \"SH\",\n",
    "    \"SIL\",\n",
    "    \"T\",\n",
    "    \"TH\",\n",
    "    \"UH\",\n",
    "    \"UW\",\n",
    "    \"V\",\n",
    "    \"W\",\n",
    "    \"Y\",\n",
    "    \"Z\",\n",
    "    \"ZH\"\n",
    "]\n",
    "\n",
    "num_classes = len(PHONEME_LIST) + 1\n",
    "print(num_classes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HS-m5Dx4zehl"
   },
   "outputs": [],
   "source": [
    "PHONEME_MAP = [\n",
    "    '_',  # \"+BREATH+\"\n",
    "    '+',  # \"+COUGH+\"\n",
    "    '~',  # \"+NOISE+\"\n",
    "    '!',  # \"+SMACK+\"\n",
    "    '-',  # \"+UH+\"\n",
    "    '@',  # \"+UM+\"\n",
    "    'a',  # \"AA\"\n",
    "    'A',  # \"AE\"\n",
    "    'h',  # \"AH\"\n",
    "    'o',  # \"AO\"\n",
    "    'w',  # \"AW\"\n",
    "    'y',  # \"AY\"\n",
    "    'b',  # \"B\"\n",
    "    'c',  # \"CH\"\n",
    "    'd',  # \"D\"\n",
    "    'D',  # \"DH\"\n",
    "    'e',  # \"EH\"\n",
    "    'r',  # \"ER\"\n",
    "    'E',  # \"EY\"\n",
    "    'f',  # \"F\"\n",
    "    'g',  # \"G\"\n",
    "    'H',  # \"HH\"\n",
    "    'i',  # \"IH\"\n",
    "    'I',  # \"IY\"\n",
    "    'j',  # \"JH\"\n",
    "    'k',  # \"K\"\n",
    "    'l',  # \"L\"\n",
    "    'm',  # \"M\"\n",
    "    'n',  # \"N\"\n",
    "    'G',  # \"NG\"\n",
    "    'O',  # \"OW\"\n",
    "    'Y',  # \"OY\"\n",
    "    'p',  # \"P\"\n",
    "    'R',  # \"R\"\n",
    "    's',  # \"S\"\n",
    "    'S',  # \"SH\"\n",
    "    '.',  # \"SIL\"\n",
    "    't',  # \"T\"\n",
    "    'T',  # \"TH\"\n",
    "    'u',  # \"UH\"\n",
    "    'U',  # \"UW\"\n",
    "    'v',  # \"V\"\n",
    "    'W',  # \"W\"\n",
    "    '?',  # \"Y\"\n",
    "    'z',  # \"Z\"\n",
    "    'Z',  # \"ZH\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19982,
     "status": "ok",
     "timestamp": 1573314855805,
     "user": {
      "displayName": "Zichen Li",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mArxVcsqOTvpJJhNNsbsrzoYSAdI2qr04Wp2U6LtA=s64",
      "userId": "09393985127784237998"
     },
     "user_tz": 300
    },
    "id": "3e1CfwnIFXFM",
    "outputId": "400baed2-b045-4806-bb35-3e69810eb3c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hTkfC1aNFlPZ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile('/content/gdrive/My Drive/11785hw3/homework-3-part-2-11-785-fall-2019.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 42274,
     "status": "ok",
     "timestamp": 1573314909357,
     "user": {
      "displayName": "Zichen Li",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mArxVcsqOTvpJJhNNsbsrzoYSAdI2qr04Wp2U6LtA=s64",
      "userId": "09393985127784237998"
     },
     "user_tz": 300
    },
    "id": "Cz4vO21uxTyy",
    "outputId": "68955374-0cfe-44d9-83b5-2acfc0c05e68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phoneme_list.py\t\t    wsj0_dev.npy\t\t  wsj0_train.npy\n",
      "sample_submission.csv\t    wsj0_test.npy\n",
      "wsj0_dev_merged_labels.npy  wsj0_train_merged_labels.npy\n"
     ]
    }
   ],
   "source": [
    "!ls HW3P2_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 41939,
     "status": "ok",
     "timestamp": 1573314910767,
     "user": {
      "displayName": "Zichen Li",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mArxVcsqOTvpJJhNNsbsrzoYSAdI2qr04Wp2U6LtA=s64",
      "userId": "09393985127784237998"
     },
     "user_tz": 300
    },
    "id": "5mVCsbxj0cDY",
    "outputId": "5b38e5ff-66b3-4c2b-a231-c7ad7264c548"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import *\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.utils.rnn as rnn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import time\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46726,
     "status": "ok",
     "timestamp": 1573314916101,
     "user": {
      "displayName": "Zichen Li",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mArxVcsqOTvpJJhNNsbsrzoYSAdI2qr04Wp2U6LtA=s64",
      "userId": "09393985127784237998"
     },
     "user_tz": 300
    },
    "id": "oKSKTFUlxWnZ",
    "outputId": "34b8c2a0-a930-496d-8ecd-f58b3916763d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24724,)\n",
      "(1106,)\n",
      "(523,)\n"
     ]
    }
   ],
   "source": [
    "class WSJ():\n",
    "    \"\"\" Load the WSJ speech dataset\n",
    "        \n",
    "        Ensure WSJ_PATH is path to directory containing \n",
    "        all data files (.npy) provided on Kaggle.\n",
    "            \n",
    "    \"\"\"\n",
    "  \n",
    "    def __init__(self):\n",
    "        self.dev_set = None\n",
    "        self.train_set = None\n",
    "        self.test_set = None\n",
    "  \n",
    "    @property\n",
    "    def dev(self):\n",
    "        if self.dev_set is None:\n",
    "            self.dev_set = load_raw(os.environ['WSJ_PATH'], 'wsj0_dev')\n",
    "        return self.dev_set\n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "        if self.train_set is None:\n",
    "            self.train_set = load_raw(os.environ['WSJ_PATH'], 'wsj0_train')\n",
    "        return self.train_set\n",
    "  \n",
    "    @property\n",
    "    def test(self):\n",
    "        if self.test_set is None:\n",
    "            self.test_set = (np.load(os.path.join(os.environ['WSJ_PATH'], 'wsj0_test.npy'), encoding='bytes',allow_pickle=True), None)\n",
    "        return self.test_set\n",
    "    \n",
    "def load_raw(path, name):\n",
    "    return (\n",
    "        np.load(os.path.join(path, '{}.npy'.format(name)), encoding='bytes',allow_pickle=True), \n",
    "        np.load(os.path.join(path, '{}_merged_labels.npy'.format(name)), encoding='bytes',allow_pickle=True)\n",
    "    )\n",
    "\n",
    "os.environ['WSJ_PATH'] = 'HW3P2_Data'\n",
    "loader = WSJ()\n",
    "trainX, trainY = loader.train\n",
    "print(trainX.shape)\n",
    "evalX, evalY = loader.dev\n",
    "print(evalX.shape)\n",
    "testX, testY = loader.test\n",
    "print(testX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d3N3Mr7O2L36"
   },
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, frames, labels):\n",
    "        self.frames=[torch.tensor(f) for f in frames]\n",
    "        self.labels = [torch.tensor(l) for l in labels]\n",
    "    def __getitem__(self,i):\n",
    "        time_steps = self.frames[i]\n",
    "        labels_original = self.labels[i]\n",
    "        labels_add1 = torch.LongTensor([label + 1 for label in labels_original])\n",
    "        \n",
    "        return time_steps.to(device), labels_add1.to(device)\n",
    "    def __len__(self):\n",
    "        return len(self.frames)\n",
    "\n",
    "def collate_lines(seq_list):\n",
    "    inputs, targets = zip(*seq_list)\n",
    "    X_lens = torch.LongTensor([len(seq) for seq in inputs])\n",
    "    Y_lens = torch.LongTensor([len(seq) for seq in targets])    \n",
    "    X = pad_sequence(inputs)\n",
    "    # batch_first = True is required for use in nn.CTCLoss.\n",
    "    Y = pad_sequence(targets, batch_first=True)    \n",
    "    return X, Y, X_lens, Y_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6nzfXUqHbKim"
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset(trainX, trainY)\n",
    "eval_dataset = Dataset(evalX, evalY)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=32, collate_fn = collate_lines)\n",
    "eval_loader = DataLoader(eval_dataset, shuffle=True, batch_size=32, collate_fn = collate_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bnZRd6Qu-YY0"
   },
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, num_classes, hidden_size):\n",
    "        super(Network, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.num_classes = num_classes\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.LSTM(input_size = input_size, hidden_size = hidden_size, dropout=0.5, num_layers=5, bidirectional=True) # defaule 1 layer, batch_first = False\n",
    "        self.scoring1 = nn.Linear(hidden_size * 2, 2048)\n",
    "        self.scoring2 = nn.Linear(2048, 1024)\n",
    "        self.scoring3 = nn.Linear(1024, 512)\n",
    "        self.scoring4 = nn.Linear(512, num_classes)\n",
    "    \n",
    "    def forward(self, seq_list, X_lens):\n",
    "\n",
    "        packed_input = rnn.pack_padded_sequence(seq_list, X_lens, enforce_sorted=False)\n",
    "        \n",
    "        hidden = None\n",
    "        output_packed, output_hidden = self.rnn(packed_input, hidden)\n",
    "        output_padded, out_lens = rnn.pad_packed_sequence(output_packed)\n",
    "        scores = self.scoring1(output_padded)\n",
    "        scores = self.scoring2(scores)\n",
    "        scores = self.scoring3(scores)\n",
    "        scores = self.scoring4(scores)\n",
    "        \n",
    "        # Log softmax after output layer is required for use in `nn.CTCLoss`.\n",
    "        out = scores.log_softmax(2)\n",
    "        \n",
    "        return out, out_lens\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_normal_(m.weight.data)\n",
    "    if type(m) == nn.LSTM:\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                torch.nn.init.constant_(param, 0.0)\n",
    "            elif 'weight' in name:\n",
    "                torch.nn.init.xavier_normal_(param)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qsb5P0dsE9ve"
   },
   "outputs": [],
   "source": [
    "def train_epoch_packed(model, train_loader, eval_loader):\n",
    "  \n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(numEpochs):\n",
    "        avg_loss = 0.0\n",
    "        for batch_num, (inputs, targets, X_lens, Y_lens) in enumerate(train_loader):\n",
    "            #inputs, targets, Y_lens = inputs.to(device), targets.to(device), Y_lens.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs, out_lens = model(inputs, X_lens)\n",
    "            loss = criterion(outputs, targets, out_lens, Y_lens)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "      \n",
    "            avg_loss += loss.item()\n",
    "            if batch_num % 50 == 49:\n",
    "                print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch+1, batch_num+1, avg_loss/50))\n",
    "                print('Epoch: {}\\tBatch: {}\\tTraining perplexity: {:.4f}'.format(epoch+1, batch_num+1, np.exp(avg_loss/50)))\n",
    "                avg_loss = 0.0\n",
    "                \n",
    "            torch.cuda.empty_cache()\n",
    "            del inputs\n",
    "            del targets\n",
    "            del Y_lens\n",
    "            del loss\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "\n",
    "        for batch_num, (inputs, targets, X_lens, Y_lens) in enumerate(eval_loader):\n",
    "\n",
    "            outputs, out_lens = model(inputs, X_lens)\n",
    "            loss = criterion(outputs, targets, out_lens, Y_lens)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "        val_loss /= len(eval_loader)\n",
    "        print(\"\\nValidation loss:\",val_loss)\n",
    "        print(\"Validation perplexity :\",np.exp(val_loss),\"\\n\")\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cpr_u6I8b0c6"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(11785)\n",
    "\n",
    "learningRate = 1e-3\n",
    "\n",
    "model = Network(40, num_classes, 512)\n",
    "model.apply(init_weights)\n",
    "#model = torch.load('hw3p2_5_dropout_epoch5.pth')\n",
    "\n",
    "criterion = nn.CTCLoss()\n",
    "\n",
    "criterion = criterion.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3027746,
     "status": "error",
     "timestamp": 1573186045874,
     "user": {
      "displayName": "Zichen Li",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mArxVcsqOTvpJJhNNsbsrzoYSAdI2qr04Wp2U6LtA=s64",
      "userId": "09393985127784237998"
     },
     "user_tz": 300
    },
    "id": "gLPFvQe4cvAj",
    "outputId": "7d9be02d-32bf-45e4-83a0-3e413bc59b5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tBatch: 50\tAvg-Loss: 5.4373\n",
      "Epoch: 1\tBatch: 50\tTraining perplexity: 229.8291\n",
      "Epoch: 1\tBatch: 100\tAvg-Loss: 3.4230\n",
      "Epoch: 1\tBatch: 100\tTraining perplexity: 30.6627\n",
      "Epoch: 1\tBatch: 150\tAvg-Loss: 3.1679\n",
      "Epoch: 1\tBatch: 150\tTraining perplexity: 23.7566\n",
      "Epoch: 1\tBatch: 200\tAvg-Loss: 2.9701\n",
      "Epoch: 1\tBatch: 200\tTraining perplexity: 19.4947\n",
      "Epoch: 1\tBatch: 250\tAvg-Loss: 2.5878\n",
      "Epoch: 1\tBatch: 250\tTraining perplexity: 13.3002\n",
      "Epoch: 1\tBatch: 300\tAvg-Loss: 2.1565\n",
      "Epoch: 1\tBatch: 300\tTraining perplexity: 8.6408\n",
      "Epoch: 1\tBatch: 350\tAvg-Loss: 1.8159\n",
      "Epoch: 1\tBatch: 350\tTraining perplexity: 6.1463\n",
      "Epoch: 1\tBatch: 400\tAvg-Loss: 1.5728\n",
      "Epoch: 1\tBatch: 400\tTraining perplexity: 4.8203\n",
      "Epoch: 1\tBatch: 450\tAvg-Loss: 1.4003\n",
      "Epoch: 1\tBatch: 450\tTraining perplexity: 4.0565\n",
      "Epoch: 1\tBatch: 500\tAvg-Loss: 1.3027\n",
      "Epoch: 1\tBatch: 500\tTraining perplexity: 3.6790\n",
      "Epoch: 1\tBatch: 550\tAvg-Loss: 1.3082\n",
      "Epoch: 1\tBatch: 550\tTraining perplexity: 3.6995\n",
      "Epoch: 1\tBatch: 600\tAvg-Loss: 1.1638\n",
      "Epoch: 1\tBatch: 600\tTraining perplexity: 3.2020\n",
      "Epoch: 1\tBatch: 650\tAvg-Loss: 1.1136\n",
      "Epoch: 1\tBatch: 650\tTraining perplexity: 3.0454\n",
      "Epoch: 1\tBatch: 700\tAvg-Loss: 1.0916\n",
      "Epoch: 1\tBatch: 700\tTraining perplexity: 2.9790\n",
      "Epoch: 1\tBatch: 750\tAvg-Loss: 0.9948\n",
      "Epoch: 1\tBatch: 750\tTraining perplexity: 2.7042\n",
      "\n",
      "Validation loss: 0.9693490811756679\n",
      "Validation perplexity : 2.6362279303691376 \n",
      "\n",
      "Epoch: 2\tBatch: 50\tAvg-Loss: 0.9340\n",
      "Epoch: 2\tBatch: 50\tTraining perplexity: 2.5447\n",
      "Epoch: 2\tBatch: 100\tAvg-Loss: 0.9244\n",
      "Epoch: 2\tBatch: 100\tTraining perplexity: 2.5204\n",
      "Epoch: 2\tBatch: 150\tAvg-Loss: 0.8890\n",
      "Epoch: 2\tBatch: 150\tTraining perplexity: 2.4328\n",
      "Epoch: 2\tBatch: 200\tAvg-Loss: 0.8877\n",
      "Epoch: 2\tBatch: 200\tTraining perplexity: 2.4295\n",
      "Epoch: 2\tBatch: 250\tAvg-Loss: 0.8690\n",
      "Epoch: 2\tBatch: 250\tTraining perplexity: 2.3846\n",
      "Epoch: 2\tBatch: 300\tAvg-Loss: 0.8361\n",
      "Epoch: 2\tBatch: 300\tTraining perplexity: 2.3073\n",
      "Epoch: 2\tBatch: 350\tAvg-Loss: 0.8499\n",
      "Epoch: 2\tBatch: 350\tTraining perplexity: 2.3394\n",
      "Epoch: 2\tBatch: 400\tAvg-Loss: 0.8132\n",
      "Epoch: 2\tBatch: 400\tTraining perplexity: 2.2551\n",
      "Epoch: 2\tBatch: 450\tAvg-Loss: 0.7886\n",
      "Epoch: 2\tBatch: 450\tTraining perplexity: 2.2003\n",
      "Epoch: 2\tBatch: 500\tAvg-Loss: 0.7937\n",
      "Epoch: 2\tBatch: 500\tTraining perplexity: 2.2115\n",
      "Epoch: 2\tBatch: 550\tAvg-Loss: 0.7981\n",
      "Epoch: 2\tBatch: 550\tTraining perplexity: 2.2212\n",
      "Epoch: 2\tBatch: 600\tAvg-Loss: 0.7983\n",
      "Epoch: 2\tBatch: 600\tTraining perplexity: 2.2217\n",
      "Epoch: 2\tBatch: 650\tAvg-Loss: 0.7101\n",
      "Epoch: 2\tBatch: 650\tTraining perplexity: 2.0343\n",
      "Epoch: 2\tBatch: 700\tAvg-Loss: 0.7375\n",
      "Epoch: 2\tBatch: 700\tTraining perplexity: 2.0907\n",
      "Epoch: 2\tBatch: 750\tAvg-Loss: 0.7808\n",
      "Epoch: 2\tBatch: 750\tTraining perplexity: 2.1832\n",
      "\n",
      "Validation loss: 0.680633304800306\n",
      "Validation perplexity : 1.9751281943935375 \n",
      "\n",
      "Epoch: 3\tBatch: 50\tAvg-Loss: 0.6546\n",
      "Epoch: 3\tBatch: 50\tTraining perplexity: 1.9244\n",
      "Epoch: 3\tBatch: 100\tAvg-Loss: 0.6562\n",
      "Epoch: 3\tBatch: 100\tTraining perplexity: 1.9275\n",
      "Epoch: 3\tBatch: 150\tAvg-Loss: 0.7029\n",
      "Epoch: 3\tBatch: 150\tTraining perplexity: 2.0197\n",
      "Epoch: 3\tBatch: 200\tAvg-Loss: 0.6543\n",
      "Epoch: 3\tBatch: 200\tTraining perplexity: 1.9237\n",
      "Epoch: 3\tBatch: 250\tAvg-Loss: 0.6529\n",
      "Epoch: 3\tBatch: 250\tTraining perplexity: 1.9211\n",
      "Epoch: 3\tBatch: 300\tAvg-Loss: 0.6529\n",
      "Epoch: 3\tBatch: 300\tTraining perplexity: 1.9211\n",
      "Epoch: 3\tBatch: 350\tAvg-Loss: 0.6471\n",
      "Epoch: 3\tBatch: 350\tTraining perplexity: 1.9100\n",
      "Epoch: 3\tBatch: 400\tAvg-Loss: 0.6586\n",
      "Epoch: 3\tBatch: 400\tTraining perplexity: 1.9322\n",
      "Epoch: 3\tBatch: 450\tAvg-Loss: 0.6089\n",
      "Epoch: 3\tBatch: 450\tTraining perplexity: 1.8385\n",
      "Epoch: 3\tBatch: 500\tAvg-Loss: 0.6389\n",
      "Epoch: 3\tBatch: 500\tTraining perplexity: 1.8943\n",
      "Epoch: 3\tBatch: 550\tAvg-Loss: 0.6976\n",
      "Epoch: 3\tBatch: 550\tTraining perplexity: 2.0089\n",
      "Epoch: 3\tBatch: 600\tAvg-Loss: 0.6702\n",
      "Epoch: 3\tBatch: 600\tTraining perplexity: 1.9547\n",
      "Epoch: 3\tBatch: 650\tAvg-Loss: 0.6214\n",
      "Epoch: 3\tBatch: 650\tTraining perplexity: 1.8616\n",
      "Epoch: 3\tBatch: 700\tAvg-Loss: 0.5969\n",
      "Epoch: 3\tBatch: 700\tTraining perplexity: 1.8164\n",
      "Epoch: 3\tBatch: 750\tAvg-Loss: 0.6295\n",
      "Epoch: 3\tBatch: 750\tTraining perplexity: 1.8766\n",
      "\n",
      "Validation loss: 0.578886319909777\n",
      "Validation perplexity : 1.784050462495177 \n",
      "\n",
      "Epoch: 4\tBatch: 50\tAvg-Loss: 0.6173\n",
      "Epoch: 4\tBatch: 50\tTraining perplexity: 1.8538\n",
      "Epoch: 4\tBatch: 100\tAvg-Loss: 0.6404\n",
      "Epoch: 4\tBatch: 100\tTraining perplexity: 1.8973\n",
      "Epoch: 4\tBatch: 150\tAvg-Loss: 0.5912\n",
      "Epoch: 4\tBatch: 150\tTraining perplexity: 1.8061\n",
      "Epoch: 4\tBatch: 200\tAvg-Loss: 0.5607\n",
      "Epoch: 4\tBatch: 200\tTraining perplexity: 1.7520\n",
      "Epoch: 4\tBatch: 250\tAvg-Loss: 0.6231\n",
      "Epoch: 4\tBatch: 250\tTraining perplexity: 1.8647\n",
      "Epoch: 4\tBatch: 300\tAvg-Loss: 0.5487\n",
      "Epoch: 4\tBatch: 300\tTraining perplexity: 1.7310\n",
      "Epoch: 4\tBatch: 350\tAvg-Loss: 0.5557\n",
      "Epoch: 4\tBatch: 350\tTraining perplexity: 1.7431\n",
      "Epoch: 4\tBatch: 400\tAvg-Loss: 0.5847\n",
      "Epoch: 4\tBatch: 400\tTraining perplexity: 1.7944\n",
      "Epoch: 4\tBatch: 450\tAvg-Loss: 0.5346\n",
      "Epoch: 4\tBatch: 450\tTraining perplexity: 1.7068\n",
      "Epoch: 4\tBatch: 500\tAvg-Loss: 0.5239\n",
      "Epoch: 4\tBatch: 500\tTraining perplexity: 1.6885\n",
      "Epoch: 4\tBatch: 550\tAvg-Loss: 0.5375\n",
      "Epoch: 4\tBatch: 550\tTraining perplexity: 1.7118\n",
      "Epoch: 4\tBatch: 600\tAvg-Loss: 0.5000\n",
      "Epoch: 4\tBatch: 600\tTraining perplexity: 1.6487\n",
      "Epoch: 4\tBatch: 650\tAvg-Loss: 0.5597\n",
      "Epoch: 4\tBatch: 650\tTraining perplexity: 1.7501\n",
      "Epoch: 4\tBatch: 700\tAvg-Loss: 0.5803\n",
      "Epoch: 4\tBatch: 700\tTraining perplexity: 1.7866\n",
      "Epoch: 4\tBatch: 750\tAvg-Loss: 0.5647\n",
      "Epoch: 4\tBatch: 750\tTraining perplexity: 1.7590\n",
      "\n",
      "Validation loss: 0.4991647720336914\n",
      "Validation perplexity : 1.6473447875048417 \n",
      "\n",
      "Epoch: 5\tBatch: 50\tAvg-Loss: 0.5364\n",
      "Epoch: 5\tBatch: 50\tTraining perplexity: 1.7098\n",
      "Epoch: 5\tBatch: 100\tAvg-Loss: 0.5370\n",
      "Epoch: 5\tBatch: 100\tTraining perplexity: 1.7109\n",
      "Epoch: 5\tBatch: 150\tAvg-Loss: 0.5225\n",
      "Epoch: 5\tBatch: 150\tTraining perplexity: 1.6863\n",
      "Epoch: 5\tBatch: 200\tAvg-Loss: 0.5422\n",
      "Epoch: 5\tBatch: 200\tTraining perplexity: 1.7198\n",
      "Epoch: 5\tBatch: 250\tAvg-Loss: 0.5569\n",
      "Epoch: 5\tBatch: 250\tTraining perplexity: 1.7452\n",
      "Epoch: 5\tBatch: 300\tAvg-Loss: 0.5247\n",
      "Epoch: 5\tBatch: 300\tTraining perplexity: 1.6899\n",
      "Epoch: 5\tBatch: 350\tAvg-Loss: 0.5078\n",
      "Epoch: 5\tBatch: 350\tTraining perplexity: 1.6616\n",
      "Epoch: 5\tBatch: 400\tAvg-Loss: 0.5027\n",
      "Epoch: 5\tBatch: 400\tTraining perplexity: 1.6531\n",
      "Epoch: 5\tBatch: 450\tAvg-Loss: 0.5506\n",
      "Epoch: 5\tBatch: 450\tTraining perplexity: 1.7343\n",
      "Epoch: 5\tBatch: 500\tAvg-Loss: 0.5422\n",
      "Epoch: 5\tBatch: 500\tTraining perplexity: 1.7197\n",
      "Epoch: 5\tBatch: 550\tAvg-Loss: 0.5371\n",
      "Epoch: 5\tBatch: 550\tTraining perplexity: 1.7111\n",
      "Epoch: 5\tBatch: 600\tAvg-Loss: 0.4943\n",
      "Epoch: 5\tBatch: 600\tTraining perplexity: 1.6393\n",
      "Epoch: 5\tBatch: 650\tAvg-Loss: 0.5083\n",
      "Epoch: 5\tBatch: 650\tTraining perplexity: 1.6625\n",
      "Epoch: 5\tBatch: 700\tAvg-Loss: 0.4928\n",
      "Epoch: 5\tBatch: 700\tTraining perplexity: 1.6369\n",
      "Epoch: 5\tBatch: 750\tAvg-Loss: 0.5646\n",
      "Epoch: 5\tBatch: 750\tTraining perplexity: 1.7587\n",
      "\n",
      "Validation loss: 0.48654526727540154\n",
      "Validation perplexity : 1.6266867334780601 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#epoch 5\n",
    "numEpochs = 5\n",
    "model.to(device)\n",
    "train_epoch_packed(model, train_loader, eval_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3320100,
     "status": "ok",
     "timestamp": 1573202328818,
     "user": {
      "displayName": "Zichen Li",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mArxVcsqOTvpJJhNNsbsrzoYSAdI2qr04Wp2U6LtA=s64",
      "userId": "09393985127784237998"
     },
     "user_tz": 300
    },
    "id": "fXH6rqZKYDn6",
    "outputId": "b6e4e2be-e8b4-4c6d-e443-2fc183b2b7b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tBatch: 50\tAvg-Loss: 0.5350\n",
      "Epoch: 1\tBatch: 50\tTraining perplexity: 1.7075\n",
      "Epoch: 1\tBatch: 100\tAvg-Loss: 0.5199\n",
      "Epoch: 1\tBatch: 100\tTraining perplexity: 1.6819\n",
      "Epoch: 1\tBatch: 150\tAvg-Loss: 0.5703\n",
      "Epoch: 1\tBatch: 150\tTraining perplexity: 1.7687\n",
      "Epoch: 1\tBatch: 200\tAvg-Loss: 0.5603\n",
      "Epoch: 1\tBatch: 200\tTraining perplexity: 1.7512\n",
      "Epoch: 1\tBatch: 250\tAvg-Loss: 0.5155\n",
      "Epoch: 1\tBatch: 250\tTraining perplexity: 1.6744\n",
      "Epoch: 1\tBatch: 300\tAvg-Loss: 0.5216\n",
      "Epoch: 1\tBatch: 300\tTraining perplexity: 1.6847\n",
      "Epoch: 1\tBatch: 350\tAvg-Loss: 0.4841\n",
      "Epoch: 1\tBatch: 350\tTraining perplexity: 1.6227\n",
      "Epoch: 1\tBatch: 400\tAvg-Loss: 0.5135\n",
      "Epoch: 1\tBatch: 400\tTraining perplexity: 1.6712\n",
      "Epoch: 1\tBatch: 450\tAvg-Loss: 0.4616\n",
      "Epoch: 1\tBatch: 450\tTraining perplexity: 1.5866\n",
      "Epoch: 1\tBatch: 500\tAvg-Loss: 0.4759\n",
      "Epoch: 1\tBatch: 500\tTraining perplexity: 1.6095\n",
      "Epoch: 1\tBatch: 550\tAvg-Loss: 0.4603\n",
      "Epoch: 1\tBatch: 550\tTraining perplexity: 1.5845\n",
      "Epoch: 1\tBatch: 600\tAvg-Loss: 0.5175\n",
      "Epoch: 1\tBatch: 600\tTraining perplexity: 1.6778\n",
      "Epoch: 1\tBatch: 650\tAvg-Loss: 0.4919\n",
      "Epoch: 1\tBatch: 650\tTraining perplexity: 1.6354\n",
      "Epoch: 1\tBatch: 700\tAvg-Loss: 0.4833\n",
      "Epoch: 1\tBatch: 700\tTraining perplexity: 1.6213\n",
      "Epoch: 1\tBatch: 750\tAvg-Loss: 0.4998\n",
      "Epoch: 1\tBatch: 750\tTraining perplexity: 1.6484\n",
      "\n",
      "Validation loss: 0.45583586863109044\n",
      "Validation perplexity : 1.5774914076930233 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#epoch 6\n",
    "numEpochs = 1\n",
    "learningRate = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)\n",
    "train_epoch_packed(model, train_loader, eval_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6660103,
     "status": "ok",
     "timestamp": 1573205668828,
     "user": {
      "displayName": "Zichen Li",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mArxVcsqOTvpJJhNNsbsrzoYSAdI2qr04Wp2U6LtA=s64",
      "userId": "09393985127784237998"
     },
     "user_tz": 300
    },
    "id": "PoTtCv1PUXfi",
    "outputId": "a5060f33-ba00-431f-e4db-d21ff13624a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tBatch: 50\tAvg-Loss: 0.4628\n",
      "Epoch: 1\tBatch: 50\tTraining perplexity: 1.5886\n",
      "Epoch: 1\tBatch: 100\tAvg-Loss: 0.4700\n",
      "Epoch: 1\tBatch: 100\tTraining perplexity: 1.5999\n",
      "Epoch: 1\tBatch: 150\tAvg-Loss: 0.4785\n",
      "Epoch: 1\tBatch: 150\tTraining perplexity: 1.6137\n",
      "Epoch: 1\tBatch: 200\tAvg-Loss: 0.4610\n",
      "Epoch: 1\tBatch: 200\tTraining perplexity: 1.5857\n",
      "Epoch: 1\tBatch: 250\tAvg-Loss: 0.4618\n",
      "Epoch: 1\tBatch: 250\tTraining perplexity: 1.5870\n",
      "Epoch: 1\tBatch: 300\tAvg-Loss: 0.4272\n",
      "Epoch: 1\tBatch: 300\tTraining perplexity: 1.5330\n",
      "Epoch: 1\tBatch: 350\tAvg-Loss: 0.5190\n",
      "Epoch: 1\tBatch: 350\tTraining perplexity: 1.6803\n",
      "Epoch: 1\tBatch: 400\tAvg-Loss: 0.5509\n",
      "Epoch: 1\tBatch: 400\tTraining perplexity: 1.7348\n",
      "Epoch: 1\tBatch: 450\tAvg-Loss: 0.4645\n",
      "Epoch: 1\tBatch: 450\tTraining perplexity: 1.5911\n",
      "Epoch: 1\tBatch: 500\tAvg-Loss: 0.4679\n",
      "Epoch: 1\tBatch: 500\tTraining perplexity: 1.5966\n",
      "Epoch: 1\tBatch: 550\tAvg-Loss: 0.4653\n",
      "Epoch: 1\tBatch: 550\tTraining perplexity: 1.5926\n",
      "Epoch: 1\tBatch: 600\tAvg-Loss: 0.4309\n",
      "Epoch: 1\tBatch: 600\tTraining perplexity: 1.5386\n",
      "Epoch: 1\tBatch: 650\tAvg-Loss: 0.4922\n",
      "Epoch: 1\tBatch: 650\tTraining perplexity: 1.6359\n",
      "Epoch: 1\tBatch: 700\tAvg-Loss: 0.4403\n",
      "Epoch: 1\tBatch: 700\tTraining perplexity: 1.5531\n",
      "Epoch: 1\tBatch: 750\tAvg-Loss: 0.4612\n",
      "Epoch: 1\tBatch: 750\tTraining perplexity: 1.5859\n",
      "\n",
      "Validation loss: 0.4692984810897282\n",
      "Validation perplexity : 1.5988721606440426 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#epoch 7\n",
    "numEpochs = 1\n",
    "learningRate = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)\n",
    "train_epoch_packed(model, train_loader, eval_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tBatch: 50\tAvg-Loss: 0.3902\n",
      "Epoch: 1\tBatch: 50\tTraining perplexity: 1.4773\n",
      "Epoch: 1\tBatch: 100\tAvg-Loss: 0.3708\n",
      "Epoch: 1\tBatch: 100\tTraining perplexity: 1.4489\n",
      "Epoch: 1\tBatch: 150\tAvg-Loss: 0.3393\n",
      "Epoch: 1\tBatch: 150\tTraining perplexity: 1.4039\n",
      "Epoch: 1\tBatch: 200\tAvg-Loss: 0.3612\n",
      "Epoch: 1\tBatch: 200\tTraining perplexity: 1.4350\n",
      "Epoch: 1\tBatch: 250\tAvg-Loss: 0.3377\n",
      "Epoch: 1\tBatch: 250\tTraining perplexity: 1.4018\n",
      "Epoch: 1\tBatch: 300\tAvg-Loss: 0.3359\n",
      "Epoch: 1\tBatch: 300\tTraining perplexity: 1.3992\n",
      "Epoch: 1\tBatch: 350\tAvg-Loss: 0.3131\n",
      "Epoch: 1\tBatch: 350\tTraining perplexity: 1.3677\n",
      "Epoch: 1\tBatch: 400\tAvg-Loss: 0.3676\n",
      "Epoch: 1\tBatch: 400\tTraining perplexity: 1.4443\n",
      "Epoch: 1\tBatch: 450\tAvg-Loss: 0.3140\n",
      "Epoch: 1\tBatch: 450\tTraining perplexity: 1.3688\n",
      "Epoch: 1\tBatch: 500\tAvg-Loss: 0.3290\n",
      "Epoch: 1\tBatch: 500\tTraining perplexity: 1.3896\n",
      "Epoch: 1\tBatch: 550\tAvg-Loss: 0.3138\n",
      "Epoch: 1\tBatch: 550\tTraining perplexity: 1.3686\n",
      "Epoch: 1\tBatch: 600\tAvg-Loss: 0.3443\n",
      "Epoch: 1\tBatch: 600\tTraining perplexity: 1.4110\n",
      "Epoch: 1\tBatch: 650\tAvg-Loss: 0.3445\n",
      "Epoch: 1\tBatch: 650\tTraining perplexity: 1.4113\n",
      "Epoch: 1\tBatch: 700\tAvg-Loss: 0.3468\n",
      "Epoch: 1\tBatch: 700\tTraining perplexity: 1.4146\n",
      "Epoch: 1\tBatch: 750\tAvg-Loss: 0.3305\n",
      "Epoch: 1\tBatch: 750\tTraining perplexity: 1.3917\n",
      "\n",
      "Validation loss: 0.34489759760243555\n",
      "Validation perplexity : 1.411845335917536 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#epoch 8\n",
    "numEpochs = 1\n",
    "learningRate = 3e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)\n",
    "train_epoch_packed(model, train_loader, eval_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9973114,
     "status": "ok",
     "timestamp": 1573208981844,
     "user": {
      "displayName": "Zichen Li",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mArxVcsqOTvpJJhNNsbsrzoYSAdI2qr04Wp2U6LtA=s64",
      "userId": "09393985127784237998"
     },
     "user_tz": 300
    },
    "id": "HeUMfnhvFAws",
    "outputId": "1e0b7d1e-8045-4187-bc66-70f635dd37c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tBatch: 50\tAvg-Loss: 0.2871\n",
      "Epoch: 1\tBatch: 50\tTraining perplexity: 1.3326\n",
      "Epoch: 1\tBatch: 100\tAvg-Loss: 0.3384\n",
      "Epoch: 1\tBatch: 100\tTraining perplexity: 1.4028\n",
      "Epoch: 1\tBatch: 150\tAvg-Loss: 0.2914\n",
      "Epoch: 1\tBatch: 150\tTraining perplexity: 1.3383\n",
      "Epoch: 1\tBatch: 200\tAvg-Loss: 0.3024\n",
      "Epoch: 1\tBatch: 200\tTraining perplexity: 1.3531\n",
      "Epoch: 1\tBatch: 250\tAvg-Loss: 0.2708\n",
      "Epoch: 1\tBatch: 250\tTraining perplexity: 1.3111\n",
      "Epoch: 1\tBatch: 300\tAvg-Loss: 0.3218\n",
      "Epoch: 1\tBatch: 300\tTraining perplexity: 1.3796\n",
      "Epoch: 1\tBatch: 350\tAvg-Loss: 0.2865\n",
      "Epoch: 1\tBatch: 350\tTraining perplexity: 1.3317\n",
      "Epoch: 1\tBatch: 400\tAvg-Loss: 0.2916\n",
      "Epoch: 1\tBatch: 400\tTraining perplexity: 1.3385\n",
      "Epoch: 1\tBatch: 450\tAvg-Loss: 0.2840\n",
      "Epoch: 1\tBatch: 450\tTraining perplexity: 1.3285\n",
      "Epoch: 1\tBatch: 500\tAvg-Loss: 0.2869\n",
      "Epoch: 1\tBatch: 500\tTraining perplexity: 1.3323\n",
      "Epoch: 1\tBatch: 550\tAvg-Loss: 0.3229\n",
      "Epoch: 1\tBatch: 550\tTraining perplexity: 1.3811\n",
      "Epoch: 1\tBatch: 600\tAvg-Loss: 0.3082\n",
      "Epoch: 1\tBatch: 600\tTraining perplexity: 1.3610\n",
      "Epoch: 1\tBatch: 650\tAvg-Loss: 0.2930\n",
      "Epoch: 1\tBatch: 650\tTraining perplexity: 1.3405\n",
      "Epoch: 1\tBatch: 700\tAvg-Loss: 0.3132\n",
      "Epoch: 1\tBatch: 700\tTraining perplexity: 1.3679\n",
      "Epoch: 1\tBatch: 750\tAvg-Loss: 0.3064\n",
      "Epoch: 1\tBatch: 750\tTraining perplexity: 1.3586\n",
      "\n",
      "Validation loss: 0.3379222818783351\n",
      "Validation perplexity : 1.402031535913515 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#epoch 9\n",
    "numEpochs = 1\n",
    "learningRate = 3e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)\n",
    "train_epoch_packed(model, train_loader, eval_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9973730,
     "status": "ok",
     "timestamp": 1573208982462,
     "user": {
      "displayName": "Zichen Li",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mArxVcsqOTvpJJhNNsbsrzoYSAdI2qr04Wp2U6LtA=s64",
      "userId": "09393985127784237998"
     },
     "user_tz": 300
    },
    "id": "-l10E6HB2VUN",
    "outputId": "52cb4530-73a9-40f6-a9b1-d495e515f69f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tBatch: 50\tAvg-Loss: 0.2752\n",
      "Epoch: 1\tBatch: 50\tTraining perplexity: 1.3168\n",
      "Epoch: 1\tBatch: 100\tAvg-Loss: 0.2901\n",
      "Epoch: 1\tBatch: 100\tTraining perplexity: 1.3366\n",
      "Epoch: 1\tBatch: 150\tAvg-Loss: 0.2537\n",
      "Epoch: 1\tBatch: 150\tTraining perplexity: 1.2888\n",
      "Epoch: 1\tBatch: 200\tAvg-Loss: 0.2716\n",
      "Epoch: 1\tBatch: 200\tTraining perplexity: 1.3120\n",
      "Epoch: 1\tBatch: 250\tAvg-Loss: 0.2835\n",
      "Epoch: 1\tBatch: 250\tTraining perplexity: 1.3278\n",
      "Epoch: 1\tBatch: 300\tAvg-Loss: 0.2897\n",
      "Epoch: 1\tBatch: 300\tTraining perplexity: 1.3360\n",
      "Epoch: 1\tBatch: 350\tAvg-Loss: 0.2775\n",
      "Epoch: 1\tBatch: 350\tTraining perplexity: 1.3199\n",
      "Epoch: 1\tBatch: 400\tAvg-Loss: 0.2624\n",
      "Epoch: 1\tBatch: 400\tTraining perplexity: 1.3001\n",
      "Epoch: 1\tBatch: 450\tAvg-Loss: 0.2717\n",
      "Epoch: 1\tBatch: 450\tTraining perplexity: 1.3122\n",
      "Epoch: 1\tBatch: 500\tAvg-Loss: 0.2780\n",
      "Epoch: 1\tBatch: 500\tTraining perplexity: 1.3205\n",
      "Epoch: 1\tBatch: 550\tAvg-Loss: 0.2780\n",
      "Epoch: 1\tBatch: 550\tTraining perplexity: 1.3205\n",
      "Epoch: 1\tBatch: 600\tAvg-Loss: 0.2845\n",
      "Epoch: 1\tBatch: 600\tTraining perplexity: 1.3291\n",
      "Epoch: 1\tBatch: 650\tAvg-Loss: 0.2741\n",
      "Epoch: 1\tBatch: 650\tTraining perplexity: 1.3154\n",
      "Epoch: 1\tBatch: 700\tAvg-Loss: 0.3026\n",
      "Epoch: 1\tBatch: 700\tTraining perplexity: 1.3533\n",
      "Epoch: 1\tBatch: 750\tAvg-Loss: 0.2759\n",
      "Epoch: 1\tBatch: 750\tTraining perplexity: 1.3178\n",
      "\n",
      "Validation loss: 0.330956768989563\n",
      "Validation perplexity : 1.3922996004885668 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#epoch 10\n",
    "numEpochs = 1\n",
    "learningRate = 3e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)\n",
    "train_epoch_packed(model, train_loader, eval_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13287489,
     "status": "ok",
     "timestamp": 1573212296225,
     "user": {
      "displayName": "Zichen Li",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mArxVcsqOTvpJJhNNsbsrzoYSAdI2qr04Wp2U6LtA=s64",
      "userId": "09393985127784237998"
     },
     "user_tz": 300
    },
    "id": "wN_P-JVIKDV1",
    "outputId": "6c94eff2-f82e-4cd6-836a-a8444fad76f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tBatch: 50\tAvg-Loss: 0.2572\n",
      "Epoch: 1\tBatch: 50\tTraining perplexity: 1.2933\n",
      "Epoch: 1\tBatch: 100\tAvg-Loss: 0.2340\n",
      "Epoch: 1\tBatch: 100\tTraining perplexity: 1.2637\n",
      "Epoch: 1\tBatch: 150\tAvg-Loss: 0.2568\n",
      "Epoch: 1\tBatch: 150\tTraining perplexity: 1.2928\n",
      "Epoch: 1\tBatch: 200\tAvg-Loss: 0.2589\n",
      "Epoch: 1\tBatch: 200\tTraining perplexity: 1.2955\n",
      "Epoch: 1\tBatch: 250\tAvg-Loss: 0.2744\n",
      "Epoch: 1\tBatch: 250\tTraining perplexity: 1.3158\n",
      "Epoch: 1\tBatch: 300\tAvg-Loss: 0.2321\n",
      "Epoch: 1\tBatch: 300\tTraining perplexity: 1.2612\n",
      "Epoch: 1\tBatch: 350\tAvg-Loss: 0.2742\n",
      "Epoch: 1\tBatch: 350\tTraining perplexity: 1.3154\n",
      "Epoch: 1\tBatch: 400\tAvg-Loss: 0.2544\n",
      "Epoch: 1\tBatch: 400\tTraining perplexity: 1.2897\n",
      "Epoch: 1\tBatch: 450\tAvg-Loss: 0.2579\n",
      "Epoch: 1\tBatch: 450\tTraining perplexity: 1.2942\n",
      "Epoch: 1\tBatch: 500\tAvg-Loss: 0.2410\n",
      "Epoch: 1\tBatch: 500\tTraining perplexity: 1.2725\n",
      "Epoch: 1\tBatch: 550\tAvg-Loss: 0.2555\n",
      "Epoch: 1\tBatch: 550\tTraining perplexity: 1.2911\n",
      "Epoch: 1\tBatch: 600\tAvg-Loss: 0.2984\n",
      "Epoch: 1\tBatch: 600\tTraining perplexity: 1.3477\n",
      "Epoch: 1\tBatch: 650\tAvg-Loss: 0.2594\n",
      "Epoch: 1\tBatch: 650\tTraining perplexity: 1.2962\n",
      "Epoch: 1\tBatch: 700\tAvg-Loss: 0.2825\n",
      "Epoch: 1\tBatch: 700\tTraining perplexity: 1.3264\n",
      "Epoch: 1\tBatch: 750\tAvg-Loss: 0.2715\n",
      "Epoch: 1\tBatch: 750\tTraining perplexity: 1.3120\n",
      "\n",
      "Validation loss: 0.3324279887335641\n",
      "Validation perplexity : 1.394349486697156 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#epoch 11\n",
    "numEpochs = 1\n",
    "learningRate = 3e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)\n",
    "train_epoch_packed(model, train_loader, eval_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3316605,
     "status": "ok",
     "timestamp": 1573215649263,
     "user": {
      "displayName": "Zichen Li",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mArxVcsqOTvpJJhNNsbsrzoYSAdI2qr04Wp2U6LtA=s64",
      "userId": "09393985127784237998"
     },
     "user_tz": 300
    },
    "id": "eMyRkbYxMoGb",
    "outputId": "c8809bfc-0ff0-4407-dc64-33c00dd1bd5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tBatch: 50\tAvg-Loss: 0.2203\n",
      "Epoch: 1\tBatch: 50\tTraining perplexity: 1.2465\n",
      "Epoch: 1\tBatch: 100\tAvg-Loss: 0.2231\n",
      "Epoch: 1\tBatch: 100\tTraining perplexity: 1.2500\n",
      "Epoch: 1\tBatch: 150\tAvg-Loss: 0.2425\n",
      "Epoch: 1\tBatch: 150\tTraining perplexity: 1.2745\n",
      "Epoch: 1\tBatch: 200\tAvg-Loss: 0.2225\n",
      "Epoch: 1\tBatch: 200\tTraining perplexity: 1.2492\n",
      "Epoch: 1\tBatch: 250\tAvg-Loss: 0.2064\n",
      "Epoch: 1\tBatch: 250\tTraining perplexity: 1.2293\n",
      "Epoch: 1\tBatch: 300\tAvg-Loss: 0.2210\n",
      "Epoch: 1\tBatch: 300\tTraining perplexity: 1.2473\n",
      "Epoch: 1\tBatch: 350\tAvg-Loss: 0.2060\n",
      "Epoch: 1\tBatch: 350\tTraining perplexity: 1.2287\n",
      "Epoch: 1\tBatch: 400\tAvg-Loss: 0.2229\n",
      "Epoch: 1\tBatch: 400\tTraining perplexity: 1.2497\n",
      "Epoch: 1\tBatch: 450\tAvg-Loss: 0.1829\n",
      "Epoch: 1\tBatch: 450\tTraining perplexity: 1.2007\n",
      "Epoch: 1\tBatch: 500\tAvg-Loss: 0.2003\n",
      "Epoch: 1\tBatch: 500\tTraining perplexity: 1.2217\n",
      "Epoch: 1\tBatch: 550\tAvg-Loss: 0.1785\n",
      "Epoch: 1\tBatch: 550\tTraining perplexity: 1.1955\n",
      "Epoch: 1\tBatch: 600\tAvg-Loss: 0.2068\n",
      "Epoch: 1\tBatch: 600\tTraining perplexity: 1.2298\n",
      "Epoch: 1\tBatch: 650\tAvg-Loss: 0.1993\n",
      "Epoch: 1\tBatch: 650\tTraining perplexity: 1.2206\n",
      "Epoch: 1\tBatch: 700\tAvg-Loss: 0.1917\n",
      "Epoch: 1\tBatch: 700\tTraining perplexity: 1.2113\n",
      "Epoch: 1\tBatch: 750\tAvg-Loss: 0.2097\n",
      "Epoch: 1\tBatch: 750\tTraining perplexity: 1.2333\n",
      "\n",
      "Validation loss: 0.32164195137364526\n",
      "Validation perplexity : 1.3793907985479308 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#epoch 12\n",
    "numEpochs = 1\n",
    "learningRate = 3e-5\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)\n",
    "train_epoch_packed(model, train_loader, eval_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13287488,
     "status": "ok",
     "timestamp": 1573212296227,
     "user": {
      "displayName": "Zichen Li",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mArxVcsqOTvpJJhNNsbsrzoYSAdI2qr04Wp2U6LtA=s64",
      "userId": "09393985127784237998"
     },
     "user_tz": 300
    },
    "id": "bQqYAJb52W8m",
    "outputId": "f6256ece-2dae-48d1-c8c7-43269c5ac13c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Network. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, 'hw3p2_best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tBatch: 50\tAvg-Loss: 0.2150\n",
      "Epoch: 1\tBatch: 50\tTraining perplexity: 1.2399\n",
      "Epoch: 1\tBatch: 100\tAvg-Loss: 0.2229\n",
      "Epoch: 1\tBatch: 100\tTraining perplexity: 1.2496\n",
      "Epoch: 1\tBatch: 150\tAvg-Loss: 0.2233\n",
      "Epoch: 1\tBatch: 150\tTraining perplexity: 1.2502\n",
      "Epoch: 1\tBatch: 200\tAvg-Loss: 0.2101\n",
      "Epoch: 1\tBatch: 200\tTraining perplexity: 1.2338\n",
      "Epoch: 1\tBatch: 250\tAvg-Loss: 0.1997\n",
      "Epoch: 1\tBatch: 250\tTraining perplexity: 1.2210\n",
      "Epoch: 1\tBatch: 300\tAvg-Loss: 0.2148\n",
      "Epoch: 1\tBatch: 300\tTraining perplexity: 1.2396\n",
      "Epoch: 1\tBatch: 350\tAvg-Loss: 0.1999\n",
      "Epoch: 1\tBatch: 350\tTraining perplexity: 1.2212\n",
      "Epoch: 1\tBatch: 400\tAvg-Loss: 0.2005\n",
      "Epoch: 1\tBatch: 400\tTraining perplexity: 1.2220\n",
      "Epoch: 1\tBatch: 450\tAvg-Loss: 0.2156\n",
      "Epoch: 1\tBatch: 450\tTraining perplexity: 1.2406\n",
      "Epoch: 1\tBatch: 500\tAvg-Loss: 0.2052\n",
      "Epoch: 1\tBatch: 500\tTraining perplexity: 1.2278\n",
      "Epoch: 1\tBatch: 550\tAvg-Loss: 0.2174\n",
      "Epoch: 1\tBatch: 550\tTraining perplexity: 1.2428\n",
      "Epoch: 1\tBatch: 600\tAvg-Loss: 0.1986\n",
      "Epoch: 1\tBatch: 600\tTraining perplexity: 1.2198\n",
      "Epoch: 1\tBatch: 650\tAvg-Loss: 0.2133\n",
      "Epoch: 1\tBatch: 650\tTraining perplexity: 1.2377\n",
      "Epoch: 1\tBatch: 700\tAvg-Loss: 0.2038\n",
      "Epoch: 1\tBatch: 700\tTraining perplexity: 1.2261\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-5bff87ca54b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlearningRate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3e-5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearningRate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_epoch_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-ef467685e78e>\u001b[0m in \u001b[0;36mtrain_epoch_packed\u001b[0;34m(model, train_loader, eval_loader)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_lens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#epoch 13\n",
    "numEpochs = 1\n",
    "learningRate = 3e-5\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)\n",
    "train_epoch_packed(model, train_loader, eval_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 683
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 205668,
     "status": "ok",
     "timestamp": 1573223023304,
     "user": {
      "displayName": "Zichen Li",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mArxVcsqOTvpJJhNNsbsrzoYSAdI2qr04Wp2U6LtA=s64",
      "userId": "09393985127784237998"
     },
     "user_tz": 300
    },
    "id": "YznbUY3p3fff",
    "outputId": "23870ceb-0217-4f9e-aa98-a4989cc27c72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'ctcdecode' already exists and is not an empty directory.\n",
      "Requirement already satisfied: wget in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (3.2)\n",
      "/home/ubuntu/ctcdecode\n",
      "Processing /home/ubuntu/ctcdecode\n",
      "Building wheels for collected packages: ctcdecode\n",
      "  Building wheel for ctcdecode (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ctcdecode: filename=ctcdecode-0.4-cp36-cp36m-linux_x86_64.whl size=11241017 sha256=3d4d3d8c7da2975b48b0c7426163657bfaf03ab516a4e97bc96a678fb7180049\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-q8lkka2j/wheels/bc/c3/2c/f1c79bf1243976282df707f69771e696116f7bddf6a40edeb4\n",
      "Successfully built ctcdecode\n",
      "Installing collected packages: ctcdecode\n",
      "  Found existing installation: ctcdecode 0.4\n",
      "    Uninstalling ctcdecode-0.4:\n",
      "      Successfully uninstalled ctcdecode-0.4\n",
      "Successfully installed ctcdecode-0.4\n",
      "/home/ubuntu\n"
     ]
    }
   ],
   "source": [
    "!git clone --recursive https://github.com/parlance/ctcdecode.git\n",
    "!pip3 install wget\n",
    "%cd ctcdecode\n",
    "!pip3 install .\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MxPJom8c3lVH"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from ctcdecode import CTCBeamDecoder\n",
    "decoder = CTCBeamDecoder(['$'] * num_classes, beam_width=100, log_probs_input=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R3Yd_ZMX4ebz"
   },
   "outputs": [],
   "source": [
    "class Testset(Dataset):\n",
    "    def __init__(self, frames):\n",
    "        self.frames=[torch.tensor(f) for f in frames]\n",
    "    def __getitem__(self,i):\n",
    "        time_steps = self.frames[i]\n",
    "        return time_steps.to(device)\n",
    "    def __len__(self):\n",
    "        return len(self.frames)\n",
    "\n",
    "# for packed_seqs, you want to return your data sorted by length\n",
    "def collate_lines(seq_list):\n",
    "    inputs = seq_list\n",
    "    X_lens = torch.LongTensor([len(seq) for seq in inputs])\n",
    "    X = pad_sequence(inputs)\n",
    "\n",
    "    return X, X_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JEamfvpf5lkq"
   },
   "outputs": [],
   "source": [
    "test_dataset = Testset(testX)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, drop_last=False, batch_size=32, collate_fn = collate_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2_dyXLy75w3x"
   },
   "outputs": [],
   "source": [
    "prediction = []\n",
    "\n",
    "def predict(model, test_loader):\n",
    "  \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "\n",
    "        for batch_num, (inputs, X_lens) in enumerate(test_loader):\n",
    "\n",
    "            outputs, out_lens = model(inputs, X_lens)\n",
    "            test_Y, _, _, test_Y_lens = decoder.decode(outputs.transpose(0, 1), out_lens)\n",
    "            \n",
    "            for i in range(len(test_Y)):\n",
    "                # For the i-th sample in the batch, get the best output\n",
    "                best_seq = test_Y[i, 0, :test_Y_lens[i, 0]]\n",
    "                sequence = ''\n",
    "                for c in best_seq:\n",
    "                    if c != 0:\n",
    "                        sequence += PHONEME_MAP[c-1] \n",
    "                prediction.append(sequence)\n",
    "                \n",
    "predict(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7lleeLiXBNdB"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "result = pd.DataFrame()\n",
    "result['Id'] = list(range(523))\n",
    "result['Predicted'] = prediction\n",
    "result.to_csv('zichenli_hw3p2_dropout_6.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1ySXniyNXQX4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hw3p2_5_512.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
